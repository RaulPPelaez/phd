\RequirePackage{silence} % :-\
%    \WarningFilter{scrreprt}{Usage of package `titlesec'}
%    \WarningFilter{scrreprt}{Activating an ugly workaround}
%    \WarningFilter{titlesec}{Non standard sectioning command detected}
\documentclass[ twoside,openright,titlepage,numbers=noenddot,%1headlines,
headinclude,footinclude,cleardoublepage=empty,abstract=on,
BCOR=5mm,paper=b5,fontsize=11pt, dvipsnames
]{scrreprt}
\usepackage[utf8]{inputenc}
\usepackage{todonotes}
\usepackage{multicol}
\usepackage{xspace}
\usepackage{float} 
\usepackage{courier}
\usepackage{lmodern}
\usepackage{titletoc}
\def\ucpp{uammd_cpp_lexer.py:UAMMDCppLexer -x}
%\def\ucpp{cuda}

\usepackage{tcolorbox}
\usepackage{xparse}
\tcbuselibrary{breakable,minted,xparse,skins,listings}
\usemintedstyle{default}
\setminted[\ucpp]{ %
  linenos=false,             % Line numbers
  autogobble=true,          % Automatically remove common white space
  fontsize=\small,
  breaklines
}
%\usepackage[tocindentauto]{tocstyle}
\usepackage{chngcntr}
\AtBeginDocument{% the counter is defined later
  \counterwithout{lstlisting}{chapter}%
}
\renewcommand\lstlistlistingname{List of Codes}
\renewcommand\lstlistingname{\textbf{Source Code}}
\AtBeginDocument{
  \newtcblisting[list inside=lol, auto counter, list type={lstlisting}, blend into=listings]{code2}[2][]{
    colback=white,
    colbacktitle=white,
    coltitle=black,
    breakable,
    pad at break*=0mm,
    listing only,
    listing engine=minted,
    listing remove caption=true,
    list text={ },
    title={~#1},
    minted language=\ucpp,
    enhanced jigsaw,
    minipage boxed title,
    attach boxed title to bottom center={xshift=0mm,yshift=-1mm},
    boxed title style={size=small, blanker},
    center title,
    #2
  }
}

%\makeatletter
%\AtBeginDocument{%
%\renewcommand\lstlistoflistings{\bgroup
%  \let\contentsname\lstlistlistingname
%  \def\l@lstlisting##1##2{\@dottedtocline{1}{-1.5em}{-2.3em}{##1}{##2}}
%  \let\lst@temp\@starttoc \def\@starttoc##1{\lst@temp{lol}}%
%  \tableofcontents \egroup}
%}
%\makeatother


\input{classicthesis-config}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\algnewcommand{\LeftComment}[1]{\Statex \(\triangleright\) #1}
\algnewcommand{\Input}[1]{\hspace*{\algorithmicindent} \textbf{Input:} #1}
\usepackage{bm}
\usepackage{hyperref}
\usepackage[acronym]{glossaries}
\usepackage{pgfplots}
\usepackage{subcaption}
\usepackage{enumitem}
\graphicspath{{./gfx/}}

\newcommand{\executeiffilenewer}[3]{
  \ifnum
  \pdfstrcmp{\pdffilemoddate{#1}}{\pdffilemoddate{#2}}>0
  {\immediate\write18{#3}}
  \fi
}
\newcommand{\includesvg}[2][width=\columnwidth]{
  \executeiffilenewer{#2.svg}{#2.pdf}
  {inkscape -D #2.svg --export-type=pdf } %  --export-latex}
  %\def\svgwidth{#2}
  % \input{#1.pdf_tex}
  \includegraphics[#1]{#2.pdf}
}

%\pagecolor[rgb]{0.2,0.2,0.2}
%\color[rgb]{1,1,1}

%
%\definecolor{kwcol}{HTML}{32b43f}
%\definecolor{comcol}{HTML}{7e8775}
%\definecolor{dircol}{HTML}{e090d7}
%\definecolor{idcol}{HTML}{ff8531}
%\definecolor{morecol}{HTML}{3ab5eb}
%\definecolor{morecol3}{HTML}{972ebd}
%\usepackage{listings}
%\lstset{ 
%  language=[11]C++,
%  basicstyle= \ttfamily\small,
%  numbers=none,
%%  numberfirstline=true,
%%  numbersep=5pt,                  % how far the line-numbers are from the code
%  backgroundcolor=\color{white},      % choose the background color. You must add \usepackage{color}
%  showspaces=false,       
%  rulecolor=\color{black},
%  breaklines=true,
%  breakatwhitespace=true,
%  keywordstyle=\bfseries\color{kwcol},      
%  commentstyle=\color{comcol},
%  identifierstyle=\color{black},
%  directivestyle=\bfseries\color{dircol},
%  stringstyle=\color{orange},   
%  keywords=[2]{Parameters, UAMMD},
%  keywordstyle=[2]\color{morecol},
%  keywords=[3]{std, thrust, uammd, access},
%  keywordstyle=[3]\color{morecol3}
%}

\makeglossaries
\newacronym{UAMMD}{UAMMD}{Universally Adaptable Multiscale Molecular Dynamics}
\newacronym{MD}{MD}{Molecular Dynamics}
\newacronym{LD}{LD}{Langevin Dynamics}
\newacronym{BD}{BD}{Brownian Dynamics}
\newacronym{BDHI}{BDHI}{Brownian Dynamics with Hydrodynamic Interactions}
\newacronym{IBM}{IBM}{Immersed Boundary}
\newacronym{PSE}{PSE}{Positively Split Ewald}
\newacronym{FCM}{FCM}{Force Coupling Method}
\newacronym{ICM}{ICM}{Inertial Coupling Method}
\newacronym{FIB}{FIB}{Fluctuating Immersed Boundary}
\newacronym{DPD}{DPD}{Dissipative Particle Dynamics}
\newacronym{SPH}{SPH}{Smoothed Particle Hydrodynamics}
\newacronym{SE}{SE}{Spectral Ewald}
\newacronym{LJ}{LJ}{Lennard-Jones}
\newacronym{WCA}{WCA}{Weeks-Chandler-Andersen}
\newacronym{MIC}{MIC}{Minimum Image Convention}
\newacronym{FFT}{FFT}{Fast Fourier Transform}
\newacronym{FCT}{FCT}{Fast Chebyshev Transform}
\newacronym{iFFT}{iFFT}{Inverse Fast Fourier Transform}
\newacronym{GPU}{GPU}{Graphical Processor Units}
\newacronym{GPGPU}{GPGPU}{General Purpose Computing on GPU}
\newacronym{RHS}{RHS}{Right Hand Side}\newcommand{\rhs}{\gls{RHS}\xspace}
\newacronym{API}{API}{Application Programming Interface}
\newacronym{BVP}{BVP}{Boundary Value Problem} \newcommand{\bvp}{\gls{BVP}\xspace}
\newacronym{BC}{BC}{Boundary Condition} \newcommand{\bc}{\gls{BC}\xspace}\newcommand{\bcs}{\gls{BC}s\xspace}
\newacronym{DP}{DP}{Doubly Periodic}
\newacronym{ODE}{ODE}{Ordinary Differential Equation}
\newacronym{PDE}{PDE}{Partial Differential Equation}
\newacronym{SDE}{SDE}{Stochastic Differential Equation}
\newacronym{EM}{EM}{Euler-Maruyama}
\newacronym{AB}{AB}{Adams-Bashforth}
\newacronym{LK}{LK}{Leimkuhler}
\newacronym{FPE}{FPE}{Fokker-Plank Equation}
\newacronym{PBC}{PBC}{Periodic Boundary Conditions}
\newacronym{RPY}{RPY}{Rotne-Prager-Yamakawa}\newcommand{\rpy}{\gls{RPY}\xspace}
\renewcommand{\vec}[1]{\bm{#1}}
\newcommand{\tens}[1]{\bm{\mathcal{#1}}}
\newcommand{\oper}[1]{\mathcal{#1}}
\newcommand{\uammd}{\gls{UAMMD}\xspace}
\newcommand{\gpu}{\gls{GPU}\xspace}
\newcommand{\dt}{\delta t}
\newcommand{\kT}{k_B T}
\newcommand{\sinc}{\textrm{sinc}}
\newcommand{\floor}{\textrm{floor}}
\newcommand{\near}{\textrm{near}}
\newcommand{\far}{\textrm{far}}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\fou}[1]{\widehat{#1}}
\newcommand{\noise}{\widetilde{W}}
\newcommand{\rafa}[1]{\todo[color=yellow,textcolor=red]{#1}}
\DeclareMathOperator{\erf}{erf}

\newcommand{\ppos}{q}
\newcommand{\pvel}{u}
\newcommand{\fpos}{r}
\newcommand{\fvel}{v}

\newcommand{\corr}{\text{corr}}
\newcommand{\dpr}{\text{\tiny DP}}
\newcommand{\qtd}{\text{\tiny q2D}}

\addbibresource{Bibliography.bib}

% \hyphenation{put special hyphenation here}
\pdfmapfile{-mpfonts.map}

\begin{document}
\frenchspacing
\raggedbottom
\selectlanguage{american} % american ngerman
% \renewcommand*{\bibname}{new name}
%\setbibpreamble{}
\pagenumbering{roman}
\pagestyle{plain}

\thispagestyle{empty}
%\pdfbookmark[1]{Titel}{title}
%*******************************************************
\begin{center}
    \spacedlowsmallcaps{\myName} \\ \medskip

    \begingroup
    \color{CTtitle}\spacedallcaps{\myTitle}\\ \bigskip
    \mySubtitle \\ \medskip
    \endgroup
\end{center}

\begin{titlepage}
    %\pdfbookmark[1]{\myTitle}{titlepage}
    % if you want the titlepage to be centered, uncomment and fine-tune the line below (KOMA classes environment)
    \begin{addmargin}[-1cm]{-3cm}
    \begin{center}
        \large

        \hfill

        \vfill

        \begingroup
        \color{CTtitle}\spacedallcaps{\myTitle} \\ \bigskip
        \mySubtitle \\ \medskip
        \endgroup

        \spacedlowsmallcaps{\myName}

        \vfill

        \includegraphics[width=13cm]{gfx/froggy} \\ \medskip

        %\myDegree \\
        \myDepartment \\
        \myFaculty \\
        \myUni \\ \bigskip

        \myTime\ -- \myVersion

        \vfill

    \end{center}
  \end{addmargin}
\end{titlepage}

\thispagestyle{empty}

\hfill

\vfill

\noindent\myName: \textit{\myTitle,} \mySubtitle, %\myDegree,
\textcopyright\ \myTime

\bigskip

\noindent\spacedlowsmallcaps{Supervisor}: \\
\myProf \\
\myOtherProf \\
\mySupervisor

%\medskip
%
%\noindent\spacedlowsmallcaps{Location}: \\
%\myLocation
%
%\medskip
%
%\noindent\spacedlowsmallcaps{Time Frame}: \\
%\myTime

\thispagestyle{empty}

\hfill

\vfill

\noindent\myName: \textit{\myTitle,} \mySubtitle, %\myDegree,
\textcopyright\ \myTime

\cleardoublepage
\thispagestyle{empty}
\phantomsection
%\pdfbookmark[1]{Quotes}{Quotes}

\vspace*{3cm}

\begin{center}
      This thing is complex... so complex not even God gets it.\\ \medskip
    --- Rafa Buscalioni
    \\ \medskip\medskip
        There is no way you are getting a quote for your thesis\\ \medskip
    --- NVIDIA

\end{center}

\medskip

%\cleardoublepage\input{FrontBackmatter/Foreword}
\cleardoublepage
%*******************************************************
% Abstract
%*******************************************************
%\renewcommand{\abstractname}{Abstract}
\pdfbookmark[1]{Abstract}{Abstract}
% \addcontentsline{toc}{chapter}{\tocEntry{Abstract}}
\begingroup
\let\cleardoublepage\relax
\let\cleardoublepage\relax
\let\cleardoublepage\relax

\chapter*{English Abstract}

Complex fluids is an umbrella term for the coexistence between two phases, in our cases of interest these being a solid and a liquid. A wide range of soft matter systems, oftentimes of a biological nature, can be categorized as complex fluids. We often find the presence of a group of certain slow solid, or soft, objects submerged in a solvent comprised by smaller particles governed by faster dynamics. A suspension of colloidal particles in water or a small virus diffusing in the cellular environment are examples of complex fluids. The dynamics of these kind of systems are more often than not influenced (and sometimes straight-up controlled) by thermal fluctuations and hydrodynamic correlations arising from the elimination of the solvent's degrees of freedom. Additionally, the two-phase coupling often imposes geometrical restrictions that fundamentally alter the dynamics of both the solution and the solvent. Both of these effects require special (and in some occasions altogether novel) mathematical and algorithmic consideration. This thesispushes the boundaries of numerical simulation of complex fluids. In this manuscript, I present an overview of past, recent and novel algorithms adapted to the GPU, a powerful, massively-parallel hardware recently developed. This thesis has culminated in the release of UAMMD, a new software infrastructure for complex fluids simulations in the GPU. Besides the novel tools and methodology, I will also go through some real-life applications which have led to scientific breakthroughs published in research journals.

\cleardoublepage
\newpage

\chapter*{Spanish Abstract}
El término fluido complejo se refiere a la consistencia entre dos fases, en nuestros casos de estudio habituales siendo una de ellas solida y la otra líquida. Un amplio rango de sistemas en materia blanda, a menudo de naturaleza biológica, pueden categorizarse como fluidos complejos. Típicamente nos encontramos con un grupo de objetos solidos, o blandos, cuya dinámica es lenta, que están sumergidos en un solvente compuesto de partículas más pequeñas y con una dinámica mucho mas rápida. Algunos ejemplos de fluidos complejos son una suspensión de partículas coloidales en agua o un pequeño virus difundiendo en el entorno celular.
La dinámica de este tipo de sistemas está habitualmente influenciada (o incluso gobernada) por fluctuaciones térmicas y correlaciones hydrodinámicas que surgen de la eliminación de algunos de los grados de libertad del solvente.
Adicionalmente, el acoplamiento entre las dos fases a menudo impone restricciones geométricas que alteran fundamentalmente la dinámica de la solución, así como la del solvente. Ambos mecanismos requieren tener en cuenta consideraciones matemáticas y algorítmicas especiales (y en ocasiones completamente novedosas).
La presente tesis extiende los límites de la simulación numérica de fluidos complejos. En este manuscrito, presento una serie de algoritmos, pasados y nuevos, adaptados a la GPU, un poderoso hardware extremadamente paralelo recientemente desarrollado. La culminación de esta tesis ha tenido como resultado la publicación de UAMMD, una nueva infrastructura de software para la simulación de fluidos complejos en la GPU. Además de las herramientas y metodología originales, también repasaré algunas aplicaciones prácticas que han resultado en avances científicos publicados en revistas de investigación.

\cleardoublepage
\newpage
\vspace*{-20pt}
\pdfbookmark[1]{Publications}{publications}
\chapter*{Publications}

The following publications resulted from the development of this work
%\noindent Put your publications from the thesis here. The packages \texttt{multibib} or \texttt{bibtopic} etc. can be used to handle multiple different bibliographies in your document.

\begin{refsection}[ownpubs]
  \small
    \expandafter\def\csname blx@maxbibnames\endcsname{99}%
    \nocite{*} % is local to to the enclosing refsection
    \printbibliography[heading=none]
\end{refsection}


\cleardoublepage
%\pdfbookmark[1]{Acknowledgments}{acknowledgments}
%
%\begin{flushright}{\slshape}
%\end{flushright}
%
%
%
%\bigskip
%
%\begingroup
%\let\cleardoublepage\relax
%\let\cleardoublepage\relax
%\let\cleardoublepage\relax
%\chapter*{Acknowledgments}
%
%Thanks y'all.
%
%
%\cleardoublepage
%
\glsunsetall
\pagestyle{scrheadings}
%\phantomsection
\pdfbookmark[1]{\contentsname}{tableofcontents}
\setcounter{tocdepth}{2} % <-- 2 includes up to subsections in the ToC
\setcounter{secnumdepth}{3} % <-- 3 numbers up to subsubsections
\manualmark
\markboth{\spacedlowsmallcaps{\contentsname}}{\spacedlowsmallcaps{\contentsname}}
\tableofcontents
\automark[section]{chapter}
\renewcommand{\chaptermark}[1]{\markboth{\spacedlowsmallcaps{#1}}{\spacedlowsmallcaps{#1}}}
\renewcommand{\sectionmark}[1]{\markright{\textsc{\thesection}\enspace\spacedlowsmallcaps{#1}}}
%*******************************************************
% List of Figures and of the Tables
%*******************************************************
\cleardoublepage
% \pagestyle{empty} % Uncomment this line if your lists should not have any headlines with section name and page number
\begingroup
    \let\cleardoublepage\relax
    \let\cleardoublepage\relax
    %*******************************************************
    % List of Figures
    %*******************************************************
    \begin{multicols}{2}[\chapter*{List of Figures}]
      \setlength{\columnseprule}{1pt}
      \def\columnseprulecolor{\color{black}}

      \phantomsection
      \addcontentsline{toc}{chapter}{\listfigurename}
      \pdfbookmark[1]{\listfigurename}{lof}
      % \listoffigures
      \makeatletter
      \@starttoc{lof}% Print List of Figures
      \makeatother


      \vspace{8ex}
    \end{multicols}
    \newpage
    \begin{multicols}{2}[\chapter*{List of Codes}]
      \setlength{\columnseprule}{1pt}
      \def\columnseprulecolor{\color{black}}
      % *******************************************************
      % List of Listings
      % *******************************************************
      \phantomsection
      \addcontentsline{toc}{chapter}{\lstlistlistingname}
      \pdfbookmark[1]{\lstlistlistingname}{lol}
      %vspace here is a hack to remove the title of the listoflistings
      \tcblistof[\vspace*]{lol}{-\baselineskip}
      \vspace{8ex}
    \end{multicols}
    %*******************************************************
    % Acronyms
    %*******************************************************
    %\phantomsection
    \pdfbookmark[1]{Acronyms}{acronyms}
 %   \markboth{\spacedlowsmallcaps{Acronyms}}{\spacedlowsmallcaps{Acronyms}}
%    \chapter*{Acronyms}
    \printglossary[type=\acronymtype,nonumberlist]
\endgroup
\glsresetall
\newpage
\cleardoublepage
\pagestyle{scrheadings}
\pagenumbering{arabic}
%\setcounter{page}{90}
% use \cleardoublepage here to avoid problems with pdfbookmark
\cleardoublepage
\part{Introduction}\label{pt:intro}
\chapter{Introduction}\label{ch:introduction}

The modeling and simulation of physical systems is always tied to a certain spatio-temporal scale (see Fig. \ref{fig:sptime-land}). Usually, studying a system through the lens of a certain scale prevents the exploration of the others.
Choosing the right lens for a problem requires understanding its characteristic times and lengths.
Say, for instance, that you want to study the flow of a waterfall throughout the next thousand years. It would be a bad decision to employ quantum mechanics for such a feat, as it is a theoretical framework for events that take femtoseconds and angstroms.

Mathematical frameworks and numerical techniques have been devised for the exploration of different spatio-temporal scales. However, many physical phenomena are intrinsically \emph{multiscale}, as the effect of small and fast scales affect the larger and slower ones.
The advent of supercomputing in the last two decades has presented the field with new powerful tools to leverage. For the first time we have more computing power than what our techniques are capable of handling. 
Developing new techniques (numerical and theoretical) directly aimed to these new technologies will enable the exploration of regimes previously unreachable, opening the doors to new phenomena.
Multiscale problems will particularly benefit from the improvements in supercomputing.
For instance, \cite{Hadden2018} to simulate every molecule of a virus capsid submerged in water ($6$ million atoms) in the span of $1\mu s$ ($5\cdot10^8$ simulation steps).
As another example of these huge simulations, the authors of \cite{Rochaix2019} use $100$ million particles to model a chunk of the interior environment of a bacterium for some hundreds of nanoseconds (around $10^7$ steps).

These sort of simulations would be impossible without the aid of a supercomputer and specialized software tools. Indeed, aside from raw computing power, new fast and efficient numerical schemes (adapted to novel computing architectures) are needed to cover larger spatio-temporal windows in physical modeling. This is precisely the objective of this thesis, which is devoted, in particular, to the development of new algorithms and software tools for the study of complex fluids and biological systems in high-performance computing environments.

Most of the properties of complex fluids often arise from an intermediate regime, called the \emph{mesoscale}.
The definition of mesoscale is fuzzy, usually it ranges from a few nanometers (a protein) to $~10\mu m$ (the size of a HeLa cell). Characteristic times range from microseconds to even minutes.
The mesoscale poses a series of theoretical and numerical challenges, because it stems from the microscopic spatio-temporal scales and yet it interacts with slower processes over large scales: an optimal challenge for a super computer.
\begin{figure}[h]
  \centering
  \includesvg[width=\columnwidth]{gfx/scales}
  \caption[ ]{The spatio-temporal landscape and its numerical techniques. Different techniques are applied to simulate the different scales involved in a biological system or, in general, a complex fluid. Zooming in from the bottom-left we have the blood inside a section of a vein, the plasmic environment in a small drop of blood (where cells, platelets, viruses, etc. are present), a virus, a single protein and finally the individual atoms that compose it along with the surrounding fluid particles (water).}
  \label{fig:sptime-land}
\end{figure}

\section{The Graphical Processor Unit}

Recently, a new paradigm of supercomputing has arisen. The \gpu, a massively parallel co-processor, so powerful that it requires to straight-up rethink our algorithms to take advantage of it.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{gpu_and_me}
  \caption[ ]{A picture a GPU (NVIDIA Tesla). This card will grant its owner the computing power of a small CPU cluster for a price of around 1000€.}
  \label{fig:gpuandme}
\end{figure}

Yet, the \gpu technology, interpreted as some kind of specialized graphic circuit or co-processor, can be tracked back to the 1970s. Once electronic devices started having screens attached to them, it made sense to have some kind of chip translating the CPU information into the analogical signal of the display. It was then only a matter of time before more functionality was put into them. It started as a way to accelerate the display process in early video game hardware, such as arcade systems.
Storing the data that goes into the screen takes a lot of space and, at the time, RAM memory was expensive (it is still, by the way!).

In 1977, the Atari 2600 (one of the first home consoles) could simply not afford to store the contents of its 160x80 pixel display into its 128 bytes of RAM. Of course, these systems employed some truly creative software tricks to reduce memory usage. However, the numbers just do not add up. Even if we limit ourselves to a monochrome output (so one pixel takes one bit to store), the frame buffer for a 160x80 pixel display takes 1600 bytes\footnote{In contrast, the NVIDIA GTX 980 (the trusty, consumer-grade, GPU that has accompanied me throughout most of my Ph.D.) has 4 GB of memory and will happily output to several screens with 4k resolution (4096x2160 pixels).}.
The solution was not to have a frame buffer at all, but rather to outsource the display operations to specialized hardware. The Atari 2600's graphic chip had a 128 color palette and allowed developers to use close to 5 sprites to design their 2D games.

These chips were little more than video display chips, but they were the predecessors to the \gpu.
Throughout the next decade the hardware evolved to support faster and more complex operations, hold more memory, etc. In 1990, a graphic adapter could draw 16 million different colors to a 1028x1024 pixel display and hold 1MB of data.

Graphics adapter were still centered around 2D graphics acceleration, such as graphical user interfaces and 2D games. However, the market was already pushing for real-time 3D graphics in games. Although one could hack the prevailing graphics 2D pipeline to draw 3D environments, without specialized hardware acceleration (as was already common for 2D), the results were far from interactive. Surely, it was a task best suited for the graphics adapter technology already established.

Soon enough, during the early and mid 1990s, several companies were releasing their graphics adapters with 3D acceleration capabilities. In 1994, the term \gpu was coined to designate this hardware, which had evolved beyond its original task of just sending pixels to a display.
OpenGL\cite{opengl}, one of the first graphics \glspl{API}\footnote{A software library.}, appeared in the early 1990s attempting to standardize the programming of graphic hardware accelerators, especially for 3D.
At the time, OpenGL had quite limited capabilities, allowing a developer to do little more than to feed a fixed pipeline with triangles. OpenGL would then interface with the \gpu to turn this geometry into a 2D image that could be displayed.
Naturally, this translation involves a great deal of raw computation, further transforming the \gpu from a video display card to an independent co-processor. Furthermore,  the usual operations required to do this happen to be inherently parallel. While the CPU evolved to be formed by a single, powerful core, the \gpu was born out of a necessity for parallel computing. Thus, a \gpu tends to have many, less powerful, processing cores.
Jumping again to the year 2000, the quality and complexity of computer graphics had exploded. Card manufacturers have included all sorts of new 3D hardware-accelerated operations that go beyond simple triangles and OpenGL has evolved along with them. As it usually happens, people started to hack around, using the graphics pipeline to perform computations not necessarily related to computer graphics. In particular, a new \gpu was presented by the NVIDIA Corporation in 2001 that allowed to modify the different stages of the graphic pipeline via so-called ``programmable shaders''. These shaders were short programs that could intercede between the different stages of drawing. A vertex shader could be written to process each triangle before sending it to a fragment shader, which could process each pixel on the screen before finally sending them to the display. This was the advent of the so-called \gls{GPGPU}.

Shaders were enough for the community to realize that drawing polygons was far from the only thing they could do with a \gpu\cite{gpgpu2002}. Applications exploiting shaders arise everywhere in a variety fields like scientific image processing \cite{gpuimage2003, gpuimage2006}, linear algebra \cite{gpulinalg2001, gpulinalg2003a, gpulinalg2003b}, physics \cite{gpulbm2004} and even machine learning\cite{gpuml2005, gpuml1998}. One can even find a molecular dynamics code running on the \gpu of a Sony PlayStation 3 \cite{ps3md2009}.

The next natural step took place in 2007, when the NVIDIA Corporation released CUDA\cite{cuda}\footnote{Although the original acronym for CUDA was Compute Unified Device Architecture, the platform quickly outgrew this definition. However, the term CUDA was already established and thus it has remained as the name of the platform.} alongside its flagship card, the GeForce 8800.

CUDA is a programming model and an extension of the C++ programming language that allows to exploit CUDA-enabled \glspl{GPU} for general-purpose computing. It can be considered as a conceptual generalization of the aforementioned graphic shaders that acknowledges the \gpu as a general-purpose processor. Via the inclusion of a small set of keywords and extensions to C++, CUDA exposes the massively parallel architecture of the \gpu under the ecosystem of an already established high-performance programming language.
Now developers aiming for the \gpu did not have to be experts in hacking a certain graphics \gls{API} and could just focus on paralleling the algorithms for its particular architecture.

The birth of CUDA became a pivotal moment in high performance computing, opening the door to a revolution in several fields such as medical analysis, bioinformatics, machine learning, economic market analysis, molecular dynamics and more.

In this thesis, we will push the boundaries of \gpu computing by developing and implementing old and new algorithms for the modeling of soft matter systems into a new infrastructure called \uammd. \uammd is designed from scratch with the \gpu in mind and is written in CUDA/C++.

One of the main criticisms of CUDA is that it is a closed source, proprietary environment that can only be compiled for NVIDIA's own \glspl{GPU}. This restricts CUDA code to only a subset of \gpu hardware.
In 2009, an open standard called OpenCL\cite{Stone2010} was born (from the creators of OpenGL) as an alternative to CUDA. Although implementations of OpenCL exist for almost any hardware (including GPUs from NVIDIA and AMD, CPUs, FPGAs,...) its adoption has been slow, partly due to an aggressive marketing campaign from NVIDIA to sell CUDA as the better alternative\footnote{Since the birth of CUDA, NVIDIA has been promoting it by donating GPUs to researchers, giving away a plethora of CUDA courses all around the world, and more.}. The general feeling in the community has always been that OpenCL should win as the de facto \gls{GPGPU}-\gls{API} in the long run. However, it simply has not happened yet.
Luckily, there are ways to translate CUDA code into OpenCL code\footnote{In particular, the HIP library works as a thin wrapper over CUDA/OpenCL, allowing to write code that can be compiled for any of the \glspl{API}.} and, given that the paradigms of both \glspl{API} are quite similar, it is fairly straightforward to port CUDA code to OpenCL. Therefore, in the future, porting the software infrastructure presented in this manuscript to OpenCL would be mainly painless and will not require changes to the public \gls{API}. So, even though at the time of writing \uammd is restricted to run on NVIDIA's hardware, it is possible to adapt it so that it runs on virtually any parallel hardware.

\section{Basic concepts of GPU programming}\label{ch:gpuintroduction}
The \gpu is a pivotal concept in this work and, as such, it is worth introducing here its basic computing model. Doing so will enable the unfamiliar reader to better grasp the importance of designing algorithms specific for the \gpu and the new hardships that arise when approaching it from a CPU-centric world. Note, however, that this is merely an introduction to the particularities that come with coding for the \gpu and will probably not contain enough information for an unfamiliar reader to be able to write \gpu code. Appendix \ref{sec:cpp} contains a more in depth introduction to programming C++ and CUDA, although a plethora of arbitrarily-detailed tutorials and courses are freely available on the web.

The \gpu (usually referred to simply as \emph{device}) works as a massively parallel co-processor, with thousands of (relatively slow) cores (workers), independent of the rest of the system for most uses and purposes. The \gpu runs instructions independently (and in principle asynchronously) from the CPU (referred to as the \emph{host}). More importantly, the \gpu has its own memory space which is separated from the system RAM. The device cannot access the host memory directly (and vice-versa)\footnote{This is not entirely true anymore for recent CUDA versions via the so-called Unified Virtual Addressing (UVA) system. However, this advanced functionality is beyond the scope of this work and, in any case, \uammd only makes use of it in very specific instances.}. If information has to be shared between each other an explicit memory copy has to be issued, which incurs a synchronization barrier and is in general a slow process. In this section, we will discuss how these properties are taken into account and how they affect the programming model.

From a programmatic point of view, we can access the \gpu capabilities using CUDA, which extends C++ (by introducing new keywords and an \gls{API}) to accommodate for the particular programming model of a \gpu\footnote{There are other ways to program a \gpu, such as OpenACC\cite{openacc} or OpenCL\cite{Stone2010}, but we will stick to CUDA.}.
%Note, however, that this Appendix is not intended to be a CUDA programming guide (the reader can look online for a plethora of CUDA learning resources), but just an introduction to the particularities that come with coding for the \gpu.

CUDA exposes the many cores of the \gpu via a series of logical groupings of workers (which in turn map to the hardware cores). The most basic computing unit is called a \emph{thread}, which can execute a single instruction at a time. A group of threads (typically around $128$ and inferior to $1024$) is called a \emph{thread-block}. Finally, several blocks are grouped into a \emph{grid}. A \emph{grid} is assigned to a special kind of (\gpu) function, called a \emph{kernel}, containing instructions that are to be executed by the group of threads.

The main memory space of the \gpu (the equivalent of the heap memory in the CPU) is called \emph{global memory}. Memory allocated in this memory space is accessible at all times by every thread. 
Similar to the CPU's heap memory, global memory is slow to access (although a series of caches exists to mitigate this).
Each thread has its own small, fast access, local memory space (called \emph{register}, or \emph{local}, memory) which is somewhat equivalent to the \emph{stack} in the CPU.
Finally, CUDA exposes another high bandwidth memory space that is private to each block but accessible for all threads within. It is the \emph{shared} memory space, as threads in a block are able to share information directly through it.

See Fig. \ref{fig:cudablocks} for a schematic representation of the CUDA threading model.
\begin{figure}[H]
  \centering
  \includesvg[width=\columnwidth]{gfx/cudablocks}
  \caption[ ]{Representation of the thread geometry for a kernel launch with $2$ blocks of $4$ threads (\emph{blockDim} = (2,2)). Threads inside the same block run concurrently, while different blocks might run desynchronized between them. Each thread has a small local (and private) register memory space. Furthermore, threads inside the same block can share memory among them via the low-latency \emph{shared memory}. All threads can access the same, low-bandwidth, \emph{global memory} space.}
  \label{fig:cudablocks}
\end{figure}

Although it is not required by the programming model, threads inside a block excel when executing the same set of instructions in lock-step. In particular, blocks are composed of groups of 32 threads called \emph{warps}. Threads in a warp will execute (at the hardware level) the same instructions at the same time, but not necessarily acting on the same data (in what is known as a SIMD model). The CUDA compiler will happily process code in which each thread in a warp executes different code (for instance, by using conditional branches), but the actual executable will simply \emph{emulate} this behavior by making all threads execute all branches and discarding the results for the ones that should not have been executed. Naturally, this so-called branch divergence (or simply divergence) greatly hurts performance\footnote{Furthermore, GPUs do not have branch-prediction as opposed to the CPU, where branch-prediction is one of the pillars of CPU architecture.}. Therefore, conditional-branches that result in divergent code in CUDA must be avoided whenever possible. Note that, on the other hand, divergence does not occur when all the threads in a given warp enter the same branch. Figure \ref{fig:branchdivergence} contains a depiction of branch divergence.
\begin{figure}[H]
  \centering
  \includesvg[width=0.8\columnwidth]{gfx/branchdivergence}
  \caption[ ]{Representation of a divergent branch. The 32 threads conforming a warp (curvy arrows) encounter a piece of divergent code (a conditional branch that separates threads in the same warp). When divergent code is encountered each branch is executed in serial (as opposed to, for instance, a parallel CPU code), execution does not continue until all threads have concluded processing all branches since all threads in the same warp must execute instructions in lock-step.}
  \label{fig:branchdivergence}
\end{figure}

On the other hand, CUDA offers no guarantee regarding the execution order of blocks.

CUDA allows us to write small programs, called kernels, that we can then launch to run on the \gpu. The instructions in a kernel are executed by a grid of threads, whose size can be customized per launch. Appendix \ref{sec:cpp} offers an introduction to programming \gpu code using C++ and CUDA.

%Finally, it is worth mentioning here the concept of \gpu architectures in CUDA. In general, with each release of a new \gpu hardware series comes a new \emph{architecture}. New architectures bring new functionality and require 

\section{Software for soft matter simulations}

\subsection*{A brief history of molecular dynamics software}
Particle based computer modeling of soft matter systems has been around since the advent of computers. The use of computers for physical simulation can be tracked back to as far as the 1940's when researches at the Los Alamos national laboratory described the use of the ENIAC\footnote{The first programmable, electronic, general-purpose digital computer.} for simulating some stochastic physical process\cite{Hurd1985} (the origins of the Monte Carlo method\cite{Johansen2010}). Shortly after, in the 1950's, computational molecular dynamics was born\cite{DeTullio2016}.

One of the first examples of a software package for molecular dynamics is CHARMM, whose initial release dates back to 1983\cite{Brooks1983}. To put it in perspective, Windows 1.0 was released in 1985, the UNIX operating system was disclosed to the public in 1973 and Linux originated in 1991. It is humbling to realize that since 1983 the basic layout of a fully fledged molecular dynamics software package has not changed that much. As a matter of fact, the infrastructure presented in this manuscript shares many of the basic conceptual sections already present in \cite{Brooks1983}.

It is important to also mention that MPI, one of the first parallel \glspl{API}, was not available until 1994. Up to that point, sequential computing was the norm.

Another famous molecular dynamics package is GROMACS~\cite{Berendsen1995}, which was released in 1991, with the first version using MPI coming shortly after it's release.

\subsection*{The GPU revolution}
Many of these software packages were born more than a decade before the term \gpu existed and were already quite established when the \gpu became a mainstream general-purpose computing hardware. Naturally almost all of them have, at least partially, ported their functionalities to be accelerated by a \gpu in some way. However, \gpu programming poses such a wildly orthogonal paradigm that it makes the porting process slow and painful. Most CPU-centric molecular dynamics packages are already so complex that redesigning from the ground up to accommodate for the \gpu is not an option and thus their inclusion can become awkward, sometimes causing so-called software rot.

It is therefore worth it to start from a clean slate every once in a while considering the new environments. One example of this is the general-purpose molecular dynamics software package HOOMD\cite{Anderson2008}. Born around the same time as CUDA itself HOOMD was designed from the ground up with the \gpu in mind. Although HOOMD provides a CPU-only backend it is centered around the \gpu and can work exclusively in it, this is crucial, as communication between the CPU and the \gpu is quite expensive, so much that it can often negate the benefits of using a \gpu altogether.

The \gpu offers mind-blowing raw computing power when used correctly. Developing new algorithms and software infrastructures from scratch particularly designed for the \gpu architecture is therefore essential to exploit this amazing technology to the fullest. This starts by taking the whole computation to the \gpu in order to avoid the communication cost. On the other hand, at a library level, one of the most powerful tools exposed by the \gpu is the \gls{FFT}. It just so happens that the existing algorithms to compute the discrete Fourier transform numerically are a perfect fit for the \gpu. We will exploit this by devising pseudo-spectral algorithms in which the majority of the computations take place in Fourier space.

\subsection*{Closed vs. Open ecosystems}
Most commercial molecular dynamics packages share one foundational design principle; they are \emph{closed} ecosystems. This means that, for a user, the only option to interface with the packages is to encode a simulation in them in its totality. In other words, it is awkward (or straight up not possible) to use these packages as external accelerators for an already established, in-house, code.\footnote{If these software packages were hardware stores, a closed ecosystem would consist of the tools inside the store (the hammer, saw, etc) being chained to the counter, so you would need to bring your furniture into the store in order to fix it.}  

A closed ecosystem allows to design the software under less assumptions, easing development. On the other hand, a closed ecosystem misses some opportunities in doing so. For instance, one of the first problems a soft matter software package must solve is the construction of a short ranged particle interaction list (a so-called neighbour list), which usually governs the overall computational cost of a simulation. Creating a public, library-like, interface exposing this list creation algorithm (in what we would call an open ecosystem) allows an user to take his existing code and accelerate it, as opposed to having to do it the other way around, that is porting the entirety of his simulation to the closed ecosystem.

\uammd attempts to provide an \emph{open} ecosystem, exposing most of its underlying algorithms in a library-like fashion\footnote{Following the hardware store analogy, the tools of an open ecosystem store would not only be unchained, but also available for rent. Furthermore, this would be an \emph{open-source} hardware store (bear with me) so the cost of renting the tools is zero euros per day. In this kind of store, you can just borrow the tool(s) you need and fix your stuff at home.}.
Furthermore, while most packages require compilation into a binary or library in order to be used, \uammd is a header-only framework. Meaning that the compilation of a source code using parts of \uammd can be seamlessly blended in during the compilation process of another software.


%Developing algorithms for the ground up with the GPU architecture in mind is essential. Leverage ultra efficient \gpu \gls{FFT} by developing spectral algorithms.
\newpage
\cleardoublepage
  
\ctparttext{An overview of the UAMMD infrastructure.\newline\newline\includegraphics[width=1.3\linewidth]{gfx/titleimage}}
\part{UAMMD: Design and components}\label{pt:uammd}

\chapter{The Fermation of subproblems}

The Trinity test, the first atomic bomb detonation in history, was carried out in 1945. The scientists of the Manhattan project dumbfoundedly observed how the detonation unraveled from a base afar. Among them was Enrico Fermi, the so-called architect of the nuclear age, who was 44 at the time. Fermi had absent-mindedly torn apart a sheet of paper moments before the detonation took place and held the pieces in his hands. Forty seconds after the blast the then withered shock-wave hit the crew. At the first sign of the shockwave Fermi, who had placed his hands above his head in expectation, released the paper bits, which landed a couple of meters past him. After a brief moment of meditation, Fermi announced to the crew that the detonation had released about 10 kilotons of energy.
It took weeks of analyzing the data from the plethora of sensors available on site to confirm Fermi's estimate of the energy, which is now believed to have been near 20 kilotons.

Fermi had mastered (and practically invented) these back-of-the-envelope calculations, which are more precise than a guess but are not mathematical proofs. He knew how to subdivide complex, intractable, problems into many small and manageable ones. In order to teach this technique to his students, Fermi prompted them with a kind of question that has become known as a \emph{Fermi problem}. A Fermi problem presents itself as an apparently unanswerable question. For instance: ``\emph{If I attach a blood vial to the wheels of my bike, how fast do I need to go and for how long for the plasma to separate from the blood cells?}'' or ``\emph{How many hairs does a polar bear have?}''. At first sight it seems that we simply do not have enough information at hand to solve these problems. However, if we break them down into small, answerable, subproblems we find that we can give a pretty good estimate in a short time: ``\emph{What is the skin area of a bear?}'', ``\emph{How many hairs per squared centimeter does a bear have?}''. These can in turn be interpreted as Fermi problems themselves and further subdivided: ``\emph{What is the average height of a bear?}'', ``\emph{Assuming a bear is somewhat cylindrical, what is it's radius}?'',``\emph{What is the diameter of a hair?}''\footnote{Let us assume bear hairs grow in a near close-packed configuration (it's cold at the poles, you know) where each hair is on the order of $20\mu m$ in radius. This yields $1\text{cm}^2/((20\mu m)^2\pi) \sim 100K \text{hairs}/\text{cm}^2$. I have not found the precise number, but we can compare with a sea otter (the mammal with the densest fur) which has between $100-400K \text{hairs}/\text{cm}^2$. This gives me some confidence about the validity of our order of magnitude estimation.}, etc.

Surely we can apply Fermi's teachings and \emph{Fermate} our endeavors into subproblems too.
Consider the development of a software capable of reproducing the dynamics of an ionic solution in a cellular membrane channel. This poses a really convoluted simulation, involving electrostatics, fluctuating hydrodynamics, steric interactions, bonded ligatures, etc.\footnote{A simulation easily handled by \uammd, by the way.}. As we will see along this manuscript, some of these algorithms are quite complex on their own, depending on lots of moving parts. The mathematical and theoretical machinery required to model something like that appears to be in the realm of a theory of everything. It sure sounds like a Fermi problem.

Still, we could try to design, from the ground up, a software that can perform this hypothetical simulation. 
However, this top-down approach is fundamentally flawed. The final product will probably have a lot of internal dependencies, many of which are probably unnecessary. In software this means that the code will be hard to adapt, extend or even scarier, to maintain.

We humans are not good with complex things (which is probably the reason why complex and complicated share the same etymology) and, as can be glimpsed throughout \uammd, my solution to this problem is often to either Fermate it or remove it altogether.

Like we did when counting hairs, we start by dividing the original problem into subproblems in a hierarchical fashion (see Fig. \ref{fig:uammdsketch}). The top of the tree represents the most general (or abstract) aspect of the problem, and subdivisions offer increasingly specific concepts. So instead of designing our framework with an ionic solution in a membrane channel in mind, we start at the top and then hierarchically specialize. What constitutes the ``top'', or \emph{soul}, of the problem is of course highly subjective and thus the contents of the next sections should be considered little more than my humble take on the problem\footnote{Note, however, that I am far from the first to come up with this logical separation. Slightly different separations and with different names are present in basically every molecular dynamics software package.}.

In particular, we can find the root by removing assumptions until we get to a point where only one (or a small number of them) remains. Therefore, we do not design in terms of ``ions'', ``cells'' or ``molecules'', rather in terms of a vague concept called ``particles''.
Following Fermi's teachings we embrace the (redundant) complexity of complex fluids instead of trying to fight it.

% The \uammd framework takes this KISS\footnote{Keep It Simple, Sir} philosophy very seriously.

The foundational concepts of \uammd are supported on a handful of (deliberately) vague assumptions which can be summarize in four:
\begin{enumerate}[label=\textbf{S.\arabic*}]
\item The \emph{System} assumption: The code will run primarily on a \gpu (the most limiting assumption in the development process).
\item The \emph{ParticleData} assumption: Simulations are based on the state of ``particles'' (whatever a particle and its state mean).
\item The \emph{Integrator} assumption: The state of these particles changes in time.
\item The \emph{Interactor} assumption: Particles can interact with each other and with an external influence.
\end{enumerate}
These are the four pillars depicted in Fig. \ref{fig:uammdsketch}. The software framework exposes these assumptions through four foundational classes (i.e objects in programming), which are, in order; \emph{System}, \emph{ParticleData}, \emph{Integrator} and \emph{Interactor}.

\begin{figure}[h]
  \centering
  \includesvg[width=\columnwidth]{gfx/sketchUAMMD}
  \caption[ ]{The basic hierarchy of concepts (and code) in \uammd, represented by a series of modules connected by arrows that convey the direction of the data flow. \emph{System} holds information about the actual physical hardware, and all the entities below rely on it to interact with the environment. \emph{ParticleData} (a ``real class'' in the code) stores all the information about the particles in the simulation, such as positions ($\vec{r}$), velocities ($\vec{v}$), mass, etc, as well as other properties like the current forces acting on it ($\vec{F}$), energy ($E$) or virial ($T$). Modules below (explained later) can request, at their leisure, a list with any of the particles properties. \emph{Integrators} and \emph{Interactors} are interfaces (called virtual classes in programming terms), are in charge of, respectively, forwarding the simulation one step in time and computing the forces, energies and/or virials acting on each particle. These ``Interfaces'' (drawn in red) are abstract objects that cannot be instanced on their own but rather must be inherited. For instance, Brownian Dynamics (section. \ref{sec:bd}) would be an \emph{Integrator}, while a module that computes gravitational forces would be an \emph{Interactor}.}
  \label{fig:uammdsketch}
\end{figure}
%\todo{Add something about gpu arch in System in the fig, gpu arch, ncores,...}
Excluding the first assumption, which is unrelated to physics, it is straightforward to see that our initial ionic solution simulation fits into this criterion. As a bonus, we have ended up with a basis that also describes things like ``a gas of argon atoms'', ``a group of planets orbiting a star'' or ``a virus filled with proteins exploding due to the force exerted on it by an AFM tip''\footnote{An oddly specific example describing a real study that is being carried out with \uammd at the moment of writing by a collaborator group.}.

At the top of the tree in Fig. \ref{fig:uammdsketch} we find the \emph{System} object which takes care of setting up the computational environment for the rest of players. Usually the user does not need to interact with this object, which is automatically started when needed. \uammd's online documentation can be visited to learn more about the \emph{System} object (see Appendix \ref{ch:online})

The second assumption is represented in the code base via the \emph{ParticleData} object, which we discuss in \ref{sec:dynmol}.

Then, we can focus on each of the last two assumptions and apply this separation again. For instance, instead of grouping all interactions (fourth assumption) into a single entity (class) that deals with all possible interactions, we can split the concept between general things like short-ranged, bonded or long-ranged interactions. In section \ref{sec:interactions} we explore deeper along the \emph{Interactor} branch to discuss the different algorithms and programming interfaces related with particle interactions (see Fig. \ref{fig:uammdsketch_interactors}).

Similarly, in section \ref{sec:dynamics} we break down the \emph{Integrator} branch (bottom left of Fig. \ref{fig:uammdsketch}) into the several ways the state of the particles can evolve (see Fig. \ref{fig:uammdsketch_integrators}).

During the following chapters, we will discuss in detail this conceptual and programmatic hierarchy and follow its branches down to the leaves. These ``final'' computational objects representing the leaves of \uammd encapsulate the actual algorithms and physics in the form of isolated, individual modules. A ``module'' can be understood as a piece of code which receives information, processes it, and sends back some output. The internal complexity of a module is usually much larger than that related with its input and output. Input and output are the essence of the communication between modules.

In general, these modules will communicate only via the closed loop described by \emph{ParticleData}-\emph{Integrator}-\emph{Interactor} in Fig. \ref{fig:uammdsketch}.

One of the main unwritten lessons we will glean is that it is never a good idea to introduce dependencies that climb up through the tree. Just to give some examples of this:
\begin{itemize}
\item A neighbour list should not assume it is going to be used to
  compute the forces of a group of repulsive particles.
\item The existence
  of a hydrodynamics integrator module should not incur the
  modification of the \emph{Integrator} interface.
\end{itemize}
In other words, conceptual complexity and problem specificity should increase only downwards through the tree. This forces us as developers to modify a certain module only to generalize it, and never to specialize it. If a module appears to require being specialized, it is almost always a sign that either it should actually be generalized or exist as a separate entity beneath the initial one. This is what makes \uammd so modular and extensible. See figures \ref{fig:uammdsketch_interactors} or \ref{fig:uammdsketch_integrators}, the next lower branches.
\uammd carefully avoids these kind of interdependencies. As a matter of fact, if one were to inspect the commit history of the \uammd repository, they would find that many times the most relevant changes come in the form of \emph{removing} code, not adding it. What is interesting about this is that, maybe counterintuitively, this code removal almost always has resulted in a more generic module.

\chapter{It's in the name}\label{ch:design}

It is common knowledge that good software starts with a good acronym. \uammd stands for Universally Adaptable Multiscale Molecular Dynamics in an attempt to abridge the simple-yet-powerful four assumptions in the previous chapter\footnote{Naturally, the fact that the acronym for the Universidad Autonoma de Madrid is there is merely a coincidence.}. Each of the words carries a different aspect of the framework's philosophy. In this chapter, we will discuss each of them.
\section{\uppercase{\textbf{U}}niversality}

Complex fluids are a prime examples of a multiphysics system. As such, multiphysics are deeply ingrained in \uammd. Our framework can potentially take into account physical phenomena emerging from seemingly disparate fields.
One of the key features for \emph{universality} is to be able to communicate between Eulerian (fields) and Lagrangian (``particles'') descriptions. This hybrid approach permits \uammd to efficiently solve problems involving hydrodynamics, electrostatics, magnetism, light-matter interactions, and more... 
  
%Although we focus on the dynamics of particles, which have a Lagrangian description, using an intermediate Eulerian description is a possibility in these situations.
In chapter \ref{sec:bdhi}, we will describe a series of Eulerian-Lagrangian algorithms that incorporate the effect of a solvent in a group of submerged particles (hydrodynamics) by describing it explicitly. On the other hand, we will adapt these techniques to compute electrostatics in chapter \ref{ch:tppoisson} by describing an explicit \emph{charge-density field}.

Quantum physics has not been mentioned yet and for the time being we have limited our toolset to the classical world. Thus, staying in the realm of classical physics could be considered a fifth (soft) assumption.

It is however important to stress that the central (class) object in \uammd are the particles (see below). In fact, the fourth assumption in chapter \ref{pt:uammd} states that particles are expected to interact with each other or via an external influence. When particles are submerged in a fluid, its effect on the former can be considered an external influence. Even when the fluid itself is also affected by the presence particles, as is the case with hydrodynamics.

\section{\uppercase{\textbf{A}}daptability}


\begin{figure}[h]
  \centering
  %\includegraphics[width=\textwidth]{landscape}
  %\includesvg[width=0.8\columnwidth]{gfx/modular}
  \includesvg[width=0.8\columnwidth]{gfx/virus}
  \caption[ ]{A simulation is constructed by putting together several modules. In this instance, the simulation of a virus submerged in water could be constructed by joining a hydrodynamics \emph{Integrator} module with several \emph{Interactors} describing the interactions between particles, such as sterics, bonds, electrostatics,...}
  \label{fig:modular}
\end{figure}

Modularity is another central tenet in the philosophy behind \uammd, which provides its adaptability. 

Even when the internal complexity of each separate module can be quite great it never bleeds into the rest of the modules, which can remain oblivious to each other. This allows each module to worry only about solving one specific problem (hydrodynamics, electrostatics,...).

Morover, this makes \uammd extremely adaptable, since new modules can be added without disturbing the ecosystem.

In \uammd a simulation is described by connecting a series of \emph{Interactors} with an \emph{Integrator}. In Fig. \ref{fig:modular} we can see an example of a simulation which connects a hydrodynamics \emph{Integrator} (such as the ones described in sec. \ref{sec:bdhi}) with an \emph{Interactor} that deals with sterics (see sec. \ref{sec:shortrange}), another one that computes electrostatics (see sec. \ref{ch:tppoisson}), and so on and so forth. The communication between these modules is minimal, being mostly restricted to the closed loop in Fig. \ref{fig:uammdsketch}. Although nothing technically prevents two \emph{Interactors} from explicitly sharing information, this never happens in \uammd. 


This does not imply that each module should exist in a completely isolated environment, since a plethora of utilities are available in \uammd to aid with the typical problems that arise in complex fluids. For instance, a module in need of a neighbour list does not need to write one. Rather, it can simply make use of one of the solutions already implemented (see chapter \ref{sec:nlist}).

Adaptability also means that, when it makes sense, a module can be specialized from the outside. For instance, the \emph{Interactor} module that computes bonded forces (particles joined by springs or similar ligatures) is not specific to a particular potential. As users, we can specialize it, via the use of C++ templates, to work with a harmonic, FENE\cite{Warner1972}, Morse\cite{Morse1929}, or an arbitrarily complex potential.

The use of metaprogramming (in the C++ sense) reaches every corner of the code base, in many cases allowing to customize (or hack) a utility or module beyond its intended scope without having to modify a single line of it.

\uammd is presented as a CUDA/C++ header-only library and it is mostly self-contained in terms of dependencies. The use of the word library is not chosen lightly here. We have discussed how an \emph{Integrator} is joined with a collection of \emph{Interactors}, maybe suggesting that the latter cannot function without the former. This is not the case since our framework provides a completely open ecosystem, facilitating (and actually encouraging) the incorporation of \uammd modules outside their \emph{normal} environment.
All the utilities in \uammd, including \emph{Interactors}, \emph{Integrators}, neighbour lists, linear solvers and many more\footnote{A list of all the modules in \uammd at the time of writing can be found in Appendix \ref{sec:modulelist}.} can be used in an isolated way from outside the code base. This also means that parts of \uammd can be used as accelerators in already established codes. For instance, an already written simulation code can include one of our fast \gpu hydrodynamics implementations into their description with just a few lines of code.

In the past, we have explored this facet of the project by providing to collaborators some of our utilities and solvers as stand-alone Python interfaces.

The modularity of \uammd is reflected directly in the code, where modules can be created independently of each other as it will be evidenced by the example codes throughout this manuscript.
Be it an \emph{Integrator} (hydrodynamics, molecular dynamics,...) or \emph{Interactor} (short range, bonded,...), all modules in \uammd can be created and stored individually.
%The \emph{ParticleData} object here, which is the direct expression in code of the second assumption, will be described in detail in sec. \ref{sec:particledata}.
%\todo{Maybe switch the order of the code examples here. In fact, join them both and say that the preamble is going to be omited. Take both code examples below ParticleData. Specify ``Module'' is an Integrator or an Interactor.}
\section{\uppercase{\textbf{M}}ultiscale}\label{sec:multiscale}
\begin{figure}[h]
  \centering
  \includesvg[width=\columnwidth]{gfx/landscape}
  \caption[ ]{The spatio-temporal landscape and its numerical techniques. Different techniques are applied to simulate the different scales involved in a biological system or, in general, a complex fluid. Image courtesy of Rafael Delgado-Buscalioni.}
  \label{fig:landscape}
\end{figure}

Describing a given system in a reductionist manner, by solving the dynamics of every atom involved, is of course unfeasible. The \uammd infrastructure can accommodate several levels of description by making use of coarse-graining techniques. Coarse graining allows to realistically describe a system using fewer degrees of freedom.
In general, a level of description is tied to a certain time scale and is characterized by a series of relevant variables coupled via a (usually stochastic) dynamic equation.

Although \uammd is not restricted to soft matter systems, this work will concentrate mainly on such systems. In this regard we can identify a series of clearly separated levels of description. It is worthwhile to summarize them here since this manuscript will discuss many of them in detail. They are furthermore available as \uammd \emph{Integrator} modules. A representation of the different levels of description is available in Fig. \ref{fig:multiscale}, while the numerical methods employed to numerically study each of them can be seen in Fig. \ref{fig:sptime-land}. Figure \ref{fig:landscape} provides an overview of the different fields of soft matter simulation and its characteristic spatio-temporal scales. We can distinguish between the following levels of description (from most to least detailed):
\begin{figure}[h]
  \centering
  \includesvg[width=\columnwidth]{gfx/multiscale}
  \caption[ ]{The different levels of coarse grained description available in \uammd for soft matter simulations. $\vec{q}_i$ represents the positions of particles, $\vec{u}_i$ their velocity. $\vec{v}(\vec{r},t)$ represents an Eulerian fluid velocity field, $M(\vec{q}_{ij})$ a mobility tensor and $\xi(\vec{q}_{ij})$ a friction kernel. Arrows represent standard routes for the formal bottom-up derivation using coarse-graining theory. In order, we have: (1) in \cite{Camargo2018}, (2) in \cite{Hijon2010} and (3) in \cite{Dhont1996}.}
  \label{fig:multiscale}
\end{figure}

\begin{enumerate}
\item Microscopic: The relevant variables are the positions and moments of every particle in the system, including the solvent and solute particles. This level is dominated by the mean collision time between the particles, of the order of $\tau \sim 10^{-12} s$. The (purely Lagrangian) Newtonian equations of motion describe the dynamics of the system and \gls{MD} can be used to solve them. We explore this description in chapter \ref{sec:md}.
\item Hydrodynamic: The degrees of freedom of the solvent particles are omitted and turned into hydrodynamic fields. The Newtonian equations for the solute particles are coupled with the (fluctuating) Navier-Stokes equations that describe the solvent in what constitutes an Eulerian-Langrangian description. An infinitesimal region of space represents a group of solvent particles, whose individual degrees of freedom are lost. The characteristic time is now controlled by the speed of sound of the solvent ($\tau \sim 10^{-9}s$) and the vorticity ($\tau \sim 10^{-6}s$). We study the hydrodynamic level in chapter \ref{sec:bdhi}.
\item Langevin: Particles move so slowly compared to the solvent characteristic times that hydrodynamic interactions are effectively instantaneous. Only the positions and momenta of the particles remain as relevant variables (Lagrangian description once again), which obey a Langevin equation. The characteristic decorrelation time of the particle's velocities governs this scale with $\tau\sim 10^{-5}s$. We explore this inertial level of detail in chapter \ref{sec:langevin}.
\item Smoluchowski: Particle positions change slowly enough that their velocity decorrelates effectively instantaneously, inertia can be disregarded and positions are the only relevant variable. The dynamics are described by the \gls{BD} equations of motion (an over-damped Langevin equation). The diffusion time of the particles, $\tau \sim 10^{-3} s$, dominates this level. We study this limit in chapter \ref{sec:bd}.
\end{enumerate}
Above these levels of description particles are no longer relevant. Here we have the Fick level, in which a continuum particle concentration field is the relevant variable, and the thermodynamic level, in which the relevant variables are the macroscopic dynamical invariants of the system (mass, energy, volume, temperature,...). We can use these descriptions to study time scales of seconds (Fick) up to infinity (thermodynamics). However, given that \uammd is a particle-centric framework, these descriptions will not be considered in this manuscript.


With the exception of the microscopic level, in which particles experience effective interactions that come directly from quantum effects, every other level emerges as a coarse grained description of the previous one. 

Going from one level to the next incurs a loss of information (AKA degrees of freedom, e.g the hydrodynamic level cannot track the positions/momenta of the microscopic solvent particles). These eliminated, fast, degrees of freedom are modeled (reintroduced) as a drag coupled with thermal noise. In the following sections, we will see how dissipation is intimately related with thermal noise via the fluctuation-dissipation theorem, such relations ensure that the coarse-grained system attains the correct equilibrium distribution.


\subsection{The Fokker-Planck Formalism}\label{sec:fpe}
One of the basic theories for coarse graining is embedded in the Fokker-Planck formalism. Let us focus on a certain relevant slow, degree of freedom (variable) that interacts, in a \emph{random} way, with many other (fast) degrees of freedom. In general, we will eliminate fast variables from our description and reintroduce them as fluctuations. The \gls{FPE} describes the evolution of the probability distribution, $P(\vec{x},t)$, of a set of relevant or slow variables, $\vec{x} = \{x_1, x_2,\dots, x_N\}$ in a quite general way:
  \begin{equation}
    \label{eq:fpe}
    \partial_t P = -\vec{\partial}_{\vec{x}}\cdot \vec{J}
  \end{equation}
Where $P_0 := P(\vec{x}_0, t_0)$ is known, $\vec{J}$ is the flux of probability vector  and $\vec{\partial}_{\vec{x}}\cdot \vec{J} := \nabla_{\vec{x}}\cdot\vec{J} = \sum_i\partial J_i/\partial x_i  $ represents the divergence (note that the derivation is made with respect to the slow variables by using supervectors, i.e. all the variables and their coordinates\cite{Dhont1996}).
Using the backwards Kramers-Moyal expansion~\cite{Risken2012} truncated at the second term\footnote{The Pawula theorem~\cite{Tabar2019} can be invoked here, which proves that all terms beyond the second are meaningless. In particular, the Pawula theorem states that there are only three possible truncations of the Kramers-Moyal expansion: (1) truncating at the first term, in which case the process is deterministic; (2) truncating at the second term, in which case the process is diffusive; (3) keeping all terms up to infinity. However, keeping any term beyond the second results in a non-positive probability density.}, the probability flux vector, $\vec{J}$, can be expressed as,
\begin{equation}
  \label{eq:fpefluxkinetic}
    \vec{J}(\vec{x}, t) := \widetilde{\vec{D}}^{(1)}(\vec{x},t)P - \vec{\partial}_{\vec{x}}\cdot\left[\tens{D}^{(2)}(\vec{x}, t)  P\right]
\end{equation}
The first term is called the total drift or transport term ($\widetilde{\vec{D}}^{(1)}$ represents the total drift coefficients) and the second one the diffusion term (being $\tens{D}^{(2)}$ the diffusion coefficients). It will later come in handy to express Eq. \eqref{eq:fpeflux} using another definition for the drift coefficient, so that
\begin{equation}
  \label{eq:fpeflux}
  \vec{J} = \left(\vec{D}^{(1)} - \tens{D}^{(2)} \vec{\partial}_{\vec{x}}\right) P,
\end{equation}
where we used the chain rule to get the diffusion coefficients outside of the derivative, placing the extra diffusive term (referred to as thermal drift) inside the definition of a systematic drift coefficient, $\vec{D}^{(1)}:=\widetilde{\vec{D}}^{(1)}-\vec{\partial}_{\vec{x}}\cdot\tens{D}^{(2)}$.
%It is useful to write Eq. \eqref{eq:fpeflux} in its kinetic form
%\begin{equation}
%  \label{eq:fpeflux}
%  \vec{J} = \widetilde{\vec{D}}^{(1)}P - \tens{D}^{(2)}\vec{\partial}_{\vec{x}} P
%\end{equation}
%By redefining the drift term as $\widetilde{\vec{D}}^{(1)} := \vec{D}^{(1)} - \vec{\partial}_{\vec{x}}\cdot\tens{D}^{(2)}$.
%
The \gls{FPE} in Eq. \eqref{eq:fpe} simply represents the conservation of the conditional probability $P(\vec{x},t)d\vec{x}$ of finding a set of variables $\vec{x}$ in the range $[\vec{x}, \vec{x} + d\vec{x}]$. The generality of Eq. \eqref{eq:fpe} makes it applicable in a wide range of situations (a suspension of colloidal particles, fluctuations in electrical circuits, mechanical oscillators and chemical reactions just to name a few).

We can use the probability distribution $P$ to compute the ensemble average of a certain function $f(\vec{x}(t))$ as

\begin{equation}
  \left\langle f\right\rangle(t) = \int {P(\vec{x},t) f(\vec{x}) d\vec{x}}.
\end{equation}
Its time evolution can be then expressed as
\begin{equation}
\label{eq:fpetimeevol}  
  \begin{aligned}
    \frac{d\left\langle f\right\rangle}{dt} &= \int{f\partial_tPd\vec{x}} = \int{P\left(\vec{D}^{(1)}\vec{\partial}_{\vec{x}}f + \vec{\partial}_{\vec{x}}\cdot\left[\tens{D}^{(2)} \vec{\partial}_{\vec{x}}f\right]\right)d\vec{x}}\\
    &= \left\langle \vec{D}^{(1)} \vec{\partial}_{\vec{x}}f\right\rangle + \left\langle \vec{\partial}_{\vec{x}}\cdot\left[\tens{D}^{(2)}\vec\partial_{\vec{x}}f\right]\right\rangle,
  \end{aligned}
\end{equation}
where the second equality comes from using integration by parts and the divergence theorem in an straightforward way~\cite{Risken2012}. Additionally, we assumed natural boundary conditions (closed system).
The choice of names for the coefficients becomes evident if we now study the temporal evolution of the first moments of the relevant variables. The first one (the mean) yields
\begin{equation}
  \label{eq:fpemean}  
  \frac{d\langle\vec{x}\rangle}{dt} = \underbrace{\left\langle \widetilde{\vec{D}}^{(1)}\right\rangle}_{\text{Drift}} = \overbrace{\left\langle \vec{D}^{(1)}\right\rangle}^{\text{Systematic drift}} + \underbrace{\left\langle \vec{\partial}_{\vec{x}}\cdot\tens{D}^{(2)}\right\rangle}_{\text{Thermal drift}}.
\end{equation}
% Where $\widetilde{\vec{D}}^{(1)} := \vec{D}^{(1)} + \vec{\partial}_{\vec{x}}\cdot\tens{D}^{(2)}$.
% Applying the chain rule in Eq. \eqref{eq:fpetimeevol} allows to compute the second moment,
The evolution of the second moment (matrix),$\langle \vec{x}\otimes\vec{x}\rangle$, can be obtained following a similar route as that leading to Eq. \eqref{eq:fpetimeevol} (see~\cite{Risken2012}), the result is
%In the next section, we will see that the second moment, the variance of the relevant variables, is closely related with the diffusion coefficients, $\tens{D}^{(2)}$. The second moment %It is however straightforward to study the variance in the case of a single dimension with a variable $x$, where the variance can be written as
\begin{equation}
  \label{eq:fpevar}
  \frac{d\langle \vec{x}\otimes\vec{x}\rangle}{dt} = \left\langle \widetilde{\vec{D}}^{(1)}\otimes\vec{x}\right\rangle + \left\langle \vec{x}\otimes\widetilde{\vec{D}}^{(1)}\right\rangle + 2\underbrace{\left\langle \tens{D}^{(2)}\right\rangle}_{\text{Diffusion}}.
\end{equation}
Here $\otimes$ represents the tensor product.
In section \ref{sec:langevin} we will study the Langevin equation, a case in which the relevant variables are the velocities (and positions) of the particles (which are surrounded by a bath of fast moving water molecules). On the other hand, in section \ref{sec:bd} we will see that in Brownian Dynamics (the over-damped limit of the Langevin equation), the relevant variables are the positions of the particles (their velocities being the fast, eliminated, degree of freedom). Finally, the applicability of the Fokker-Planck formalism at the hydrodynamic level will be briefly discussed in chapter \ref{sec:bdhi}.

\subsection{Fluctuation-Dissipation balance}\label{sec:fdb}
A system in contact with a heat bath will experience both friction and fluctuations stemming from the elimination of a series of fast degrees of freedom. Both magnitudes are directly related to each other in equilibrium, as the equipartition theorem states that every degree-of-freedom has $\kT/2$ kinetic energy, which requires that fluctuation and dissipation are in a certain specific balance.

We will see what the \gls{FPE} tells us about the fluctuation-dissipation balance by studying the covariance matrix,
\begin{equation}
  \label{eq:covariance}
\tens{\sigma} := \left\langle\vec{x}\otimes\vec{x}\right\rangle-\left\langle\vec{x}\right\rangle\otimes\left\langle\vec{x}\right\rangle=\left\langle \delta\vec{x}\otimes\delta\vec{x}\right\rangle,
\end{equation}
of the relevant variables and how it behaves at equilibrium. Here $\delta\vec{x} := \vec{x} - \langle\vec{x}\rangle$ is a fluctuation of the relevant variables around their averages.
The time evolution of the covariance can be written as
\begin{equation}
  \label{eq:covariancetimeevol}
\partial_t\tens{\sigma} =\partial_t\left\langle\vec{x}\otimes\vec{x}\right\rangle-\left\langle\vec{x}\right\rangle\otimes\partial_t\left\langle\vec{x}\right\rangle -\partial_t\left\langle\vec{x}\right\rangle\otimes\left\langle\vec{x}\right\rangle,
\end{equation}
Let us also define $\delta\vec{D} := \vec{D} - \langle\vec{D}\rangle$ as the fluctuations around the average of the transport coefficients.
Plugging Eqs. \eqref{eq:fpemean} and \eqref{eq:fpevar} into Eq. \eqref{eq:covariancetimeevol} we arrive at a compact expression for the time evolution of the covariance
%\begin{equation}
%  \label{eq:fpesigma}
%  \partial_t\tens{\sigma} = 2\left\langle\tens{D}^{(2)} \right\rangle + \left\langle \vec{x}\otimes\vec{D}^{(1)} \right\rangle - \left\langle \vec{x} \right\rangle\otimes\left\langle \vec{D}^{(1)}\right\rangle
%\end{equation}
%This allows to rewrite Eq. \eqref{eq:fpesigma} as  %, where  $\bar{\vec{x}} := \langle \vec{x} \rangle$.
\begin{equation}
  \label{eq:fpefdsigma}
  \partial_t\tens{\sigma} = 2\left\langle\tens{D}^{(2)} \right\rangle + \left\langle \delta\vec{x}\otimes\delta\widetilde{\vec{D}}^{(1)} \right\rangle + \left\langle \delta\widetilde{\vec{D}}^{(1)}\otimes\delta\vec{x} \right\rangle.
\end{equation}
%For convenience, let us define $\bar{\vec{x}}:=\left\langle\vec{x}\right\rangle_{\text{eq}}$.
Consider now a linearization around the equilibrium state\footnote{Note that, at equilibrium $\vec{J}=0$ and $P(\vec{x},t) = P_{\text{eq}}(\vec{x})$.}, in which the transport coefficients can be decomposed as
\begin{equation}
  \label{eq:fpefdcoeff}
  \vec{D}^{(i)} = \left\langle\vec{D}^{(i)}\right\rangle + \left.\vec{\partial}_{\vec{x}}\vec{D}^{(i)}\right|_{\text{eq}}\delta\vec{x}+\dots
\end{equation}
Where $i$ can refer to the drift or diffusion coefficients.
Using Eq. \eqref{eq:fpefdcoeff} we can approximate
\begin{equation}
  \delta\widetilde{\vec{D}}^{(1)} = \left.\vec{\partial}_{\vec{x}}\widetilde{\vec{D}}^{(1)}\right|_{\text{eq}}\delta\vec{x} + O\left(\delta\vec{x}^2\right).
\end{equation}
Let us now define the dynamic matrix as
\begin{equation}
  \tens{H} := -\vec{\partial}_{\vec{x}}\widetilde{\vec{D}}^{(1)}(\bar{\vec{x}},t),  
\end{equation}
where $\bar{\vec{x}}:=\left\langle\vec{x}\right\rangle_{\text{eq}}$ represents the equilibrium state of the relevant variables.
So that
\begin{equation}
  \label{eq:fpefdhd}
  \delta\vec{x}\otimes\delta\widetilde{\vec{D}}^{(1)} = -\tens{H}\left(\delta\vec{x}\otimes\delta\vec{x}\right) + O\left(\delta\vec{x}^3\right).
\end{equation}
In a stationary or equilibrium state we have $\partial_t\left.\tens{\sigma}\right|_{\text{eq}} = 0$. Replacing Eq. \eqref{eq:fpefdhd} into \eqref{eq:fpefdsigma} in equilibrium yields
\begin{equation}
  \label{eq:fpefdbal}
  2\left\langle\tens{D}^{(2)}\right\rangle = \tens{H}\tens{\sigma}_{\text{eq}} + \tens{\sigma}_{\text{eq}}\tens{H}^T.
\end{equation}
This key relation is known as the Fluctuation-Dissipation Balance, and it can be also used in steady, yet non-equilibrium, states~\cite{Zarate2006}.

We can now go a little bit further and investigate the evolution of the relevant variables during a small time interval, $dt$, which will allow us to derive a \gls{SDE}.
Let us assume that we know the values of the relevant variables at some initial point in time, $t_0 = 0$, so that $P(\vec{x}, t_0) = \delta(\vec{x}-\vec{x}_0)$ (where $\vec{x}_0 :=\vec{x}(t_0)$).

If the time interval, $dt := t-t_0$, is small enough, we can safely assume that the transport coefficients remain mostly constant ($\left\langle\vec{D}^{(i)}(\vec{x},t)\right\rangle_{t\in[0,t_0]}=\vec{D}^{(i)}(\vec{x}_0)$). Furthermore, we can neglect any terms beyond $dt$. 
Making use of the definition of derivative, we can now write Eq. \eqref{eq:fpemean} as

\begin{equation}
  \label{eq:fpesdemean}
  \left\langle \vec{x}\right\rangle(t_0+dt) = \vec{x}_0 + \widetilde{\vec{D}}^{(1)}(\vec{x}_0)dt.
\end{equation}
On the other hand, we have that $\tens{\sigma}(t_0) = 0$ since we know $\vec{x}_0$ exactly. Using Eq. \eqref{eq:fpesdemean}, \eqref{eq:fpevar} and the definition of covariance in \eqref{eq:covariance} we can approximate
\begin{equation}
  \label{eq:fpesdefluc}
    \tens{\sigma}(t_0+dt) = \left\langle \delta\vec{x}\otimes\delta\vec{x}\right\rangle = 2\tens{D}^{(2)}(\vec{x}_0)dt + O(dt^2).
\end{equation}
Finally, we can decompose any function into its average and a fluctuation, so that $d\vec{x} = d\left\langle\vec{x}\right\rangle + \delta\vec{x}$. We can then write an \gls{SDE} (using the Itô interpretation\cite{Cohen2015}) for the relevant variables
\begin{equation}
  \label{eq:fpesde}
  d\vec{x} = \vec{x}(t_0 + dt) - \vec{x}_0 = \widetilde{\vec{D}}^{(1)}dt + \delta\vec{x}.
\end{equation}
Where the fluctuating part must satisfy Eq. \eqref{eq:fpesdefluc}. Furthermore, the diffusion and drag terms in the dynamic matrix, $\tens{H}$, are related via the fluctuation-dissipation balance in Eq. \eqref{eq:fpefdbal}.

In future chapters, we will apply the Fokker-Planck formalism to the different levels of detail described in Fig. \ref{fig:multiscale} and its related discussion.
%\todo{The math in this section is confusing. add some references about FP and the relation with the SDE}
\subsection{Einstein relation}\label{sec:einstein}
An extremely important relation between the transport coefficients appearing in $\vec{D}^{(1)}$ and the diffusion $\tens{D}^{(2)}$ can be derived from the equilibrium state, $\vec{J} = 0$ in Eq. \eqref{eq:fpeflux}. This relation was first defined by Einstein\cite{Einstein1905} with arguments similar to those exposed in what follows.

Consider the zero flux condition $\vec{J}=0$, which is solved by the equilibrium distribution $P_{\text{eq}}(\vec{x})$ (independent of time)
\begin{equation}
  \label{eq:fluxeq}
  \vec{J} = \vec{D}^{(1)}P_{\text{eq}} - \vec{\partial}_{\vec{x}}\cdot\left(\tens{D}^{(2)}P_{\text{eq}}\right) = 0
\end{equation}
Thus
\begin{equation}
  \label{eq:fluxeqln}
\vec{\partial}_{\vec{x}}\ln \left(P_{\text{eq}}\right) = \frac{\vec{D}^{(1)}(\vec{x})}{\tens{D}^{(2)}(\vec{x})}
\end{equation}
Now, quite generally, consider that $\vec{D}^{(1)} = -\tens{M}(\vec{x})\vec{\partial}_{\vec{x}}U(\vec{x})$ where $\tens{M}(\vec{x})$ is the so called ``mobility matrix'' which relates forces ($\vec{F} = -\vec{\partial}_{\vec{x}}U$) with the systematic part (mean) of the displacement $d\langle\vec{x}\rangle = \tens{M}\vec{F} dt$.

Note that $U(\vec{x})$ is the free energy of the system providing the equilibrium distribution $P_{\text{eq}} = \frac{1}{Z}\exp\left[-\beta U(\vec{x})\right]$. This relation can also be written as $\vec{\partial}_{\vec{x}}\ln\left(P_{\text{eq}}\right) = -\beta \vec{\partial}_{\vec{x}} U(\vec{x})$. Inserting $\vec{D}^{(1)} = -\tens{M}\vec{\partial}_{\vec{x}}U$ into Eq. \eqref{eq:fluxeqln} and comparing with the equilibrium result, we conclude that
\begin{equation}
  \frac{\vec{D}^{(1)}(\vec{x})}{\tens{D}^{(2)}(\vec{x})} = \frac{-\tens{M}(\vec{x})\vec{\partial}_{\vec{x}}U}{\tens{D}^{(2)}(\vec{x})} = -\beta\vec{\partial}_{\vec{x}}U
\end{equation}
So that,
\begin{equation}
  \label{eq:einsteinrel}
  \tens{M}(\vec{x}) = \beta\tens{D}^{(2)}(\vec{x})\quad\text{ or }\quad\tens{D}^{(2)}(\vec{x}) = \kT\tens{M}(\vec{x})
\end{equation}
Which is the celebrated Einstein relation. Note that the Einstein relation also relates fluctuations (governed by $\tens{D}^{(2)}$) with dissipation (determined by the inverse mobility, aka friction).

In colloidal systems $\tens{M}(\vec{x})$ is calculated from macroscopic (deterministic) hydrodynamics. For instance, the mobility of a single (isolated) sphere with a no-slip surface moving in a fluid at rest with velocity $\vec{\pvel}$ can be obtained \cite{Dhont1996} by studying the total traction created by the fluid stress on its surface, $\vec{F} = \oint\tens{\sigma}\cdot \hat{\vec{n}}dS$ where $\tens{\sigma}$ is the fluid pressure tensor. The result is the Stokes relation
\begin{equation}
  \label{eq:stokesrel}
  \vec{F} = -M_0^{-1} \vec{\pvel}
\end{equation}
With the so-called ``self-mobility'' being $M_0 = \left(6\pi\eta a\right)^{-1}$ where $a$ is the particle radius and $\eta$ the fluid viscosity\footnote{Note that perfect slip surfaces lead to $M_0 = (4\pi\eta a)^{-1}$.}. Note that the inverse of the mobility, $\xi := M_0^{-1}$, is the so-called friction coefficient.

Combining the Einstein (Eq. \eqref{eq:einsteinrel}) and Stokes (Eq. \eqref{eq:stokesrel}) relations leads to the Stokes-Einstein diffusion coefficient
\begin{equation}
  \label{eq:spherediff}
  D_0 = \frac{\kT}{6\pi\eta a}
\end{equation}
a manifestation of Eq. \eqref{eq:einsteinrel} in the case of a single no-slip sphere moving through a fluid at rest.


\section{\uppercase{{\bfseries M}}olecular \uppercase{\textbf{D}}ynamics}\label{sec:dynmol}
Thus far we have used the term ``particle'' without explicitly defining it. We interpret a particle as an arbitrarily coarse-grained simulation unit (atoms, groups of atoms, colloids, groups of colloids, buckets of water, planets...). In this sense, the term molecular dynamics does not refer to the numerical simulation technique (described in sec. \ref{sec:md}). Rather, it should be interpreted as \emph{dynamics of molecules}, or more appropriately, \emph{particles}.

In \uammd particles are the relevant simulation entity as specified by the \emph{ParticleData} class (see section \ref{sec:particledata}). When a fluid is present (either implicitly or explicitly) it is only as a medium to move the particles. Hence, \emph{Integrators} work out the time evolution of particles, which interact via \emph{Interactors}. However, our loose description of a particle allows assigning any kind of property to them. For instance, in the simulation of an ideal gas of atoms, describing the positions, velocities and masses of the atoms might be enough. But in a more sophisticated situation, like a solution of charged colloidal particles, we can include forces, energies, charges, torques, angular velocities, radii, etc.



\chapter{Initial remarks and overview}

%\todo{Image:
%  Inner level: design, GPU algorithm in detail, implementation. Library like behavior.
%  Middle: Algorithms providing physical mathematical foundation (not that in depth implementation, only usage).
%  External level: User code, enables to use these tools for applied problems, multiphysics. Examples, polymers, optofluidics, QCM, electro chemical impedance.
%
%  Dashed circles represents ongoing work or features that will not be discussed in this manuscript.
%}
%
This manuscript encases three distinct, somewhat interleaved, narratives: physics, algorithms and their implementations for a GPU (always under UAMMD).
The lion's share of the author's contribution to the field has gone into the implementation (i.e UAMMD), which is not thoroughly discussed here. In UAMMD's online documentation (see Appendix \ref{ch:online}) the weight leans more heavily on implementation. A reader with a special interest in the implementation of a particular module should have the UAMMD code base at hand when going through the module's chapter in this manuscript. The UAMMD codebase is thoroughly documented, many times in an almost pedagogical manner.
Most chapters beyond this point have a corresponding UAMMD module and are organized as follows; first the physical problem to solve is introduced and its mathematical foundations are laid out. Then, the relevant algorithms are discussed. If the discussed algorithm contains some novel aspects, its implementation is described in detail. Regardless, each chapter ends with a usage example of the module under the UAMMD infrastructure.

%We can distinguish between two families of numerical techniques, particle (Lagrangian) and grid (Eulerian-Lagrangian) based methods.
%The second one also uses particles, but the heavyweight of the algorithm is carried out in a grid, as apposed to just the positions of the particles.
%In some way require another, all algorithms have to take into account interactions between particles.

%In chapter \ref{ch:design} we introduced the \emph{Interactor} interface, the conceptual and programmatic entity that embodies the interaction between particles. In the following chapters, we will descend through the \emph{Interactor} branch in Fig. \ref{fig:uammdsketch} and discuss how the concept can be partitioned and specialized in several ways. Figure \ref{fig:uammdsketch_interactors} depicts this next (deeper) level of the \emph{Interactor} branch.
As previously stated the central workpiece in \uammd are the particles (see Fig. \ref{fig:uammdsketch}). The \emph{Integrator} solves the time evolution of the particles and to that end, it requires the driving interactions (e.g. forces) which are, in turn, provided by the \emph{Interactors}. For this reason, from the code perspective ``forces'' and ``velocities'' respectively pertain to \emph{Interactor} and \emph{Integrator} concepts. Grid-based methods generally consist in solving \glspl{PDE} for some spatio-temporal field in a mesh, i.e. in an Eulerian framework. \uammd deploys Eulerian solvers for several purposes: i) to obtain the velocity field of the solvent in Eulerian-Lagrangian hydrodynamics and ii) to solve the force field generated by a distribution of source charges. Both schemes are based on pseudo-spectral methods, which shall be first introduced when explaining hydrodynamic \emph{Integrators} in Chapter \ref{sec:bdhi}. The \emph{Interactor} dealing with electrostatics in \uammd (pseudospectral Poisson solver) is explained later, in Chapter \ref{ch:tppoisson}.

Figure \ref{fig:overview} presents an overview of the different layers of UAMMD, listing most of the tools (modules) that we will discuss in future chapters. These essential tools are enhanced by the power of C++ templates, which allow a user to introduce new potentials, interpolation kernels,... into the existing modules without modifying them. In many instances it is even possible to generalize the scope of these tools. For instance, spreading/interpolation in a non-regular grid (discussed in chapter \ref{sec:ibm}), or abusing a neighbour list's (chapter \ref{sec:nlist}) packing efficiency to perform some novel sparse matrix multiplication. Inside UAMMD, the communication between the different modules (which is limited to a minimum to reduce internal dependencies) is carried out via the root, \emph{ParticleData}, and the other foundational modules already introduced in chapter \ref{ch:design} (see Fig. \ref{fig:uammdsketch}).
\begin{figure}[H]
  \centering
  \includesvg[width=\columnwidth]{gfx/outline}
  \caption[ ]{A bird's-eye of the different conceptual layers of the UAMMD infrastructure. Many (but not all) of the implemented modules are represented inside the red circle, we will discuss all of them through this manuscript. At the innermost level, the library level, we have the basic tools and algorithms on which the rest of the project is supported. The next layer is the solver layer, with modules for particle interaction and dynamics. In general, all these modules are black-box input/output modules that can be used in any other code in a library-like fashion as external accelerators (especially the ones at the library level). Outside the red circle we find the application layer, in which several of the inner tools are joined to study the physics of a new system. For instance, to study the phenomenon of electrochemical impedance we need electrostatics (interactions) and hydrodynamics (dynamics) in a slit channel (chapters \ref{ch:dppoisson} and \ref{sec:dpstokes}, respectively), the modules inside blue circles are connected first to construct the interaction and dynamics modules, which in turn are used to set up a simulation.}
  \label{fig:overview}
\end{figure}
The rest of the manuscript is organized in three main parts: In the first one (part \ref{part:voyage}) we will go through the different techniques used to solve particle interactions and dynamics (as described by the different levels of detail introduced in chapter \ref{sec:multiscale}) with a particular focus on their adaptation and implementation in a \gpu architecture. Whenever a new technique/algorithm is introduced an accompanying code example showing how to use it in \uammd will be present.
 In the second part (part \ref{pt:algo}) we will go through a series of novel algorithms that have resulted from the development of this thesis. Like before, \uammd code examples will always be accompanying the description of the algorithms. Finally, in the third part (part \ref{pt:applications}) we will see a summary of works (publications in scientific journals) in which \uammd has played a role either as a simulation engine or as an accelerator for an already established external code. The manuscript ends with some future directions and conclusions (chapter \ref{ch:conclusions}), listing ongoing research and future endeavors for the \uammd infrastructure.


%\todo{Rafa, something more here?}

\cleardoublepage
 
\newpage

\cleardoublepage

\ctparttext{Let us go through the different numerical techniques that are used to simulate the different ranges within the spatio-temporal landscape.}
\part{A voyage through numerical space-time}\label{part:voyage}


From this point on in the manuscript we will see descriptions of GPU algorithms and code examples written in CUDA/C++. Although the code examples can be mostly omitted without missing the overall message of the text, understanding the jargon, rationale and details of the described algorithms will require at least basic knowledge of the GPU programming model. If the reader is unfamiliar with either the C++ programming language or the GPU he or she can find an introduction to the basic concepts in Appendix \ref{sec:cpp}. While not a full-fledged guide, Appendix \ref{sec:cpp} has been designed to provide the bare minimum required to understand most of the GPU and programming related concepts laid out in the rest of the manuscript.

\chapter{Building an UAMMD module}

Throughout this manuscript, we will typically create a module named ``Module'' using instructions similar to those in code \ref{code:module}. ``Module'' will be called, for instance, \emph{VerletNVE} if it is an \gls{MD} \emph{Integrator} (see sec. \ref{sec:velocityverlet}) or \emph{BondedForces} if it is a ligature (joining particles with springs) \emph{Interactor} (see sec. \ref{sec:bonded}).

\begin{code2}[Template code for the creation of a module.]{label=code:module}
#include<uammd.cuh>
//Additional includes when necessary. The definition of the UAMMD structure can be found in the next example, the code of which should be included here.
//A module named "Module" is created and returned.
auto createModule(UAMMD sim){
  //Some parameter preparations...
  return std::make_shared<Module>(/*Required parameters*/);
}
\end{code2}

The argument of the function \emph{createModule} in example code \ref{code:module}, of type \emph{UAMMD}, is simply an aggregate (a \emph{struct} in C++) of recurrent parameters written for this manuscript. In particular, this structure can be defined as in code example \ref{code:uammdstruct}, which will serve as an implicit preamble code for the rest of the examples in this manuscript\footnote{Incidentally, I often use this same construction when writing simulations using \uammd, as evidenced by the examples in the UAMMD repository.}. It is important to stress that all the code examples in this manuscript can be compiled (with the exception of source code \ref{code:module}) by including the preamble in code \ref{code:uammdstruct} (a.i. copy/pasting it at the start) and defining, inside the \mintinline{\ucpp}{Parameters} struct, the parameters required by the example (for instance, example code \ref{code:verletnve} will require two parameters: \mintinline{\ucpp}{real dt} and \mintinline{\ucpp}{real targetEnergy}).

\begin{code2}[Definition of the UAMMD auxiliary structure created for this manuscript. Other examples making use of the \emph{UAMMD} struct need this snippet as a preamble to compile.]
  {label=code:uammdstruct}
#include<uammd.cuh>
using namespace uammd; 
//An aggregate of parameters required by the relevant modules in the code
struct Parameters{
  //real dt;
  //...
};
//This structure packs a Parameters along with a ParticleData instance
struct UAMMD{
  std::shared_ptr<ParticleData> pd;
  Parameters par;
};
\end{code2}

\chapter{ParticleData}\label{sec:particledata}
\uammd is a \gpu software, which requires special considerations. In particular, given that the \gpu works as a co-processor with its own separated memory space, when allocating or accessing memory we must specify \emph{where} that memory resides. This location can sometimes be subtle when looking at an isolated piece of code. The separation of memory spaces can become a nuisance especially when dealing with quantities that must naturally be accessed from both the CPU and the \gpu.

The \emph{ParticleData} \uammd class\footnote{Note that since \uammd usually exposes a so-called module via a single C++ \emph{class} we will often use the words \emph{class} and \emph{module} as synonyms.} hides the two memory spaces from the user by handling them internally, ensuring that the CPU and \gpu versions of the arrays it provides are synchronized when requested. This is reflected mainly when requesting a particle property from \emph{ParticleData}, which requires to specify both a location (CPU or \gpu) and a usage intention (read, write or both). \emph{ParticleData} can then use this information to provide an up-to-date reference to the data in the correct memory space.
In CUDA/C++, a function is executed on the CPU unless it is explicitly marked as a \gpu one (see Appendix \ref{sec:cpp} for more information about CUDA/C++ programming). Accessing \gpu memory in a CPU function is thus illegal and results in the program crashing.

\emph{ParticleData} can provide handles giving access to the different particle properties.
When requesting a handle to a property from \emph{ParticleData} with the intention of writing the requesting entity becomes the sole owner of it until the handle is released (destroyed or out of scope). If some other part of the code tries to access the same property (while another part still has the write handle) an error will be issued, since this could result in both versions of the property becoming out of sync. The solution is to simply refrain from storing the \emph{ParticleData}-issued handles (for example as members of a class), requesting the handles just before use and destroying them when they are no longer needed. Furthermore, trying to use a handle for a different intention than requested (choosing the GPU as the device but accesing from the CPU or writing to a handle that was requested for reading) will result in either undefined behavior or the code crashing\footnote{Whenever is possible \uammd will throw an exception instead of crashing.}. The example code \ref{code:pd} showcases the basic usage of the \emph{ParticleData} class. In particular, showing how to access and modify particle properties.

\begin{code2}[Creating and using an instance of \emph{ParticleData}.]
  {label=code:pd}
#include<uammd.cuh>
using namespace uammd; 
int main(){
  //Creation requires a number of particles
  int numberParticles = 1e6;
  auto pd = std::make_shared<ParticleData>(numberParticles);
  //An arbitrary particle index
  int someIndex = rand()%numberParticles;
  //A handle to any existing 
  // property can be requested like this
  auto positions = pd->getPos(access::cpu, access::write);
  //Positions are stored as a 4-dimensional number with elements x,y,z,w
  //The fourth element, w, represents a "color" or "type" and is largely ignored by uammd
  //Let's set some position to x=1, y=2, z=3 and w=0:
  positions[someIndex] = {1,2,3,0}; //Legal access
  auto masses = pd->getMass(access::gpu, access::write);
  //This would be an illegal access, since masses was requested for the GPU:
  masses[someIndex] = 1.0;
  auto charges = pd->getCharge(access::cpu, access::read);
  //This would be an illegal access, since charges was requested for reading:
  charges[someIndex] = -1.0;
  return 0;
}
\end{code2}
Adding new particle properties in \uammd amounts to including them in a special list (in the source code \emph{ParticleData.cuh}, to be precise). Many particle properties are already available in this list at the time of writing, such as positions, velocities, forces, mass, radius and many more. Each particle is assigned a name when \emph{ParticleData} is created. \emph{ParticleData} refers to this special property as ``Id'' (and exposes it just like the rest, with a corresponding \mintinline{\ucpp}{getId(access::gpu, access::read);} function). The name, or id, property has the unique quality of being immutable, meaning that it can only be requested for reading. \uammd can under some circumstances change (reorder) the memory location of the particles\footnote{For instance to increase the data locality in memory for particles close in physical space.} (so that a particle named \emph{id} that started at some index \emph{i} would no longer be located at index \emph{i}). Naturally, it is possible to query \emph{ParticleData} for the index of a particle given its name\footnote{The \emph{ParticleData} member function \mintinline{\ucpp}{getIdOrderedIndices();} returns an array that stores the memory location (index) of a particle given its name (aka id).} (the reader can refer to the online \uammd documentation, see Appendix \ref{ch:online}, to learn more about this). Note that in general in a GPU-centric algorithm, when we assign a worker (be it a thread or a thread block) to each particle, given that each worker runs asynchronously with the rest the specific order of the particles in memory becomes irrelevant from an algorithmic point of view.

Besides working as a multi container, \emph{ParticleData} offers a series of functionalities overlooked here for simplicity (the reader can find more information in Appendix \ref{ch:online}). However, it is worth mentioning that \emph{ParticleData} has the ability to spatially hash and sort particles to increase data locality when accesing the particles' properties.

%\todo{Maybe reference to the different chapters in Fig. \ref{fig:uammdsketch_interactors}?}
%The Poisson leaf in Fig. \ref{fig:uammdsketch_interactors} deals with electrostatics and involves a series of numerical techniques that will not be discussed until much later in this manuscript. Thus, we simply mention it here and defer its description to chapter \ref{ch:tppoisson}.


\chapter{A self-contained UAMMD code example}

Before moving on, it is worth laying out here a complete example of an UAMMD simulation in hopes of helping the reader not to loose sight of the framework as a whole. Source code \ref{code:selfcontained} contains the UAMMD code required to perform a constant-energy (NVE ensemble) \gls{MD} simulation of a collection of \gls{LJ} particles.
Note that most of the concepts (both physical and programming ones) in this example have not been introduced in detail yet. In the course of this manuscript we will introduce the necessary tools required for the reader to understand each part of code \ref{code:selfcontained}. It is not the goal of this example to be understood in its entirety at this point of the manuscript, rather offer to the reader an overview, in general terms, of how a complete simulation code using UAMMD might look like.
A quick look at the main function at the end of source code \ref{code:selfcontained} reveals the general structure of the program, with most steps executing one of the functions defined before.

\newpage

\begin{code2}[A constant-energy UAMMD simulation of a collection of Lennard-Jones particles. For simplicity, all parameters are hard-coded (as opposed to, for instance, being read from a file). ]{label=code:selfcontained}
# include "uammd.cuh"
# include "utils/InitialConditions.cuh"
# include "Interactor/Potential/Potential.cuh"
# include "Interactor/NeighbourList/CellList.cuh"
# include "Interactor/PairForces.cuh"
# include "Integrator/VerletNVE.cuh"

using namespace uammd;
using std::make_shared;
using std::shared_ptr;

auto initializeParticles(int numberOfParticles, Box box){
  auto particles = make_shared<ParticleData>(numberOfParticles);
  auto position = particles->getPos(access::cpu, access::write);
  auto initial =  initLattice(box.boxSize, numberOfParticles, sc);
  std::copy(initial.begin(), initial.end(), position.begin());
  return particles;
}

auto createVerletNVE(shared_ptr<ParticleData> particles){
  using Verlet = VerletNVE;
  Verlet::Parameters VerletParams;
  VerletParams.dt = 0.01;
  VerletParams.initVelocities = true;
  VerletParams.energy = 1.0;
  auto integrator = make_shared<Verlet>(particles, VerletParams);
  return integrator;
}

auto  createLJInteraction(shared_ptr<ParticleData> particles,
			  Box box){
  auto LJPotential = make_shared<Potential::LJ>();
  Potential::LJ::InputPairParameters LJParams;
  LJParams.epsilon = 1.0;
  LJParams.sigma = 1.0;
  LJParams.cutOff = 2.5*LJParams.sigma;
  LJParams.shift = true;
  LJPotential->setPotParameters(0, 0, LJParams);
  using LJForces = PairForces<Potential::LJ>;
  LJForces::Parameters interactionParams;
  interactionParams.box = box;
  auto interaction = make_shared<LJForces>(particles, interactionParams, LJPotential);
  return interaction;
}

void runSimulation(shared_ptr<Integrator> integrator){
  int numberOfSteps = 1000;
  for(int step = 0; step < numberOfSteps; ++step) {
    integrator->forwardTime();
  }
}

void writePositions(shared_ptr<ParticleData> particles){
  std::string outputFile = "Lennard-Jones.dat";
  std::ofstream out(outputFile);
  auto position = particles->getPos(access::cpu, access::read);
  const int * index = particles->getIdOrderedIndices(access::cpu);
  out<<std::endl;
  for(int id = 0; id < numberOfParticles; ++id){    
    auto pi = box.apply_pbc(make_real3(position[index[id]]));
    out<<pi<<std::endl;
  }
}

int main(int argc, char *argv[]){
  int numberOfParticles = 100000;
  // Simulation box (periodic by default)
  real L = 128;
  Box box(make_real3(L, L, L));
  // Initial configuration
  auto particles = initializeParticles(numberOfParticles, box);
  // Integration scheme (Verlet)
  auto integrator = createVerletNVE(particles);
  // Add interactions (Lennard-Jones)
  auto lj = createLJInteraction(particles, box);
  integrator->addInteractor(lj);
  // Numerical integration of the equations of motion
  runSimulation(integrator);
  // Output final configuration
  writePositions(particles);
  return 0;
}
\end{code2}

\newpage

\chapter{Particle interactions}\label{sec:interactions}

One of the reasons for our vague definition of particles is that often, from an algorithmic point of view, it does not really matter what a particle represents. Sometimes a particle will be an atom or molecule, other times it will make more sense that our simulation unit is a big colloid, a virus or a whole star.

The important thing about particles is that when several of them are present, they more than often have a certain effect on each other.
Stars in a galaxy attract each other over an infinitely long range of space, while two argon atoms will repel each other at close range.
Even if particles are somehow oblivious to each other, they might interact with some other thing, like fluid particles being repelled by a wall.
Each kind of interaction requires a different approach, and figuring out how to efficiently compute it in a \gpu is a field in itself. 
In this chapter we will see some of the strategies that can be employed and how they are implemented in \uammd. Figure \ref{fig:uammdsketch_interactors} shows the next level of the \emph{Interactor} branch in Fig. \ref{fig:uammdsketch}.
Most \uammd \emph{Interactors} are generic (i.e. they can be externally specialized), indicated in Fig. \ref{fig:uammdsketch_interactors} in blue.

We start with the case of particles interacting with each and every other in chapter \ref{sec:nbody}. In chapter \ref{sec:shortrange} we discuss the optimization opportunities that arise when this interaction has a short range (compared to the typical particle size). After that, in Chapter \ref{sec:bonded} we explore the case of particles being joined by spring-like ligatures (be it in pairs, triplets, quadruplets, etc).
%Finally, in chapter \ref{sec:external} we briefly discuss the simple case of particles interacting with an unespecified external influence that does not depend on the rest of the particles.

\begin{figure}[h]
  \centering
  \includesvg[width=\columnwidth]{gfx/sketchUAMMD_interactors}
  \caption[ ]{The next level in the \emph{Interactor} branch of the tree presented in Fig. \ref{fig:uammdsketch}. Names in blue represent \uammd modules that can be specialized with outside logic. For instance, the bonded interactions module is general to any arbitrarily complex potential, which can be provided to the module via the use of metaprogramming.}
  \label{fig:uammdsketch_interactors}
\end{figure}

\begin{figure}[h]
  \centering
  \includesvg[width=0.7\columnwidth]{gfx/sketchUAMMD_interactor_slr}
  \caption[ ]{Overview of the concepts in the following chapters. Green text represents template arguments, while generic algorithms are purple. Long- and short-ranged interactions can be specialized from the outside via the \emph{Potential} interface (see Appendix \ref{sec:transverser}). Arrows denote logical dependence.}
  \label{fig:uammdsketch_interactors_slr}
\end{figure}

Let us start by describing \uammd's \emph{Interactor} interface in detail and then proceed to descend to the \emph{Interactor} logical branch in Fig. \ref{fig:uammdsketch}, for which the next downward leaves are depicted in Fig. \ref{fig:uammdsketch_interactors}. In particular, we will focus on discussing the long-range branch in chapter \ref{sec:nbody} and the short-range branch in chapter \ref{sec:shortrange} (depicted in figure \ref{fig:uammdsketch_interactors_slr}).
\section{The Interactor interface} \label{sec:interactor}

\emph{Interactor} encapsulates the concept of a group of particles interacting, either with each other or with some external influence.
An \emph{Interactor} can be issued to compute, for each particle, the forces, energies and/or virial due to a certain interaction.
To do so it can access the current state of the particles (like positions, velocities, etc) via \emph{ParticleData}.
A minimal example of an Interactor can be found in code \ref{code:interactor}.
\begin{code2}[The basic outline of a new \emph{Interactor}. A lot of advanced functionality has been omitted here for simplicity, refer to Appendix \ref{ch:online} for more information. See chapter \ref{sec:particledata} for more information about how to access particle properties.]
  {label=code:interactor}
#include<uammd.cuh>
#include<Interactor/Interactor.cuh>
using namespace uammd;

//A class that needs to behave as 
// an UAMMD Interactor must inherit from it
class MyInteractor: public Interactor{
  public:
  //The constructor must initialize the base Interactor class, for which a ParticleData instance is required.
  //Other than that, it can take any necessary arguments (such as a group of parameters).
  MyInteractor(std::shared_ptr<ParticleData> pd):
          Interactor(pd, "MyInteractor"){
    //Any required initialization 
  }

  //An Interactor can be issued, mainly
  // by Integrators, to sum
  // forces, energies and/or virial
  // on the particles
  void sum(Computables comp, cudaStream_t st) override{
    //"sys" and "pd" are provided by the Interactor base class
    sys->log<System::MESSAGE>("Computing interaction");
    if(comp.force){
      //Sum forces to each particle
      //For instance, adding a force to the x coordinate
      // of the first particle
      auto forces = pd->getForces(access::cpu, access::write);
      forces[0].x += 1;
    }
    if(comp.energy){
      //Sum energies to each particle
    }
    if(comp.virial){
      //Sum virial to each particle
    }
  }
};

\end{code2}

The \emph{Computables} type in the \emph{sum} function simply contains a list of boolean values describing the needs of the caller (which will typically be an \emph{Integrator}). As of today, an \emph{Interactor} can be asked to compute only forces, energies and or virials\footnote{Where the virial of a particle $i$ is defined as $T_i=\half\sum_j\vec{F}_{ij}\cdot\vec{r}_{ij}$.} is defined acting on the particles. The \emph{Computables} structure exists also to facilitate the future inclusion of additional quantities to the \emph{Interactor} responsibilities.

Note that \emph{Interactor} is what is called a \emph{pure-virtual} class in C++ (and programming in general). This means that \emph{Interactor} is not a class that can be used by itself (such as, for instance, \emph{ParticleData}). It is a conceptual base class that must be inherited\footnote{See Appendix \ref{sec:cpp} for more information about C++ classes an inheritance.}, as in code snippet \ref{code:interactor}. The same way a \emph{dog} is a type of \emph{animal}, \emph{MyInteractor} in code \ref{code:interactor} is a type of \emph{Interactor}. We will see other examples shortly.

Any class inheriting from \emph{Interactor} will have access to an instance of \emph{System} with the name \emph{sys}, that can be used to query properties of the GPU and log messages, and a \emph{ParticleData} instance with the name \emph{pd}.

It is worth mentioning that the \emph{Interactor} interface offers, in addition to what is showcased in code \ref{code:interactor}, a series of optional functionalities, omitted here for simplicity, allowing more sophisticated communication between modules. The reader can learn about these extra capabilities through \uammd's online documentation (see Appendix \ref{ch:online}).

\subsection{The simulation domain}
Particles are often located inside a certain domain. In each direction, we distinguish between the domain being open or periodic. In the first case, particles near opposite walls of the domain will not interact. This can be used to model either a truly open boundary system (if the domain length is infinite) or in general a non periodic domain (e.g. due to the presence of a repulsive wall).
\begin{figure}[h]
  \centering
  \includesvg[width=0.6\columnwidth]{gfx/pbc}
  \caption[ ]{A representation of periodic boundary conditions. The unit cell, containing the system, will interact with the surrounding copies of itself if periodic boundary conditions are in place. Given two particles in the unit cell, if the \gls{MIC} is considered, they will only interact with the closest images of each other (included the one in the unit cell).}
  \label{fig:pbc}
\end{figure}

On the other hand, periodic domains are common in numerical simulation as a way of approximating a large system while simulating only a small region of it (called unit cell, see figure \ref{fig:pbc}). In a domain with periodic boundary conditions, particles in the unit cell interact with the particles in the system images. Usually, particles are made to interact only with the closest image of the other particles (so a pair of particles near opposite walls of the system see each other ``through'' the wall). The so-called \gls{MIC} is used to determine the distance of the closest image, the algorithm is laid out in algorithm \ref{alg:mic}.
%\subsection*{Periodic Boundary Conditions}
%\todo{This goes in a section called Domain in UAMMD. Explain periodic vs open etc.}
%\todo{This goes to ParticleData chapter, as part of an example}
%In order to conserve the total volume of the system we use the \gls{MIC}, where a particle that leaves the simulation domain though one side appears on the other.

\begin{algorithm}
  \caption[ ]{Minimum Image Convention, takes a position or displacement vector and returns the coordinates of the image the simulation domain}
  \label{alg:mic}
  \begin{algorithmic}[1]
    \Function{MIC}{r, L}   
    \State \Return r - floor(r/L + 0.5)*L
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\subsection*{Use in UAMMD}
In \uammd, the \emph{Box} class holds information about the domain. When required, individual modules will request a \emph{Box} instance as a parameter (for instance, short ranged interactions require a box if the domain is periodic). Source code \ref{code:box} contains a usage example of the \emph{Box} class
\begin{code2}[Using the Box class.]{label=code:box}
#include<uammd.cuh>
using namespace uammd;
int main(){
  real lx, ly, lz;
  lx = ly = lz = 32.0;
  //A Box requires the size of the domain in each direction, which can be infinite
  Box box({lx, ly, lz});
  //Periodicity can be set independently for each direction. 1 meaning periodic and 0 aperiodic
  box.setPeriodicity(1,1,0);
  //The unit cell goes from -L/2 to L/2 in each direction.
  //Let's store in a variable a position just outside the unit cell.
  real3 position_outside_box = {0.5*lx+1, 0, 0};
  //Given that we have set the box as periodic in X, the position will be folded to the other side of the domain
  real3 position_in_box = box.apply_pbc(position_outside_box);
  //position_in_box holds {-lx*0.5+1,0,0}
  return 0; 
}
\end{code2}

\chapter{Long range interactions}\label{sec:nbody}


Say we are set to simulate a galaxy, so far away from others that it can be safely assumed their action is negligible. Say we want to simulate the dynamics of each of the $N$ stars in this galaxy. Gravity is the dominating interaction between each star, sadly it decays slowly enough to be considered an infinitely ranged one. The gravity potential can be written as follows
\begin{equation}
  \label{eq:gravity}
  U(r) = G\frac{m_1m_2}{r}
\end{equation}
Where $G$ is the gravitational constant and $m_1$, $m_2$ are the masses of each particle.
In this case, if we want to compute the force acting on a star due to the presence of all the others we must check each and every one of them. This leads to lots of interactions to check, more precisely $N(N-1)$ of them (see Fig. \ref{fig:nbodycon}). Of course, there are more sophisticated ways of performing such a computation, such as fast multipole methods \cite{Greengard1987} or Ewald splitting (which will be introduced in section \ref{ch:tppoisson}). But some times these techniques are not a possibility so it is valuable to see how to efficiently handle this computation.
Furthermore, this problem is a really good fit for a \gls{GPU} as we are going to see.
\begin{figure}[h]
  \centering
  \includesvg[width=0.75\columnwidth]{gfx/nbodycon}
  \caption[ ]{Five particles (black circles) must interact with every other (represented by red lines) in a NBody interaction. There are $N(N-1)=20$ lines, since self interactions are not depicted here.}
  \label{fig:nbodycon}
\end{figure}

\section{The NBody algorithm}
Algorithm \ref{alg:nbodynaive} summarizes the naive parallel approach. With the naive approach we assign a particle (or a group of them) to a thread. Then, each thread iterates over all the other particles.
\begin{algorithm}[H]
  \caption[ ]{Naive NBody algorithm. Each particle, i, visits all the others.}\label{alg:nbodynaive}
  \Input{A list of $N$ particles}
  \begin{algorithmic}[1]
    \Require A thread is launched per particle    
    \State $i \gets$ thread ID \Comment{Particle index}
    \For{ $j=0$ until $N$}
    \LeftComment{Process $i$-$j$ pair}
    \EndFor
  \end{algorithmic}
\end{algorithm}
This is an embarrassingly parallel operation in which, in principle, a thread can work without collaborating with the others. However the naive algorithm is missing an opportunity in doing so. We can leverage that all threads have to access the same segments of memory. Instead of letting each thread diverge we can enforce that all threads access the same particles at the same time. This can effectively reduce the number of global memory accesses to a certain particle from $N$ to, potentially, just one.
This algorithm is based on the \emph{nbody} algorithm originally devised by NVIDIA\cite{Nguyen2008,Wilt2013}. It leverages the shared memory capabilities of the \gpu\footnote{See Appendix \ref{sec:cpp} for information about the different memory spaces in the \gpu.}.
Each thread is assigned to a particle, with threads inside a block having consecutive particles.
Groups of particles (called tiles) are then loaded collaboratively by the threads in the block into shared memory.
The number of particles per tile is restricted by the amount of shared memory available\footnote{Thus far, are CUDA-capable cards above architecture 3.0 provide 48KB of shared memory per thread block. In single precision, the 3D coordinates of a particle take $12$ bytes, limiting the tile size to 4096 particles. In practice, we set the tile size equal to the number of threads per block (typically 128) and allow the possibility of storing other particle properties into shared memory (mass, velocity,...).}.

Then the tile is processed by the thread block, in principle, with all threads accessing the same shared memory elements in lock-step.
This is highly beneficial, since global memory accesses incur a high latency (in the order of hundreds of clock cycles). In contrast, loading a value from shared memory yields a near register-memory performance ($\sim 4$ cycles for shared memory and $\sim 1$ cycle for register memory). Since each thread's global memory accesses are reused by the rest, the latency of the former is hidden. In particular, if the tile size is equal to the number of threads in a block, $N_{th}$, (so each threads loads only one particle), the global memory latency is effectively reduced $N_{th}$-fold.

Once the tile has been processed, the next tile is loaded. This process is repeated until no particles remain (note that the last tile can potentially be smaller than the rest).
The optimal size of a tile will be an optimization parameter also depending on the required shared memory per particle, being the number of threads in a block (or a multiple of it) a good default.
This results in an overall speedup of 30 compared with the naive algorithm.
The algorithm is summarized in algorithm \ref{alg:nbody} and a schematic representation of the process is presented in Fig. \ref{fig:nbody}.
\begin{algorithm}[H]
  \caption[ ]{Shared memory NBody algorithm GPU kernel. Although the number of particles per tile is unconstrained, for simplicity this pseudocode assumes a tile has a size equal to the number of threads per block, with a number of tiles equal to the number of thread blocks.} \label{alg:nbody}
  \Input{A list of $N$ particles (positions, velocities,...)}
  \begin{algorithmic}[1]
    \Require
    \Statex A group of $N_b$ thread blocks with $N_{th}$ threads per block is launched.
    \Statex $N_bN_{th}\ge N$.
    \Statex A shared memory array with $N_{th}$ elements.
    \Ensure
    \Statex Threads with index larger than $N$ do not participate.
    \State i $\gets$ thread ID \Comment{The index of the particle asigned to this thread.}
    \State tid $\gets$ mod($i$, $N_{th}$) \Comment{Index of thread in the block.}
    \For{tile $=0$ until $N_b$}
    \State loadId $\gets \text{tile}\cdot N_{th}+$tid
    \State shared$[$tid$]\gets$ particles$[$loadId$]$
    \State Synchronize threads in block. \Comment{Ensures the entire tile is loaded.}
    \For{counter $ =0$ until $N_{th}$}
    \State $j \gets \text{tile}\cdot N_{th} + \text{counter}$
    \State pj $\gets$ shared$[$counter$]$\Comment{Read particle $j$ from shared memory.}
    \State{Process $i$-$j$ pair}
    \EndFor
    \State Synchronize threads in block. \Comment{Ensures shared memory can be rewritten}
    \EndFor
  \end{algorithmic}
\end{algorithm}

Note that the computation as described here is not restricted to computing forces or energies between particles, it might be used for widely different computations that can be encoded as an nbody operation (like a dense matrix-vector product).
\uammd acknowledges this generality via the \emph{Transverser} interface (see Appendix \ref{sec:transverser}), that can be used to specialize these algorithms.

\begin{figure}[H]
  \centering
  \includesvg[width=0.75\columnwidth]{gfx/nbody}
  \caption[ ]{Representation of algorithms \ref{alg:nbodynaive} (upper half of the figure) and \ref{alg:nbody} (bottom half). Two thread blocks ($b_{0,1}$), with two threads each ($t_{0,1,2,3}$), have to process four particles ($p_{0,1,2,3}$). Each thread must go through all particles and fetch some arbitrary information for each of them (i.e. positions, velocities and/or any other data required by the computation), represented by colored lines. The naive algorithms does not make use of the shared memory space available to threads in the same block and thus each thread needs to read all particles. On the other hand, the shared algorithm benefits from the shared memory space. In particular, each particle must be fetched only once per block (as opposed to once per thread in the naive algorithm). This is depicted by the reduced number of colored lines in the lower side of the figure.}
  \label{fig:nbody}
\end{figure}

\subsection*{Use in UAMMD}
There are three ways to access this algorithm depending on the usage:
\begin{enumerate}
\item Processing a group of particles using a \emph{Transverser} outside the \uammd ecosystem (without \emph{ParticleData})\footnote{A \emph{Transverser} applies a series of operations to each item in a list, taking advantage of the particularities of GPU computing described above. See Appendix \ref{ch:transverser} for instructions on how to craft a \emph{Transverser}.}.
\item Applying a \emph{Transverser} to the particles in \emph{ParticleData}.
\item As an \emph{Interactor} to compute forces, energies and/or virials. This is done by providing the \emph{PairForces} module with a \emph{Potential} \footnote{A \emph{Potential} encases a \emph{Transverser} specifically tailored to compute forces, energies or virials. See Appendix \ref{ch:transverser} and related examples for more information.} with an infinite cut-off distance. We will come back to \emph{PairForces} in the chapter for short-ranged interactions, as it can also work with short ranged potentials.
\end{enumerate}
An example of each one is available in code \ref{code:nbody}.
The \emph{Transverser} and \emph{Potential} interfaces are described in detail in Appendix \ref{ch:transverser}.

If the \emph{NeighbourCounter} example in Appendix \ref{ch:transverser} is given as a \emph{Transverser}, the number of particles will be stored in the result of each particle.

While the first two functions in source code \ref{code:nbody} actually perform a computation (as encoded in the provided \emph{Transverser}), the third merely creates an \emph{Interactor} (in particular, of type \emph{PairForces}). In order for the \emph{Interactor} to compute forces, energies and/or virials, its member function \mintinline{\ucpp}{sum} must be called, either directly or by an \emph{Integrator}.

Source code \ref{code:nbody} is the first one in this manuscript that provides a function creating an actual instance of an \emph{Interactor} (in this case, \emph{PairForces}), to place it into context, Appendix \ref{ch:fullexample} provides an example showcasing how to put together an \emph{Integrator} and an \emph{Interactor} to construct a simulation.

\begin{code2}[Usage examples of the three different ways the NBody algorithm is exposed in \uammd. The \emph{Transverser} and \emph{Potential} interfaces are described in Appendix \ref{ch:transverser}. Note that nothing in this example hints at the computation being long ranged, besides of the fact that the NBody algorithm is being used. While this ensures that every pair of particles in the system will be visited, the \emph{Transverser} (or \emph{Potential}, any of which will provide the actual computation logic/physics) can simply choose to, for instance, ignore any pair further than a certain distance.]{label=code:nbody}
#include<uammd.cuh>
using namespace uammd;

#include<Interactor/NBodyBase.cuh>
template<class Transverser>
void transverseWithoutUAMMD(real4* gpupositions, 
                            int numberParticles,
                            Transverser tr){
  NBodyBase nb;
  nb.transverse(gpupositions, tr, numberParticles);
}

#include<Interactor/NBody.cuh>
//For each particle, applies a Transverser with all other particles.
template<class Transverser>
void tranverseWithNBody(UAMMD sim, Transverser tr){
  NBody nb(sim.pd);
  nb.transverse(tr);
}

#include<Interactor/PairForces.cuh>
template<class Potential>
auto createPairForcesWithPotential(UAMMD sim, Potential pot){
  using PF = PairForces<Potential>;
  return std::make_shared<PF> pf(sim.pd, pot);
}
\end{code2}
%\todo{This code and the explanation are mystifying. At least mention what a transverser does ``A transverser   applies a series of operations to each item in a list, taking    advantage of the particularities of GPU computing described above.''. The code by itself is a little lacking without context. An example using this for something in particular should be present. In particular the third function is just creating an interactor, so it is not performing any computation, this should be made clear. Furthermore, Nothing in the code says that the interaction is long-ranged. The sheer abstraction makes it difficult to see how to use functions in practice.}

Long range interactions in a periodic domain pose a special challenge. Direct summation would require taking into account the periodic copies of the system. On the other hand, this summation might even be conditionally convergent, as is the case with electrostatics. It is necessary to elaborate special algorithms for these cases. In \uammd, algorithms to solve the Poisson equation for electrostatics with triple and double periodicity are available. However, these algorithms rely on a mathematical machinery that will be discussed later on in this manuscript and thus we have delayed their description to chapter \ref{ch:tppoisson}.



\chapter{Short range interactions}\label{sec:shortrange}
Particles interacting closely allow for specific optimization taking advantage of the locality of the required computation.
In reality it can happen that the effect of one particle on another is short ranged in nature, as could be the case for interactions stemming directly from quantum effects such as van der Walls forces. It can also happen that a combination of several long-ranged interactions result in an effective short-ranged one, such as a screened electrostatic interaction (like the DLVO potential). Other times when the interaction cannot be made to decay rapidly using natural arguments we can still partially transform it into a short range one with techniques such as Ewald splitting, which will be discussed in future sections.

The reason for giving short range interactions so much credit is simple: imagine a system with $N$ uniformly distributed particles inside a given domain. If we let each particle interact with every other we need to check $N^2$ pairs of particles as we already saw. On the other hand, restricting the interaction to a certain distance, such that each particle has a number of neighbours $k<<N$ reduces the number of checks to $kN$.

The canonical example of a short range interaction is the Lennard-Jones potential.
\section{The Lennard-Jones potential}\label{sec:lj}
A standard potential employed to model the short range Van-der-Waals interactions is the \gls{LJ} potential \cite{Thol2015}. A rapidly-decaying repulsive radial potential with an attractive tail.
\begin{equation}
  \label{eq:lj}
  U_{LJ}(r) = 4 \epsilon \left[ \left(\frac{\sigma}{r}\right)^{12} - \left( \frac{\sigma}{r}\right)^6 \right] 
\end{equation}
Which results in this expression for the force at a certain distance $\vec{r}$ between two points
\begin{equation}
  \label{eq:ljf}
  \vec{F}_{LJ}(\vec{r}) = -\nabla U_{LJ} = -24 \epsilon \left[ \left(\frac{\sigma}{r}\right)^{7} - 2\left( \frac{\sigma}{r}\right)^{13} \right] \frac{\vec{r}}{r}
\end{equation}
Where $r = ||\vec{r}||$ is the modulus of the distance vector. The characteristic length, $\sigma$, and energy, $\epsilon$, are typically used as natural units in simulations.
\begin{figure}
  \centering
  \includegraphics[width=0.75\textwidth]{lj}
  \caption[ ]{The Lennard-Jones potential described in Eq. \ref{eq:lj}}
  \label{fig:lj}
\end{figure}

Given the rapidly-decaying nature of this otherwise infinite range potential it is standard to truncate it at a certain distance, usually $r_{cut} = 2.5\sigma$. At this distance the potential has a value of $U_{LJ}(r = r_{cut}) = -0.0615\epsilon$.
Ignoring the long range effects of this potential changes the equation of state of a \gls{LJ} fluid in a way that is well understood and tabulated~\cite{Thol2015}. Doing this allows to devise more efficient algorithms that only need to take into account short range interactions.
Finally, in order to avoid jumps in the energy it is also standard to shift the potential in addition to this truncation. This is referred to as the truncated and shifted Lennard-Jones potential
\begin{equation}
  \label{eq:ljts}
  U_{LJTS}(r) =
  \begin{cases}
      U_{LJ}(r) - U_{LJ}(r_{cut}) & r<r_{cut}\\
      0 & r\ge r_{cut}\\                                    
    \end{cases}
\end{equation}
The expression for the force remains as in Eq. \eqref{eq:ljf}, simply truncated at $r=r_{cut}$.

Sometimes we are only interested in the repulsive part of the LJ potential, as a way of modeling hard spheres (note that is only an approximation to the behavior of hard spheres). In order to do so we truncate Eq. \eqref{eq:lj} at its minimum, located at a distance $r_m = 2^{1/6}\sigma$, where $U_{LJ} = 0$. The resulting potential is called Weeks-Chandler-Andersen, or WCA, potential. We will refer to it as $U_{WCA}$.

\section*{Use in UAMMD}

A short range \emph{Interactor} is available in \uammd under the name \emph{PairForces}. We have already seen this \emph{Interactor} module when discussing long ranged interactions in example code \ref{code:nbody}. For a user of this module, the only difference between a long range or short range interaction is the cut-off distance. If it is large enough, the \emph{PairForces} module will decide to use the NBody algorithm instead of a neighbour list. An example of the creation of a \emph{PairForces} module is available in code \ref{code:pf}.

The \emph{PairForces} module can be specialized for a certain \emph{Potential} (see Appendix \ref{sec:potential}) and \emph{NeighbourList} (see chapter \ref{sec:nlist}).

In the following chapters, we will discuss in detail the different neighbour list strategies.
\begin{code2}[Creating and returning a short range interaction module. The \emph{Potential} interface is described in Appendix \ref{ch:transverser}.] {label=code:pf}
#include<uammd.cuh>
using namespace uammd;
#include<Interactor/PairForces.cuh>
template<class Potential>
auto createPairForcesWithPotential(UAMMD sim, Potential pot){
  //Any neighbour list can be used
  using NeighbourList = CellList;
  //using NeighbourList = VerletList;
  //using NeighbourList = LBVHList;
  using PF = PairForces<Potential, NeighbourList>;
  PF::Parameters par;
  //A box can be specified here.
  //Note that periodicity can be set independently 
  // in any direction.
  par.box = sim.par.box;
  //If the box is omitted, it is considered 
  // aperiodic and infinite in the three directions
  //Optionally, an instance of a neighbour list
  // can also be provided as a parameter
  //par.nl = std::make_shared<NeighbourList>(sim.pd);
  return std::make_shared<PF> pf(sim.pd, par, pot);
}
\end{code2}



\chapter{Neighbour Lists}\label{sec:nlist}

\begin{figure}[h]
  \centering
  \includesvg[width=\columnwidth]{gfx/nlist}
  \caption[ ]{A depiction of a neighbour list in a section of a particle distribution (left). Particles inside the blue circle (of radius $r_c$) are neighbours of the red particle. The Verlet list strategy (section \ref{sec:verletlist}) defines a second safety radius, $r_s$, that can be leveraged to reuse the list even after particles have moved. In the worst-case scenario of the red particle and another particle just outside $r_s$ approaching each other (right), the list will be invalidated only when each has moved $r_t = \half(r_s-r_c)$ since the last rebuild.}
  \label{fig:nlist}
\end{figure}

\begin{figure}[h]
  \centering
  \includesvg[width=0.7\columnwidth]{gfx/sketchUAMMD_nlist}
  \caption[ ]{The three types of neighbour lists in \uammd. All of them can be used via the same unifying interfaces; \emph{Transverser} (described in Appendix \ref{sec:transverser}) and \emph{NeighbourContainer} (described in sec \ref{sec:ncontainer}). Additionally, the internal structures of each list can be requested for custom use.}
  \label{fig:uammdsketch_nlist}
\end{figure}

Imagine a system composed of a uniform distribution of argon atoms, whose interaction can be quantitatively modeled via the \gls{LJ} potential. When we discussed the gravitational interaction between stars, it made sense to use an efficient algorithm that checks all star pairs in the system. However, in this new system governed by Eq. \eqref{eq:ljts}, checking all pairs becomes pointless given that the vast majority of them will contribute practically nothing to the final result. We would like to skip all these pairs that lie beyond a certain cut off distance, $r_{cut}$, beforehand. A neighbour list contains, for each particle, a list of other particles that are closer than $r_{cut}$ to it. There are a few approaches to constructing neighbour list, and \uammd implements three of them as we will shortly see.

Once a list is constructed, the traversal algorithm depends on the type of neighbour list. However, the logic behind traversal will always be the same (see algorithm \ref{alg:nlist}) and compatible with the \emph{Transverser} interface (which is actually the main reason for its existence). In \uammd neighbour lists can always be used by providing them with a \emph{Transverser}.

\begin{algorithm}
  \caption[ ]{Traversing a neighbour list. Each particle, i, visits all the others in its interaction list.
    In general, instead of launching a thread per particle, it is also possible to launch a thread block per particle and then perform a block reduction to obtain the final result. However, the \emph{NeighbourContainer} interface is restricted to one thread per particle}\label{alg:nlist}
  \Input{A list of $N$ particles, A neighbour list}
  \begin{algorithmic}[1]
    \Require A thread is launched per particle
    \State i $\gets$ thread ID \Comment{Particle index}
    \For{ $j$ in nlist$[i]$}
    \LeftComment{Process $i$-$j$ pair}
    \EndFor
  \end{algorithmic}
\end{algorithm}

This allows to unify any neighbour algorithm into a common interface. However, sometimes we would rather have access to an actual neighbour list. In these cases we generally have three options:
\begin{enumerate}
\item Use the \emph{NeighbourContainer} interface, which provides a pseudo-container mimicking a neighbour list.
\item Use a \emph{Transverser} to construct one with whatever format is needed.
  This is always valid and can be crafted with slight modifications to the neighbour counter example in \ref{code:ncounter} in Appendix \ref{ch:transverser}.
\item Use the internal structures the particular neighbour list construction algorithm provides.
  We will see how to do this in the following sections on a case by case basis.
\end{enumerate}

\section{The NeighbourContainer interface}\label{sec:ncontainer}
A \emph{NeighbourContainer} is an interface that can provide, for each particle, each of its neighbours. It does not provide any guarantees besides this, which means that as long as it provides the neighbours using the common interface, it does not have to actually construct a list\footnote{The CellList in sec. \ref{sec:celllist} actually exploits this by traversing the $27$ cells around a given particle when needed, instead of reading from a precomputed list of neighbour particles}. Contrary to a \emph{Transverser}, instead of providing the neighbour list with a certain logic, a \emph{NeighbourContainer} allows to request the traversal information \emph{from} the list. A summary of the usage of a \emph{NeighbourContainer} can be found in code \ref{code:ncontainer}.

The \emph{NeighbourContainer} interface works under the assumption that the underlying neighbour list can have an internal indexing of the particles and an internal copy of the positions in this order. We will see the rationale of this assumption shortly.
It loosely behaves as a C++ standard container, providing \emph{begin} and \emph{end} methods returning forward input iterators to the first and last neighbours. This means that the neighbour container of a certain particle can only be advanced in sequential order starting from the first neighbour.
By using this interface we can write code that will work for any neighbour list algorithm.

\begin{code2}
  [A CUDA kernel that uses a \emph{NeighbourContainer} to go though all the neighbours of each particle (when launched with as many threads as particles). This code is an implementation of the pseudo code in Algorithm \ref{alg:nlist}.]
{label=code:ncontainer}
//The actual type name of the container
//  will be different for each type of list
template<class NeighbourContainer>
__global__ void traverseNeighbours(NeighbourContainer &nc, int N){
  //This kernel has to be launched with at least N threads so tid goes from 0 to N-1.
  const int tid = blockIdx.x*blockDim.x + threadIdx.x;
  //We have to filter out any surplus threads
  if(tid >= N) return;                                   
  //Set the container to provide the list 
  // of neighbours of particle tid.
  //Note that tid is the internal indexing of the container
  nc.set(tid);
  //Get the group index of the particle:
  const int i = nc.getGroupIndexes()[tid];
  //Get the position of a particle given its internal index
  real3 pos_i = make_real3(nc.getSortedPositions()[tid]);
  //Loop through neighbours
  for(auto neigh: nc){
    const int j = neigh.getGroupIndex();
    const real3 pos_j = make_real3(neigh.getPos());
    //Process pair i-j
    ...
  }
  //Do something with result
  //output[i] = result;
  }
\end{code2}

We are now ready to explore the different neighbour list algorithms, including construction and traversal. Although the latter can be solved via \emph{Transversers} and/or \emph{NeighbourContainers} we will see how each algorithm deals with it. This can be useful for more specialized computations that can leverage the internal workings of the different lists.

Let's start with the naive way to construct a neighbour list; Using algorithm \ref{alg:nbody} to fill a list of particles closer than $r_{c}$ to each other. 
If for some reason we ought to traverse all neighbour pairs several times before the particles change positions this tactic is already a win over simply using algorithm \ref{alg:nbody}. However, it negates one of the reasons for using a neighbour list, reducing the complexity of the overall operation from $O(N^2)$ to $O(kN)$ (with $k<<N$). We can now traverse the list in $O(kN)$ operations, but construction still requires going through every pair anyway.
Still, it is worth to go through the data format of this list since we can make use of it in the future. When it comes to the data format of a neighbour list we can typically find three ways of storing the neighbours of each particle:
\begin{enumerate}
\item Store the list in a particle major format~\cite{Anderson2008}\cite{Plimpton1995}.
  This format stores all the neighbours of each particle contiguously in a matrix of size $N$x$n_{max}$, where $n_{max}$ is a maximum number of neighbours allowed for a single particle. This can result in a lot of wasted space if there are large disparities in the number of neighbours per particle. On the other hand, having all the neighbours stored contiguously can be cache friendly in certain architectures or traversal strategies. The number of neighbours per particle can be encoded in a second list or via a special value in the main list (like $-1$). In a \gpu, if we are to process the neighbours by assigning a thread block to each particle, this can be cache friendly, since contiguous threads in a warp will access contiguous elements in the list.
\item Store the list in a neighbour major format \cite{Berendsen1995}.
  This format has the same storage requirements as the previous one. The only difference is that instead of storing the neighbours for each particle contiguously, we store contiguously one single neighbour for all particles. Starting at the $N$ element we find the next neighbour for every particle and so on until the $n_{max}$ neighbour is reached. The same storage waste concerns are present for this format. In a \gpu, if the traversal is carried out assigning a thread per particle, this is the most cache friendly strategy, since contiguous threads will fetch contiguous elements. In \uammd this is the chosen format when a neighbour list is constructed. 
  As before, the number of neighbours per particle can be stored in an auxiliary array.
\item Compact any of the above lists.%\todo{Who does this?}
  With the first \gpu architectures the thread access patterns (the so-called coherence in CUDA) described in the previous points made a crucial difference. Nowadays the latest architectures are more forgiving about this, furthermore we might need to reduce the memory footprint of the list to the minimum. It is possible to compact any of the above lists so there are no ``empty'' spaces. This can be beneficial when the list has to be downloaded into the CPU, in which case the size of the memory transfer has to be optimized (and will probably dominate the cost of the overall operation). Nonetheless, I am not aware of any major implementation that chooses to do this.
\end{enumerate}

Lets now see the different \gpu algorithms implemented in \uammd.

\section{Cell list}\label{sec:celllist}

\begin{figure}
  \centering
  \includesvg[width=0.65\columnwidth]{gfx/celllist_sketch}
  \caption[ ]{Sketch of the cell list algorithm. Space is binned (black grid) and the bin (cell) of each particle is computed. In order to look for the neighbours of the black particle (those inside the green dashed circle) all the particles inside the adjacent cells (bins inside the orange dashed square, $27$ cells in three dimensions) are checked. The orange particles are therefore false positives. Finally, the yellow particles are never considered when looking for neighbours of the black one.}
  \label{fig:celllist_sketch}
\end{figure}





This algorithm is based on the \emph{particles} algorithm originally devised by NVIDIA and published in the acclaimed book GPU Gems 3\cite{Nguyen2008}. It is reminiscent of the classic CPU linked cells algorithm\cite{Allen2017} and in some ways an adaptation of it to the \gpu architecture. This algorithm is the standard in \gls{GPU} cell list generation and lots of works can be found describing it or variations of it~\cite{Anderson2008}\cite{Dominguez2011}\cite{Howard2016}\cite{Brown2011}. In particular, our approach to building a cell list shares many similarities with the one described in~\cite{Tang2014}.
The main idea behind the cell list is to perform a spatial binning and assign a hash to each particle according to the bin it is in. If we then sort these hashes we get a list in which all the particles in a given cell are contiguous. By accessing, for a certain particle, the particles in the $27$ surrounding cells we can find its neighbours without checking too many false positives, Fig. \ref{fig:celllist_sketch} provides an sketch of the cell list algorithm. Ideally, for a given particle, we would want to check the distance only with the $N_{neigh}$ particles that lie at a distance closer than (or equal to) the cut off distance, $r_{c}$, thus requiring to check a volume of $V_{min}=\frac{4}{3}\pi r_{c}^3$. However, as the cell list partitions the space into cubes of side $r_{c}$, a volume of $V_{cl} = 27r_{c}^3$ around each particle has to be visited.
Assuming the particles are uniformly distributed, the cell list will, on average, check a number of unnecessary particles that scales as $N_{cl}/N_{neigh} = V_{cl}/V_{min} \approx 6$. Although the cell list potentially requires visiting more than $6$ times the number of particles that would strictly count as neighbours, its construction and traversal are so efficient that it is usually worth it.
We are going to describe the algorithm for a rectangular box of side $\vec{L}=(L_x, L_y, L_z)$ with a cut-off distance $r_{c}$. This requires partitioning the domain into a number of cells $\vec{n}=\textrm{floor}(\vec{L}/r_{c}) = (n_x, n_y, n_z)$. Where \emph{floor} represents the largest integer smaller than the argument. It is important to ensure the resulting cell size is at least the cut-off radius, so that $\vec{l} = \vec{L}/\vec{n} \ge r_{c}$.

It is possible to reduce the cell size in exchange for visiting more neighbouring cells ($126$ if $l=r_{c}/2$), which will also reduce the number of neighbour candidates that lie beyond the cut-off distance. However, testing suggests that this is in general not worth the effort~\cite{Anderson2008}.
\begin{figure}
  \centering
  \includesvg[width=\columnwidth]{gfx/celllist1}
  \caption[ ]{Representation of the spatial binning for a 2D distribution of particles. The particle marked as $i$ lies in the cell with coordinates $(1,2)$, these coordinates will then be used to assign a hash to particle $i$. The dotted red line represents the order given by the space-filling curve, starting at the (0,0) cell. Blue particles lie in arbitrary indexes, aka memory locations, (blue numbers). After hashing the positions according to the red dashed line and sorting, particle positions of particles in the same cell are stored contiguously in \emph{sortPos} (right), particles inside the same cell have an undetermined order. Solid red lines mark the end of a cell or, equivalently, the start of the next one, information that is stored in the \emph{cellStart} and \emph{cellEnd} arrays.}
  \label{fig:cl1}
\end{figure}
The algorithm for the cell list construction can be summarized in three separate steps
\begin{enumerate}
\item Hash (label) the particles according to the cell (bin) they lie in.
\item Sort the particles and hashes using the hashes as the ordering label (technically this is known as sorting by key). So that particles with positions lying in the same cell become contiguous in memory.
\item Identify where each cell starts and ends in the sorted particle positions array.
\end{enumerate}
After these steps we end up with enough information to visit the $27$ neighbour cells of a given particle.1
We have to compute the assigned cell of a given position at several points during the algorithm. Doing this is straightforward. For a position inside the domain, $x \in [0, L)$, the bin assigned to it is $i = \textrm{floor}(x/n_x) \in [0, n_x- 1]$. It is important to notice that a particle located at exactly $x = L$ will be assigned the cell with index $n_x$, special consideration must be taken into account to avoid this situation. In particular, in a periodic domain, a particle at $x=L$ should be assigned to the cell $i=0$\footnote{Although this might sound evident, it is an easy-to-miss detail and the consequences resulting from its omission will haunt a naive developer for weeks (or so I am told).}.
Figure \ref{fig:cl1} contains a representation of the binning.
Let's describe now each step in detail
\subsection*{Hashing the particles}

We want to assign to each position a hash that is unique to the cell in the grid in which it lies (see Fig. \ref{fig:cl1}).
These are stored in an auxiliary array holding the hashes for each index in the position array.

One possibility is to simply use the linear cell index as a hash, i.e $\textrm{hash}(i,j,k) := i + (j + kn_y)n_x$.
However, we can leverage that the positions will be sorted according to this hash to improve the data locality for traversal later on. Using the cell index as hash will place particles with the same $x$ coordinate close in the final array, but this is not optimal, since we will have to also explore contiguous cells in $y$ and $z$ which will end up far away.
The usual solution is to use as hash some kind of space-filling curve that tends to assign similar values to cells that are spatially close. The key idea is to find a function that maps the three dimensional cell index to one dimension while preserving their locality. Any space filling curve, like Peano~\cite{Peano1890} or Hilbert~\cite{Hilbert1935} curves, can be used.
We are going to use the Morton hash~\cite{Morton1966}, a popular Z-order curve (see Fig. \ref{fig:cl1}). Construction is fairly straightforward, although the implementation can be challenging to understand.

By assigning to each cell coordinate its binary representation and interleaving the three resulting patterns we get a hash that follows the path depicted in Fig. \ref{fig:cl1}. In particular, we interleave three $10$ bit coordinates in a $32$ bit Morton hash (leaving 2 bits unused), which allows us to encode up to $1024$ cells per direction, enough for most applications. We can achieve this with algorithm \ref{alg:mortonhash}.

This algorithm is comprised of two functions; First a \emph{mask} function that uses a series of bit masks to encode the bits of a number between $0$ and $1023$ in every third bit of a $32$ bit unsigned integer. Then a \emph{mortonhash} function that takes three numbers masked by the first function and interleaves them bit by bit by shifting them accordingly.

As an example, lets say we want to compute a $6$ bit Morton hash from two $3$ bit coordinates. In that case a particle in a 2D cell with coordinates $(i,j)$, encoded as two $3$ bit numbers with bits $(i_0i_1i_2, j_0j_1j_2)$ will first get converted to the two $6$ bit numbers $(i_00i_10i_20, j_00j_10j_20)$ (via two calls to \emph{mask} in algorithm \ref{alg:mortonhash}) and then converted into a Morton hash (via the \emph{mortonhash} function) by interleaving them into a single $6$ bit number, $i_0j_0i_1j_1i_2j_2$.

\begin{algorithm}
  \caption[ ]{Computing a hash from the coordinates of a cell by interleaving three Morton hashes. The symbols $\ll$ (left shift), | (bitwise OR) and \& (bitwise AND) represent the bitwise C operators. } \label{alg:mortonhash}
  \Input{The 3d coordinates of a cell in the grid $(i,j,k)$}
  \begin{algorithmic}[1]
    
    \LeftComment{Interleave three 10 bit numbers into a 32 bit number}
    \Function{mortonhash}{i,j,k}
    \State \Return id $\gets$ mask($i$) $|$ (mask($j$)  $\ll 1$) $|$ (mask($k$) $\ll 2$);
    \EndFunction

    \LeftComment{Encode a 10 bit number in every third bit of a 32 bit one}
    \Function{mask}{i}
    \Ensure i $< 1024$
    \State x $\gets$ i     \Comment{x must be an unsigned 32 bit integer}
    \State x $\gets$ x \& 0x3FF
    \State x $\gets$ (x | x $\ll$  16) \& 0x30000FF
    \State x $\gets$ (x | x $\ll$  8)  \& 0x300F00F
    \State x $\gets$ (x | x $\ll$  4)  \& 0x30C30C3
    \State x $\gets$ (x | x $\ll$  2)  \& 0x9249249
    \State \Return x
    \EndFunction

%    \Ensure
%    \Statex All positions are contained in $[-L/2, L/2]$
%    \State Assign hashes to positions
%    \State Sort positions using hashes as key
%    \State 
  \end{algorithmic}
\end{algorithm}



\subsection*{Sorting the hashes}
Once each particle position has been assigned a hash we sort them. For our use case, we need to sort the positions according to the values in the auxiliary array with the hashes, this operation is known as sort by key. Since all particles in a given cell are assigned the same, unique, hash this operation results in a list of positions such that all the particles in a given cell are guaranteed to lie contiguous in memory.
The most efficient way to perform this sorting operation in a \gpu (as far as the author is aware) is the radix sort algorithm~\cite{Ha2009,Singh2018,Merrill2011}. This algorithm is capable of sorting with an $O(n)$ complexity and its implementation happens to be quite a good fit for a \gpu. In particular, \uammd uses the radix sort implementation in the CUDA library \emph{cub}~\cite{cub}. Radix sort needs to traverse the input a number of times that increases with the number of bits in the keys. Our hashes are encoded in $30$ bit numbers and, depending on the geometry of the grid, the rightmost bit set in the largest hash might be even lower. This allows us to further optimize the sorting process. With our implementation of the Morton hash we can find the largest possible hash simply by evaluating the hash of cell $(n_x-1, n_y-1, n_1-1)$. We can leverage the C function \emph{ffs} (find first set) or \emph{clz} (count leading zero) to find the most significant bit set in an integer.


\subsection*{Constructing the cell list}
By construction, positions lying in the same cell are contiguous in the array of positions sorted by hash. We can construct a cell list by finding the indexes such that the cell of that index is different from the previous one. If we store the indexes for the first and last particle in each cell, we can later use them to access the range of particles in it.
This is an easily parallelizable operation, we can simply assign a thread to each particle and ask whether its cell is different from the cell of the previous particle in the list. If this is the case, we know that this particle is the first in its cell (and store it in the \emph{cellStart} array) and also the last of the previous cell (and store it in the \emph{cellEnd} array). The procedure is listed in algorithm \ref{alg:fillcl}.
\begin{algorithm}
  \caption[ ]{Constructing a cell list from a list of sorted-by-hash positions.} \label{alg:fillcl}
  \textbf{Input:} Grid information, a list of $N$ postions sorted by hash\\
  \textbf{Output:} Arrays cellStart and cellEnd containing the indexes (in the positions array) of the first and last particles in each cell.\\
  \textbf{Requires:} A thread per particle
  \begin{algorithmic}[1]
    \State $i\gets$ thread ID \Comment{Index of a particle}
    \If{id$<$N}
    \State $c_i\gets$ cellIndex(pos$[i]$)
    \State $c_{i-1}\gets$ cellIndex(pos$[i-1]$)
    \EndIf
    \If{$c_i\neq c_{i-1}$ \textbf{or} $i=0$}\Comment{Particle $i$ is the last of it's cell}
    \State cellStart$[c_i]\gets$ id
    \If{$i>0$}
    \State cellEnd$[c_{i-1}]\gets$ id;
    \EndIf
    \EndIf
    \If{$i== N-1$}
    \State cellEnd$[c_i]\gets N$;
    \EndIf
  \end{algorithmic}
\end{algorithm}

The cost of the whole construction operation grows linearly with the number of particles and is quite inferior to the cost of traversing. On the other hand, this algorithm partitions the system into equal-sized bins independent of the particle distribution or sizes. The resulting neighbour list is therefore best suited for uniform distributions of particles with equal, or similar, sizes (interaction ranges).

When large range disparities are present (for instance, a big sphere interacting with a solvent made up of much smaller ones) the cell list requires choosing as cut-off distance the largest interaction range, which will yield many false positives for the smaller particles. This can be alleviated by constructing a list for each interaction range. Then each particle traverses all lists, thus reducing the number of checks. Similar approaches (like stenciled lists) can be found in the literature\cite{Howard2016}.

Another option is to use a different strategy altogether, as we will see in sec \ref{sec:lbvh}.
\subsection*{Use in UAMMD}
There are two main ways to construct a Cell List in \uammd (and the other neighbour lists behave in a similar way):
\begin{itemize}
\item Through the \uammd ecosystem.
  We can provide a \emph{CellList} with a \emph{ParticleData} and let it take care of when rebuilding is needed.
\item Outside the ecosystem.
  Some use cases might benefit from a way to access the algorithm as less intrusively as possible. In particular a user might be interested in constructing the cell list for a list of positions not handled by \emph{ParticleData}.
\end{itemize}
The example code \ref{code:cellist} shows how to construct and traverse a cell list using both methods.

\begin{code2}[The different ways of using a Cell List in \uammd. The \emph{Transverser} interface, already used in chapter \ref{sec:nbody}, is described in detail in Appendix \ref{ch:transverser}.]{label=code:cellist}
#include<uammd.cuh>
#include<Interactor/NeighbourList/CellList.cuh>
using namespace uammd;

//Construct a list using the UAMMD ecosystem
void constructListWithUAMMD(UAMMD sim){
  //Create the list object
  //It is wise to create once and store it
  CellList cl(sim.pd);
  //Update the list using the current positions in sim.pd
  cl.update(sim.par.box, sim.par.rcut);
  //Now the list can be used via the
  //  various common interfaces
  //-Providing a Transverser:
  //cl.transverseList(some_transverser);
  //-Requesting a NeighbourContainer
  auto nc = cl.getNeighbourContainer();
  //Or by getting the internal structure of the Cell List
  auto cldata = cl.getCellList();
}
//Construct a CellList without UAMMD
template<class Iterator>
void constructListWithPositions(Iterator positions, 
                                int numberParticles,
                                real3 boxSize, 
                                int3 numberCells){
  //Create the list object
  //It is wise to create once and store it
  CellListBase cl;
  //CellListBase requires specific cell 
  // dimensions for its construction
  Grid grid(Box(boxSize), numberCells);
  //Update the list using the positions
  cl.update(positions, numberParticles, grid);
  //Now the internal structure of the Cell List
  // can be requested
  auto cldata = cl.getCellList();
  //And a NeighbourContainer can be constructed from it
  auto nc = CellList_ns::NeighbourContainer(cldata);
}
\end{code2}

In both cases the internal structure of the cell list can be requested to facilitate any usage outside the proposed interfaces for neighbour traversing. In the case of the cell list, this means having access to an ordered list containing the indexes of the particle's in each cell. Remembering that the cell list works by binning the domain, assigning a hash to each position according to the bin they are in and then sorting this hash list.
As explained in sec. \ref{sec:celllist}, the cell list algorithm mainly stores two arrays containing the indexes of the particles for each cell.
However, there are some technical implementation details that we will discuss now.
As part of the construction algorithm we need to sort the hashed positions into a, potentially, different order than the input positions. This allows us to easily identify the particles in each cell. Furthermore this has the fortunate side effect of providing us with a sorted positions array with the interesting property that particles that are close in space happen to be close (on average) in memory. In order to take advantage of this possibility, \emph{CellList} constructs the list based on the indexes in this auxiliary array and provides it along the rest of the arrays.
A really good visual representation of the data locality benefits from using these sorted positions for traversal can be found in Fig. 6 of \cite{Tang2014}.

Additionally, if the total number of cells is too large in particle configurations, then that results in a lot of empty cells. The process of filling up the \emph{cellStart} array with a default value (marking the cell as ``empty'') can become a large part of the total runtime. In order to overcome this, instead of cleaning the array (which incurs visiting each element in the array and setting it to a certain value), \emph{CellList} simply invalidates all the list in $O(1)$ time by interpreting any value lower than a certain threshold (called \emph{VALID\_CELL}) as the cell being empty.
This value starts being $0$ in the first update and is increased by the number of particles in each following update. When \emph{VALID\_CELL} is close to overload the maximum value representable by a C++ unsigned integer \emph{cellStart} is then cleaned and \emph{VALID\_CELL} goes back to $0$. So, when the cell list is constructed for the first time, \emph{cellStart} contains values in the range $[0, N-1]$, but the next time (when \emph{VALID\_CELL} is equal to $N$), we will have values in the range $[N, 2N-1]$.
Let's describe the full contents of the \emph{struct} returned by the \emph{CellList} method \mintinline{\ucpp}{CellList::getCellListData()}.
\begin{itemize}
\item\mintinline{\ucpp}{Grid grid}:
  The grid data for which the list was constructed, holds the number of cells $(n_x, n_y, n_z)$ and a series of utility functions to, for instance, find the cell of a particle. Also holds a \emph{Box} object (for applying the minimum image convention).
\item\mintinline{\ucpp}{uint VALID_CELL}:
  The threshold value in the cell list, any value in \emph{cellStart} lower than this encodes the cell being empty. 
\item\mintinline{\ucpp}{real4* sortPos}:
  Positions sorted to be contiguous if they fall into the same cell. 
\item\mintinline{\ucpp}{uint* cellStart}:
  For a given cell with index $(i,j,k)$. The element $icell = i + (j + kn_y)n_x$ stores the index of the first particle in \emph{sortPos} that lies in that cell.
\item\mintinline{\ucpp}{int* cellEnd}:
  Contrary to \emph{cellStart} this array stores the index of the last particle in the cell.
\item\mintinline{\ucpp}{int* groupIndex}
  Given the index of a particle in \emph{sortPos}, this array returns the index of that particle in \emph{ParticleData}.
  This indirection is necessary when something other than the positions is needed (like the forces).
\end{itemize}


\section{Verlet list}\label{sec:verletlist}

This list uses \emph{CellList} to construct a neighbour list up to a distance $r_{s} > r_{cut}$, in this case the list only has to be reconstructed when any given particle has travelled more than a threshold distance,
\begin{equation}
r_t = \frac{r_{s}-r_{c}}{2}.
\end{equation}
See Fig. \ref{fig:nlist}.

Constructing the list has therefore a cost given by the cost of traversing the cell list (but larger due to the extra memory writes). Once it is constructed, the number of false positives will depend on the safety radius, $r_s$. In particular, assuming a uniform distribution of particles, the number of false positives will be given by $N_s/N_{\text{neigh}} = V_s/V_{\text{neigh}} = r_s^3/r_c^3$. In order to get an optimal number of false positives, similar to the ones checked by the cell list (in which $N_{cl}/N_{\text{neigh}} \approx 6$) we would need to set $r_s > 1.8r_c$, which results in a threshold distance of $r_t = 0.4r_c$. Assuming the cut-off radius is the typical one for the \gls{LJ} potential ($r_c \sim 2.5\sigma$, see sec. \ref{sec:lj}) this means that the list does not have to be reconstructed until one particle has diffused more than $r_t \sim \sigma$ from its starting position. Assuming each position update typically moves a particle less than $1\%\sigma$ this means that the list is rebuilt only once every several hundred steps.

On the other hand, the cost of constructing the neighbour list from the cell list also increases with $r_s$. There is therefore a trade-off between the compensated cost of list construction over many steps and the artificially inflated traversal cost. The optimal safety radius should be tuned for each type of simulation by inspecting how the timing depends on it. For instance, in a really dense system where particles are mostly still the optimal safety radius will be wildly different from a gas of rapidly diffusing particles. A good default is usually around $r_s\approx 1.15r_c$. 

In the special case of $r_s=r_c$, which yields $r_t = 0$, the Verlet list can be used as a regular neighbour list.
The list is constructed by storing a private list of neighbours for each particle in a column-major fashion. In order to achieve a cache-friendly memory pattern this ``private'' lists are stored in the same contiguous array. Since we do not know in advance how many neighbours each particle has we set up a maximum number of neighbours per particle, $N_{\text{max}}$, and allocate an array of size $N_{\text{max}}N$ elements. Later on we traverse the list by assigning a thread per particle, which prompts for a column-major layout of the list. That is, threads will tend to read contiguous memory locations if we place the first neighbours of all particles contiguously, then the second, and so forth and so on. A row-major layout (in which we place all neighbours of a certain particle contiguously) will be beneficial if we assign a \emph{block} of threads per particle when traversing.
Measuring is required to know which strategy is best in each case (thread-per-particle vs block-per-particle). UAMMD chooses a column-major format, as testing suggests this is the better choice in our habitual use-cases.\footnote{Nonetheless this fact is abstracted away in the interface and changing between column- and row-major formats can be done easily and without affecting the users code.}

Finally, the maximum number of neighbours per particles, which affects both performance and memory consumption, is autotuned at each update to be the nearest multiple of 32 (the CUDA warp size) of the particle with the greatest number of neighbours.

\subsection*{Use in UAMMD}

As with the \emph{CellList}, \uammd exposes the Verlet list algorithm as part of the ecosystem and as an external accelerator. If the internal option is used the safety factor is automatically autotuned. Regardless, the safety radius can be modified via a member function call (see code \ref{code:verletlist}).

In both cases, accessing the \emph{VerletList} via the common \uammd interfaces (\emph{Transverser} and \emph{NeighbourContainer}) makes it interchangeable with a \emph{CellList}, as evidenced in the example code \ref{code:verletlist}.

On the other hand, we can request \emph{VerletList} to provide its internal structures for manual traversal. In this case, the format of the returned structure is as follows.
\begin{itemize}
\item\mintinline{\ucpp}{real4* sortPos}:
  Positions sorted to be contiguous if they fall into the same cell. Same as with \emph{CellList}.
\item\mintinline{\ucpp}{int* groupIndex}
  Given the index of a particle in \emph{sortPos}, this array returns the index of that particle in \emph{ParticleData} (or the original positions array if the list is used outside the ecosystem).
  This indirection is necessary when something other than the positions is needed (like the forces). Same as with \emph{CellList}.
\item\mintinline{\ucpp}{int* numberNeighbours}:
  The number of neighbours for each particle in sortPos.
\item\mintinline{\ucpp}{StrideIterator particleStride}:
  For a given particle index, $i$, \mintinline{\ucpp}{particleStride[i]} holds the index of its first neighbour in \mintinline{\ucpp}{neighbourList}. \mintinline{\ucpp}{StrideIterator} is a random access iterator\footnote{In particular, in the current implementation this iterator simply returns, for a given index $i$, \mintinline{\ucpp}{maxNeighboursPerParticle*i}.}.
\item\mintinline{\ucpp}{int* neighbourList}:
  The actual neighbour list. The neighbour $j$ for particle with index $i$ (of a maximum of \mintinline{\ucpp}{j=numberNeighbours[i]}) is located at \mintinline{\ucpp}{neighbourList[particleStride[i] + j];}.
\end{itemize}

Note that, as evidenced by its type name, the particle stride (or offset) is not necessarily a raw array but rather a generic C++ random access iterator (that behaves as a raw array for most purposes). For instance the offset might just be the same for all particles. In \uammd's current implementation, the particle stride is simply the maximum number of neighbours per particle (see the initial discussion in chapter \ref{sec:nlist}). However, this interface allows for the possibility of compacting the list in the future, with a similar behavior as the \emph{cellStart} array in \emph{CellList}.

\begin{code2}[The different ways of using a Verlet List in \uammd. The \emph{Transverser} interface, already used in chapter \ref{sec:nbody}, is described in detail in Appendix \ref{ch:transverser}. Note that the \emph{Transverer} and \emph{NeighbourContainer} options are identical to the case of a \emph{CellList}.]{label=code:verletlist}
#include<uammd.cuh>
#include<Interactor/NeighbourList/VerletList.cuh>
using namespace uammd;

//Construct a list using the UAMMD ecosystem
void constructListWithUAMMD(UAMMD sim){
  //Create the list object
  //It is wise to create once and store it
  VerletList vl(sim.pd);
  //Update the list using the current positions in sim.pd
  vl.update(sim.par.box, sim.par.rcut);
  //Now the list can be used via the
  //  various common interfaces
  //-With a Transverser:
  //vl.transverseList(some_transverser);
  //-Requesting a NeighbourContainer
  auto nc = vl.getNeighbourContainer();
  //Or by getting the internal structure of the Verlet List
  auto vldata = vl.getVerletList();
  //The safety radius can be specified
  vl.setCutOffMultiplier(sim.par.rsafe/sim.par.rcut);
  //The number of update calls since the last time
  // it was necessary to rebuild the list can be 
  // obtained
  int nbuild = vl.getNumberOfStepsSinceLastUpdate();
}
//Construct a VerletList without UAMMD
template<class Iterator>
void constructListWithPositions(Iterator positions, 
                                int numberPartivles,
                                real3 boxSize, 
                                int3 numberParticles){
  //Create the list object
  //It is wise to create once and store it
  VerletListBase vl;
  //The safety radius can be specified
  vl.setCutOffMultiplier(sim.par.rsafe/sim.par.rcut);
  //Update the list using the positions
  vl.update(positions, numberParticles, 
            sim.par.box, sim.par.rcut);
  //Now the internal structure of the Verlet List
  // can be requested
  auto vldata = vl.getVerletList();
  //And a NeighbourContainer can be constructed from it
  auto nc = VerletList_ns::NeighbourContainer(vldata);
  //The number of update calls since the last time
  // it was necessary to rebuild the list can be 
  // obtained
  int nbuild = vl.getNumberOfStepsSinceLastUpdate();
}//Construct a VerletList without UAMMD
\end{code2}


\section{LBVH list}\label{sec:lbvh}

The Linear Bounding Volume Hierarchy (LBVH) neighbour list works by partitioning space into boxes according to a tree hierarchy in such a way that interacting pairs of particles can be located quickly. The innermost level of partitioning encases single particles, and boxes sharing faces are hierarchically bubbled together to form a tree structure. In contrast, the cell list partitions space in identical cubes independently of the particles positions or their distribution inside the domain. The ``object awareness'' of the LBVH results in a better handling of systems with large density fluctuations or objects of highly different size\footnote{In contrast, the cell list is not aware of the size of the particles, only accepting a cut off distance. If a system contains particles of different sizes (and therefore interaction cut offs), the cell list must be constructed using the largest one. In the presence of large size disparities (say a set of large particles interacting with tiny ones), a lot of unnecessary particle pairs will be visited (in particular when checking the neighbours of a tiny particle).}.
A full detailed description of the algorithm goes beyond the scope of this manuscript. We provide here is a brief break down, which it is beautifully laid out in detail in references \cite{Howard2016}, \cite{Howard2019} and \cite{Torres2009}, with only minor modifications to them present in UAMMD's implementation.

\begin{enumerate}
\item We start by assigning a different type to each particle based on its size (understanding that if particles have not been assigned a size, they will all have the same type).
\item Then, we sort the particles by assigning a hash to each one in a way such that two given particles of the same type that are close in physical space tend to be close in memory. We achieve this by sorting particles first by type and then by Z-order hash (actually, UAMMD uses the Morton hash presented at \ref{alg:mortonhash}, encoding the type in the last two bits, which are typically unused).
\item The sorted particle hashes are included in a binary tree structure following Karra's algorithm \cite{Karras2012}.
  By including the type in the particle hashing, we can generate a single tree, ensuring that a different subtree is constructed for each type. The root of the subtree for each type can be then identified by descending the tree. This appears to scale well with the number of types when compared to generating an entirely new tree for every type as in \cite{Howard2016,Howard2019}.
\item After, we Assign an Axis Aligned Bounding Box (AABB) to each node of the tree that joins the AABBs of the nodes below it (with the particles, aka leaf nodes, being at the innermost level). This is done using Karra's algorithm \cite{Karras2012}. The AABBs are stored in a ``quantized'' manner that allows to store a node in a single int4, improving traversal time \cite{Howard2019}. The bubbling of boxes stops at the root of every type subtree.
\item Finally, the neighbours of a given particle are found by traversing the AABB subtrees of every type\cite{Torres2009}.
\end{enumerate}
%\todo{Revise grammar. Talk about why types are a thing here}
%\todo{ Perhaps you could mention at the beginning of 8.4 that the LBVH
%     algorithm partitions space into boxes according to a tree hierarchy,
%     in such a way that interacting particles can be located quickly (or
%     some kind of summary like this). }
   
Tree traversal is carried out in a top-down approach, where each particle starts by checking its distance to the root of a given subtree and subsequently descending as needed. If a particle's AABB overlaps a node within a given cut off, the algorithm goes to the next child node, otherwise it skips to the next node//tree.
For a given particle, overlap with the 27 (in 3D) periodic images of the current subtree is computed before traversal of a tree and encoded in a single integer to reduce divergence (except the main box, which is traversed first by default) (see \cite{Howard2019}).
After a type subtree is entirely processed, the process is repeated with next one until none remain.

In my personal experience, the sheer raw power of the cell list in the GPU makes this algorithm not worth the effort in general. Note, however, that this algorithm is bound to outperform the cell list in certain situations, mainly when the size disparities between the different particles in the simulation is pronounced (as in the largest particle having at least twice the size of the smallest) or when the configuration presents a very low density (in terms of the cut-off distance of the interaction).

\subsection*{Usage in UAMMD}

The interface for this neighbour list in UAMMD is more restricted than those of the previously introduced ones. The reason for this being its, yet to be found, applicability in our simulations. Nonetheless, it can be used in any place where \emph{CellList} or \emph{VerletList} can be used.

The internal data structure of the LBVH list can be queried, but we have not discussed in detail the algorithm in this manuscript. A reader who is particularly interested in making use of the LBVH list or a more in-depth understanding of its inner workings is referred to UAMMD's online documentation\footnote{or the code itself, located at the source file \emph{Interactor/NeighboutList/LBVH.cuh}.}(see Appendix \ref{ch:online}).
 
\begin{code2}[Example usage of the LBVH List in \uammd. The \emph{Transverser} interface is described in detail in Appendix \ref{ch:transverser}. Note that the \emph{Transverer} and \emph{NeighbourContainer} options are identical to the case of a \emph{CellList}.]{label=code:lbvhlist}
#include<uammd.cuh>
#include<Interactor/NeighbourList/LBVHList.cuh>
using namespace uammd;

//Construct a list using the UAMMD ecosystem
void constructListWithUAMMD(UAMMD sim){
  //Create the list object
  //It is wise to create once and store it
  LBVHList vl(sim.pd);
  //Update the list using the current positions in sim.pd
  vl.update(sim.par.box, sim.par.rcut);
  //Now the list can be used via the
  //  various common interfaces
  //-With a Transverser:
  //vl.transverseList(some_transverser);
  //-Requesting a NeighbourContainer
  auto nc = vl.getNeighbourContainer();
  //Or by getting the internal structure of the LBVH List
  auto vldata = vl.getLBVHList();
}

\end{code2}
 
\section{Performance comparisons}

It is worth comparing the performance of the three kinds of neighbour list strategies laid out in previous sections. In particular, we will inspect the time each list takes to construct and traverse in order to compute the \gls{LJ} forces (see section \ref{sec:lj}) of a suspension of particles in a periodic domain. We will integrate the temporal dynamics of the system using a Langevin thermostat (which will be introduced in section \ref{sec:langevin}), which will allow us to see the effect of the particles diffusion on the performance of the Verlet list. Since the other strategies (the cell list and the LBVH list) are reconstructed at each step their performance is largely independent on the temperature.

We start with a suspension of particles in an equilibrium configuration and average the time per step during 1000 steps. The time required to update the particle positions (as well as the rest of the computations besides list construction and traversal) is neglegible.

We consider a LJ liquid in a cubic box with periodic boundary conditions and all particles having $\sigma = 2a$, where $a=1$ (particle radius) represents the units of length, and $\epsilon = \kT$ (unless stated otherwise). The interaction is cut off at the standard distance, $r_c=2.5a$. In order to study linear scaling we use a uniform density of particles, either $\rho :=N/L^3= a^{-3}$ or $\rho = 0.1a^{-3}$. The biggest system considered has $N=16777216$ particles with a cubic size of $L=322.54a$ (for $\rho=a^{-3}$) and $L=694.98a$ (for $\rho=0.1a^{-3}$).

The performance of the Verlet list is affected by the safety cut-off radius, $r_s$ (see Fig. \ref{fig:verletlistsafety}). Increasing $r_s$ will cause the list to update more sparingly at the cost of more false-positive neighbour checks and an enlarged memory requirement\footnote{The Verlet list requires to reserve an array of size $N_{\text{max}}$ for each particle, where $N_{\text{max}}$ is, at least, the maximum number of neighbours that a particle has.}. On the other hand, a small value of $r_s$ will result in a more ``optimal'' neighbour list (in memory access pattern, false positive neighbours and size) at the expense of more regular updates. Based on Fig. \ref{fig:verletlistsafety} we choose $r_s = 1.2r_c$ for all tests.
\begin{figure}
  \centering
  \includegraphics[width=0.75\linewidth]{nlistperfmult}
  \caption[ ]{Performance of the Verlet list versus the safety cut-off radius, $r_s$, in a \gls{LJ} fluid simulation. The system has one million particles. Tests ran on an RTX2080 GPU.}
  \label{fig:verletlistsafety}
\end{figure}
We compare the performance of the three neighbour lists strategies in Figure \ref{fig:nlistcomp}. At low system sizes the performance is mostly independent on the number of particles. This is a standard behavior in GPU computing coming from the fact that not all resources in the GPU are being utilized. When the computational load is large enough to fill the GPU (which happens at around 50K particles and beyond) performance exhibits the expected linear scaling, evidencing the fact that all of this strategies show a $O(N)$ complexity. 
\begin{figure}
  \centering
  \includegraphics[width=0.75\linewidth]{nlistperf_dens1}\\
  \includegraphics[width=0.75\linewidth]{nlistperf_dens0.1}
  \caption[ ]{Performance comparison of the different neighbour list strategies in a LJ liquid at two different particle concentrations. The number of steps between reconstructions of the Verlet list is highly dependent on any parameter that modifies the diffusivity of the particles (like the temperature or the time step), which directly affects performance. In contrast, the other lists, being reconstructed every step regardless, are oblivious to the temperature or, in general, to changes in the diffusivity of particles. All tests were carried out in a RTX2080 GPU.}
  \label{fig:nlistcomp}
\end{figure}

The tests in this section constitute a very specific benchmark that can be unfair with, for instance, the LBVH list. In particular, a uniform density benefits the cell list and thus, the Verlet list. On the other hand, the cell list excels when all particles have the same interacting radius ($r_c=2.5a$ in this case) as in these tests. A more in depth comparison of the neighbour list strategies should take into account systems with density and/or cut-off (i.e particles of different radius) disparities. In general, the cell list is the best one-size-fits-all choice, although this decision should be reevaluated on a case by case basis.
Memory requirements are the other consideration when choosing a neighbour list. The Verlet list is always the more memory-hungry option, given that it relays on other implementation to construct the neighbour list (for instance, the cell list) and stores an additional ``private'' list for each particle. The storage for the cell list grows with the total number of cells, $n_{\text{tot}}=(L/r_c)^3$ in a cubic box. Finally, the LBVH is the most lightweight option, as it only requires $O(N)$ storage.


\chapter{Bonded Interactions}\label{sec:bonded}

As bonds can be expected to present great disparity in the number of bonds per particle, instead of using the same strategy as with the neighbour list, for the bonded interactions \uammd generates a compacted list of bonds per particle similar to the cell list.
For each particle, a list of all the bonds it is involved in is stored. Note that this means that the information for a given bond is stored several times (two times for pair bonds, three for angular bonds and so on).
This improves the data locality when traversing the bond list (at the expense of extra storage).
In contrast with the cell list algorithm, the bond list creation algorithm does not have to be particularly efficient since it will be generated only once at initialization\footnote{Creating the bond list only at initialization hinders the ability of the algorithm to efficiently allow for the dynamic inclusion/exclusion of bonds. However, we have not yet devised an efficient and sufficiently generic algorithm for constructing a dynamic bond list.}.

Regarding traversal, as explained in sec. \ref{sec:nlist}, for a compacted list such as this we need to store the bond list itself (containing a list of bonds for each particle) and then two auxiliary arrays; One with offset information (the index in the bond list where the particular list for a given particle is located) and another with the number of bonds for each particle.
Note that it is not necessary to store information about particles that are not involved in any bond. UAMMD employs another auxiliary array storing the indexes of particles that have at least one bond.
The special type of bond that involves a single particle is referred to as a fixed-point bond, usually attaching a particle to a certain point in space. In UAMMD this is encoded by storing the tether point position as part of the bond information.


\section*{Use in UAMMD}
The bond \emph{Interactor} can be specialized for a certain number of particles per bond and a potential. Some potentials are already defined for bonds involving two, three (angular bonds) and four (torsional bonds) particles per bond.

The interface for the potential can be found in source code \ref{code:bonded}. The function \mintinline{\ucpp}{compute} of the bond potential (\mintinline{\ucpp}{HarmonicBond} in this example) requires some additional explanation. This function will be called for every bond read in the bond file and is expected to compute force/energy and/or virial for a certain particle involved in that bond\footnote{Note that this implies that not only the information for each bond is stored several times, but the forces/energies/virials for the same bond are queried independently for each of the involved particles (as opposed to a model in which the bond potential would perform the computation for all particles in the bond at once). Although this might seem like a strange decision at first, it makes sense in the context of a GPU for the same reason that we do not make use of the fact that when pair forces are used $F_{ij} = F_{ji}$. Moreover, the computation of the potential can be wildly different for each particle involved in a bond with, for instance, 4 particles (like a torsional bond).}. The arguments of this function are (see the example code \ref{code:bonded}):
\begin{itemize}
\item \mintinline{\ucpp}{bond_index}: The index of the particle to compute force/energy/virial on.
\item \mintinline{\ucpp}{ids}: list of indexes of the particles involved in the current bond.
\item \mintinline{\ucpp}{pos}: list of positions of the particles involved in the current bond.
\item \mintinline{\ucpp}{comp}: computable targets (wether force, energy and/or virial are needed).
\item \mintinline{\ucpp}{bi}: bond information for the current bond (as returned by the function \mintinline{\ucpp}{readBond}).
\end{itemize}

\begin{code2}[Constructing and returning a Bonded interaction module.] {label=code:bonded}
#include<uammd.cuh>
#include<Interactor/BondedForces.cuh>
using namespace uammd;

//Harmonic bond for pairs of particles
struct HarmonicBond{
  //Place in this struct whatever static information is needed for the different bonds.
  //In this case spring constant and equilibrium distance.
  //The function readBond below takes care of reading each BondInfo from the file.
  //Naturally, the present class can also store any additional information.
  struct BondInfo{
    real k, r0;
  };

  __device__ ComputeType compute(int bond_index,
                        int ids[2], real3 pos[2],
                        Interactor::Computables comp,
                        BondInfo bi){
    real3 r12 = pos[1]-pos[0];
    real r2 = dot(r12, r12);
    const real invr = rsqrt(r2);
    const real f = -bi.k*(real(1.0)-bi.r0*invr);
    ComputeType ct;
    ct.force = comp.force?f*r12:real3();
    ct.energy = 0.0; //Whatever
    ct.virial = 0.0; //Whatever
    return (r2==real(0.0))?(ComputeType{}):ct;
  }

  //This function will be called for each bond in the bond file and read the information of a bond
  BondInfo readBond(std::istream &in){
    //BondedForces will read i j, readBond has to read the rest of the line
    BondInfo bi;
    in>>bi.k>>bi.r0;
    return bi;
  }
};

//Create a bond interactor for bonds with two particles, uses the Harmonic potential defined above.
auto createBondInteractor(UAMMD sim){
  //If the bond involves, for instance, 3 particles, the module should be specialized accordingly below.
  //Furthermore, the potential should be modified to take this into account.
  using BF = BondedForces<HarmonicBond, 2>;
  typename BF::Parameters params;
  params.file = "bonds.dat";
  auto bf = std::make_shared<BF>(sim.pd, params);
  return bf;
}

\end{code2}

As usual, some additional functionality of this module has been omited for simplicity. For instance, a shared pointer to an instance of the bond potential can be provided at the \emph{BondedForces} construction to retain control of it (maybe to mutate the properties of the potential over time). See Appendix \ref{ch:online} for more information.

\chapter{Particle Dynamics}\label{sec:dynamics}

Thus far we have dealt with particle interactions. In the following chapters we will see a series of numerical strategies to solve the time evolution dynamics of the particles in the different levels of description introduced in chapter \ref{ch:design}. We will do so by descending through the Integrator branch in Fig. \ref{fig:uammdsketch}. We can find the leaves below the Integrator branch in Fig. \ref{fig:uammdsketch_integrators}.

In a bottom-up manner, referring to the complexity of the numerical methods, we start with the microscopic level (via \gls{MD}) in sec. \ref{sec:md}, then Langevin in sec. \ref{sec:langevin} and finally the hydrodynamics and Smoluchowski levels in chapter \ref{sec:bdhi}.
  
\begin{figure}
  \centering
  \includesvg[width=\columnwidth]{gfx/sketchUAMMD_integrators}
  \caption[ ]{The next level in the \emph{Integrator} branch of the tree presented in Fig. \ref{fig:uammdsketch}. Different methods are available for each of the levels of description (pink) introduced in chapter \ref{ch:design}. Methods available as \uammd modules are in black.}
  \label{fig:uammdsketch_integrators}
\end{figure}


\section{The Integrator interface}\label{sec:integrator}
We introduced the \emph{Integrator} concept back in chapter \ref{ch:design}, where we adhered to it the responsibility of taking the simulation to the next time step. In this chapter, we will discus in detail \uammd's \emph{Integrator} software interface. Similar to \emph{Interactor}, \emph{Integrator} is a purely virtual C++ class that must be inherited (specialized). Instead of providing functions to compute forces (\emph{Interactor}), \emph{Integrator} exposes functions to take the simulation to the next time step. Additionally, \emph{Integrators} hold a list of \emph{Interactors} that they can use at their leisure to obtain up-to-date forces, energies and/or virials for each particle.

An arbitrary number of \emph{Interactor} instances can be added to an \emph{Integrator} (e.g. short-ranged forces, bonded forces...) via its member function \mintinline{\ucpp}{Integrator::addInteractor();}. Source code \ref{code:integrator} is an example of how to create a novel \emph{Integrator} (parallel to Example \ref{code:interactor} for \emph{Interactors}).

\begin{code2}[The basic outline of a new \emph{Integrator}. Some advanced functionality has been omitted here for simplicity, refer to Appendix \ref{ch:online} for more information. See chapter \ref{sec:particledata} for more information about how to access particle properties. A class inheriting from \emph{Integrator} has access to a series of built-in members, in particular, a \emph{ParticleData} instance is always available under the name ``pd''. Furthermore, a list of \emph{Interactors} is available in the variable ``interactors''. The Integrator defined below implements a simple forward-Euler integration scheme (which will be introduced in sec. \ref{sec:euler}).]
{label=code:integrator}
#include<uammd.cuh>
#include<Integrator/Integrator.cuh>
using namespace uammd;

//A class that needs to behave as 
// an UAMMD Integrator must inherit from it
class MyIntegrator: public Integrator{
  real dt = 0.1; //A time step with a default value
public:
  MyIntegrator(std::shared_ptr<ParticleData> pd):
          Integrator(pd, "MyIntegrator"){
    //Any required initialization,
    //Typically, an Integrator will require
    // a series of input parameters,
    // such as the time step, etc.
  }

  //An Integrator can be issued to take the simulation
  // to the next step in time
  virtual void forwardTime(cudaStream_t st) override{
   //Whatever is required to take particles to the next step.
   { //Reset forces
     auto force = pd->getForce(access::cpu, access::write);
     std::fill(force.begin(), force.end(), real4(0.0));
   }
   //Typically we will issue the interactors to 
   // compute forces, energies and/or virials.
   for(auto i: interactors) i->sum({.force=true}, st)
   //Use computed forces to update positions and vels
   auto pos = pd->getPos(access::cpu, access::readwrite);
   auto vel = pd->getVel(access::cpu, access::readwrite);
   auto mass = pd->getMass(access::cpu, access::read);
   auto force = pd->getForce(access::cpu, access::read);
   for(int i = 0; i<pos.size(); i++){
     vel[i] += force[i]*dt/mass[i];
     pos[i] += vel[i]*dt;
   }
  }
};
\end{code2}

As with the \emph{Interactor} interface (described in section \ref{sec:interactor}), some functionality exposed by the \emph{Integrator} interface has been omitted in this manuscript for simplicity. The reader can learn about these additional capabilities in \uammd's online resources (see Appendix \ref{ch:online}). Nonetheless, the capabilities showcased in codes \ref{code:integrator} and \ref{code:interactor} are sufficient to encode any of the interactions and integration algorithms described in this thesis.


\chapter{Molecular Dynamics}\label{sec:md}

At the lowest level our simulation units are interacting atoms or molecules whose motion can be described using classical mechanics. We refer to the numerical techniques used in this regime as \gls{MD}.
In \gls{MD} molecules are moving in a vacuum following the Newtonian equation of motion. If we need to include some kind of solvent, i.e. water, in an \gls{MD} simulation we must do so by explicitly solving the motion of all the involved molecules of water (with some suitable force field).
Even so, \gls{MD} represents the basis for all particle-based methods, where the term \emph{particle}, depending on the level of coarse graining, might refer to anything from an atom to a colloidal particle.
Although it is not the most fundamental way of expressing the equations of motion, we will stick to a somewhat simplified but still quite general approach. For a system of $N$ molecules interacting via a certain potential, Newton's second law states that the acceleration experienced by each particle comes from the total force, $\vec{F}$, acting on it.
\begin{equation}
  \label{eq:md}
  \vec{F} =  m\ddot{\vec{\ppos}} = m\vec{a},
\end{equation}
where $m$ is the mass of the molecule, $\vec{\ppos}$ its position in cartesian coordinates and $\vec{a}$ its acceleration.
The force can usually be expressed as the gradient of an underlying potential energy landscape, $U$.
\begin{equation}
  \label{eq:mdfv}
  \vec{F} = -\nabla_{\vec{\ppos}} U(\{\vec{\ppos}_1,...,\vec{\ppos}_N\}),
\end{equation}
which in general is a function of the positions of the particles.


At their core, Eqs. \eqref{eq:md} and \eqref{eq:mdfv} are an expression of the conservation of the total energy of the entire system. Consequently, these equations of motion can be used to perform simulations in the so-called microcanonical ensemble (NVE)\footnote{Although other systems can also be simulated with \gls{MD}. For example, \gls{MD} could be used to simulate the dynamics a group of gas molecules inside a piston (in which volume is not conserved).}, where the number of particles (N), the volume of the domain (V) and the total energy (E) are conserved. Note, however, that the energy is only conserved when forces stem from the gradient of a potential.


Our goal is to numerically integrate Eq. \eqref{eq:md}. Starting from the state of the system at a certain time $t$ (where the system is conformed by the positions and velocities of the particles and the forces acting on them) we want to find the state of the system at time $t + \dt$. We refer to $\dt$ as the time step.

The so-called finite-difference methods are the most commonly employed techniques to achieve this.
\section{Finite-difference methods}
\glsreset{PDE}
Finite-difference methods~\cite{Grossmann2007} solve \glspl{ODE} or \glspl{PDE} by approximating the derivative operators with finite differences.
We can employ this idea to solve both spatial and temporal discretized differentiation operators. Thus far we are only interested in solving temporal derivatives, such as the one in Eq. \eqref{eq:md}.
The derivative of a function, $f$, is defined as
\begin{equation}
  \dot{f}(t) = \lim_{\dt\rightarrow 0} \frac{f(t+\dt) - f(t)}{dt}
\end{equation}

If the function is smooth and analytic (well behaved), we can write its Taylor expansion to approximate $f(t+\dt)$
\begin{equation}
  \label{eq:fdmtaylor}
  f(t+\dt) = f(t)+\dot{f}(t)\dt + \half\ddot{f}(t)\dt^2 + O(\dt^3)
\end{equation}
For convenience, we will refer to the function evaluated at time $t$ as $f^n := f(t)$ and as $f^{n\pm 1} := f(t\pm \dt)$ when evaluated at $t\pm \dt$. This notation can be extended to refer to points separated further in time, in general defining $f^{n\pm j} := f(t\pm j\dt)$.
Solving Eq. \eqref{eq:fdmtaylor} for the first derivative
\begin{equation}
 \dot{f}^n  =  \frac{f^{n+1} - f^n}{\dt} - \half\ddot{f}^n\dt + O(\dt^2),
\end{equation}
which approximates the definition of derivative for sufficiently small $\dt$, when only the first term remains.
Most of the integration techniques we will use make use of this trick.
Lets see how we can leverage this technique to solve Eq. \eqref{eq:md}.
\subsection{Euler Methods}\label{sec:euler}

If we truncate Eq. \eqref{eq:fdmtaylor} at order $\dt$ we get the so called Euler method~\cite{Butcher2016}, which approximates the integral as
\begin{equation}
  \label{eq:basiceuler}
  f^{n+1} = f^n + \dot{f}^n\dt
\end{equation}
Which yields a solver with $O(\dt)$ accuracy~\cite{Butcher2016}.
It is worth exploring here a different derivation of the Euler method using the definition of integral instead of derivative. We can apply the fundamental theorem of calculus to get
\begin{equation}
  \label{eq:eulerftc}
  f^{n+1} - f^n = \int_t^{t+\dt}\dot{f}(t')dt'
\end{equation}
On the other hand, we can approximate the integral using the left Riemann sum with only one interval (which is a good approximation as $\dt\rightarrow 0$)
\begin{equation}
  \label{eq:eulerriemman}
  \int_t^{t+\dt}\dot{f}(t')dt' \approx \dt \dot{f}^n
\end{equation}
By combining Eqs. \eqref{eq:eulerftc} and \eqref{eq:eulerriemman} we arrive at \eqref{eq:basiceuler} again.
This derivation will come in handy later when describing integration schemes for equations with terms that cannot be approximated by Eq. \eqref{eq:fdmtaylor}.
Furthermore, Eq. \eqref{eq:eulerriemman} opens the door to a family of numerical integrators based on improving this approximation by using more intervals between $t$ and $t+\dt$. Some examples are the Runge-Kutta methods or the so-called linear multistep methods, such as the Adams Bashforth family of algorithms~\cite{Butcher2016}.
Often these methods improve numerical stability at the cost of evaluating the function derivative (in our case meaning the force and also, often, the velocity which is less expensive computationally) several times per time step. Given that in a \gls{MD} simulation this is, often by orders of magnitude, the most expensive part of the computation we wont be making use of them for the time being.
Finally, there are several modifications to the Euler algorithm that can improve its stability, such as the backward Euler~\cite{Butcher2016} or the exponential Euler\cite{Butcher2016}.
We will expand into Euler methods in section \ref{sec:bd}.
For now, we can use it to integrate Eq. \eqref{eq:md} to get the velocity and the positions
\begin{equation}
  \label{eq:basiceulermd}
  \begin{aligned}
  \vec{\pvel}^{n+1} &\approx& \vec{\pvel}^n + \vec{a}^n\dt\\
  \vec{\ppos}^{n+1} &\approx& \vec{\ppos}^n + \vec{\pvel}^n\dt
\end{aligned}
\end{equation}
Where $\vec{\pvel} := \dot{\vec{\ppos}}$ represents the velocity of the molecule.
The Euler integrator as defined above is not symplectic\cite{Hairer2006} and as such presents several stability issues that make it impractical as compared with other strategies that will be discussed shortly. In particular, not being symplectic makes it unsuitable for our main goal: energy conservation\cite{Hairer2006}. On the other hand it is a first order method and lowering the time step too much to improve accuracy leads to numerical rounding errors (due to the limits of floating point accuracy).
However, we can make a small modification to the Euler method to make it symplectic\cite{Hairer2006}
\begin{equation}
  \label{eq:semiimpliciteuler}
  \begin{aligned}
    \vec{\pvel}^{n+\half} &\approx& \vec{\pvel}^n + \half\vec{a}^n\dt\\
    \vec{\ppos}^{n+\half} &\approx& \vec{\ppos}^n + \half\vec{\pvel}^{n+\half}\dt
  \end{aligned}
\end{equation}
This mid-point scheme leads to the semi implicit Euler scheme. While symplectic, this algorithm effectively requires two force evaluations per step (interpreting step as going from $t$ to $t+\dt$) and it is still a first order method.
In order to efficiently solve Eq. \eqref{eq:md} we need a symplectic and memory-saving algorithm that offers good numerical stability while requiring to evaluate the forces only once per step.
The most widespread algorithm for \gls{MD} is the so-called velocity Verlet method\cite{Allen2017}, a second-order algorithm that needs the forces only once per step.
\section{The velocity Verlet algorithm}\label{sec:velocityverlet}
Instead of truncating Eq. \eqref{eq:fdmtaylor} at $O(\dt)$ as we did with Euler let's now truncate it at $O(\dt^3)$ and write the expressions for the positions at $t+\dt$ and $t-\dt$
\begin{eqnarray}
  \label{eq:verletrp1}
  \vec{\ppos}^{n+1} &=& \vec{\ppos}^n + \vec{\pvel}^n\dt + \half\vec{a}^n\dt^2 + \vec{b}^n\dt^3 + O(\dt^4)\\
  \vec{\ppos}^{n-1} &=& \vec{\ppos}^n - \vec{\pvel}^n\dt + \half\vec{a}^n\dt^2 - \vec{b}^n\dt^3 + O(\dt^4)
\end{eqnarray}
Where $\vec{b}$ is just the term accompanying the $O(\dt^3)$ term, typically refered to as \emph{jerk}.
By adding both equations together and solving for $\vec{\ppos}^{n+1}$ we get
\begin{equation}
  \label{eq:stormerverlet}
  \vec{\ppos}^{n+1} = 2\vec{\ppos}^n - \vec{\ppos}^{n-1} + \half\vec{a}^n\dt^2 + O(\dt^4)
\end{equation}
When written like this, this equation can be used to integrate Eq. \eqref{eq:md} and is known as the Störmer integration method. It can also be seen as an application of the central difference method to the position and as such, Eq. \eqref{eq:stormerverlet} is also known as the explicit central difference method.
Although Eq. \eqref{eq:stormerverlet} presents really good numerical stability ($O(\dt^4)$) at a small cost, it presents some issues that hinder its applicability.
It requires storing the position at two points in time, which is inconvenient and poses the challenge of starting the simulation.
Furthermore it eliminates the velocity from the integration, which can be computed from the positions using the central difference method
\begin{equation}
  \label{eq:stormervel}
  \vec{\pvel}^n = \frac{\vec{\ppos} ^{n+1} - \vec{\ppos}^{n-1}}{2\dt} + O(\dt^2)
\end{equation}
Mathematically this is a second order approximation to the velocity. However, computing the velocity in this way can result in large roundoff errors stemming from the subtraction of two similar quantities.
We can manipulate Eqs. \eqref{eq:verletrp1} and \eqref{eq:stormervel} to arrive at the more commonly used version of the Verlet method, the so-called velocity Verlet algorithm.
We can use the central difference method to evaluate the velocity at half step as $\vec{\pvel}^{n+\half} = \frac{\vec{\ppos}^{n+1} - \vec{\ppos}^n}{\dt}$. By replacing $\vec{\ppos}^{n+1}$ with Eq. \eqref{eq:verletrp1} and truncating at $O(\dt^2)$ we get
\begin{equation}
  \label{eq:verletvelhalf}
  \vec{\pvel}^{n+\half} = \vec{\pvel}^n + \half\vec{a}^n\dt + O(\dt^2)
\end{equation}
Which allows to get a second order approximation of the position by  solving Eq. \eqref{eq:verletvelhalf} for $\vec{\pvel}^n$ and inserting it into Eq. \eqref{eq:verletrp1}
\begin{equation}
  \vec{\ppos}^{n+1} = \vec{\ppos}^n +  \vec{\pvel}^{n+\half}\dt + O(\dt^3)
\end{equation}
We can then use Eq. \eqref{eq:verletvelhalf} again to compute the velocity at $t+\dt$
\begin{equation}
  \vec{\pvel}^{n+1} = \vec{\pvel}^{n+\half} + \half\vec{a}^{n+1}\dt
\end{equation}
Note that these equations are just a rewrite of the Störmer-Verlet method above and thus present the same stability and accuracy while eliminating the aforementioned inconveniences presented by the former.
Thus the overall algorithm is second-order accurate in the velocity and third-order accurate in the positions.
Finally, the velocity Verlet algorithm can be summarized in the following steps
\begin{equation}
  \label{eq:verletnve}
  \begin{aligned}
    \vec{\pvel}^{n+\half}&= \vec{\pvel}^n + \half \vec{a}^n\dt\\
  \vec{\ppos}^{n+1}      &= \vec{\ppos}^n +  \vec{\pvel}^{n+\half}\dt\\
  \vec{\pvel}^{n+1}      &= \vec{\pvel}^{n+\half} + \half\vec{a}^{n+1}\dt
\end{aligned}
\end{equation}
Note that, with the exception of the first step, there is no need to compute the accelerations twice, since the forces at time $t$ are known from the previous step. Forces at $t+\dt$ should be evaluated after computing $\vec{\ppos}^{n+1}$, typically overwriting the current forces. The only storage needed are the velocities, positions and accelerations per particle.
Thus, the velocity Verlet algorithm presents all the necessary properties for being a popular integrator. It has good numerical stability, a small memory footprint, good energy conservation, guaranteed momentum conservation,...

%\subsection{Testing the algorithm}
%
%One way to measure the effectiveness of the algorithm is to check the energy conservation\todo{Is this worth doing?}
\section*{Use in UAMMD}
The velocity Verlet algorithm in the NVE ensemble is available in \uammd as an Integrator module called \emph{VerletNVE}.

As an energy conserving algorithm, VerletNVE offers the possibility of setting a certain energy per particle at creation.
The starting kinetic energy of the system will be set to match this energy. Note that this will result in particle velocities being overwritten.
Alternatively you can instruct VerletNVE to leave particle velocities untouched, which will result in the initial total energy being uncontrolled by the module.
The only tool used by this module to set the initial total energy is the kinetic energy, which is always positive. As such, an error will be thrown if the target energy is \emph{less} than the potential energy at creation.

\begin{code2}[Example of the creation of a \emph{VerletNVE} \emph{Integrator}.]{label=code:verletnve}
#include<uammd.cuh>
#include<Integrator/VerletNVE.cuh>
using namespace uammd;
//A function that creates and returns a VerletNVE integrator
auto createIntegratorVerletNVE(UAMMD sim){
  VerletNVE::Parameters par;
  par.dt = sim.par.dt; //The time step
  //Optionally a target energy can be passed
  par.energy = sim.par.targetEnergy;
  //If false, VerletNVE will not initialize velocities
  //par.initVelocities = false;
  return std::make_shared<VerletNVE>(sim.pd, par);
}
\end{code2}

On its own, the \emph{Integrator} object created in source code \ref{code:verletnve} does not have much use (besides simulating a gas of ideal particles, for instance). As discussed when introducing the \emph{Integrator} interface, we can call the member \mintinline{\ucpp}{Integrator::addInteractor(std::shared_ptr<Interactor> inter)} present in every \emph{Integrator} to add any interaction to it (such as any of the ones laid out in previous chapters and their accompanying source code examples). Regardless, we can call the member \mintinline{\ucpp}{Integrator::forwardTime()} (also present in every \emph{Integrator}) to take the simulation state to the next time step. Appendix \ref{ch:fullexample} contains an example showcasing how to add \emph{Interactors} to an \emph{Integrator} and forward a simulation in time.
%With \gls{MD} we have to track the movement of each individual atom in the system, which is not practical for a mesoscale simulation of a complex fluid. Say we want to study the folding process of a single, average sized, protein (about $3$nm). A process that can take milliseconds to complete\cite{proteinfolding}. Lets put this protein inside a modest sized box with $10$nm per side. The only way to take into account the environment is to fill the box with water molecules, the 33 million that fit inside our box. And that is if we have somehow coarse grained a water molecule into a single unit, as including every invidual atom would take 100 million particles.
%And that is just water, without taking into account the protein that we are trying to simulate. There are some coarse grained models of water that can help reducing this number, like the MARTINI force field \cite{martini} and others\cite{cgwater}, but we are going to need millions of solvent particles in any case.
%Even if this kind of simulation is technically feasible\cite{antonprotein}, it requires absurd amounts of computing resources which do not really scale for something more complex, like the inside of a cell or a collection of viruses.
%
%Furthermore,
%Including the small water molecules requires to solve their individual dynamics, which is orders of magnitude faster than our time scale of interest (milliseconds).
%In particular, the solvent effect will be coarse grained in the form of fluctuations and hydrodynamic correlations between the colloidal particles resolved.
% We cannot simply ignore the effect of the solvents since we would be neglecting some of the most important phenomena in the system, such as hydrodynamics and thermal fluctuations.
During the following sections, we will see different strategies to take into account the effects of the solvent in an implicit manner. In particular, the solvent effect will be coarse-grained in the form of fluctuations and hydrodynamic correlations between the resolved colloidal particles.
The next natural step after \gls{MD} equations are the so-called \gls{LD}.

%\todo{There should be some example here on how to add interactors to integrators.}

\chapter{Langevin Dynamics}\label{sec:langevin}
\glsreset{FPE}
\glsreset{SDE}
In section \ref{sec:fpe} we introduced the Fokker-Planck formalism, which coarse grains the effects of a series of fast variables into drift and diffusion coefficients, enabling us to leave in our description only the slow relevant variables.

At the Langevin level of description, the dynamics of the solvent molecules (e.g. water) are orders of magnitude faster than the solute particles, leaving the positions and momenta of the solute particles as relevant variables. The dynamics of the solute particles are governed by a so-called Langevin equation, a manifestation of the \gls{FPE} that is often described as ``\gls{MD} with a thermostat''.
%The solvent particles are removed entirely, the idea is that the motion of the solvent particles (water) is much faster than that of the submerged objects. Furthermore, the size of the simulation units is much larger than the size of the solvent particles. Instead of simulating a time scale fast enough to keep track of each individual collision between a slow object and a solvent particle we assume that between one instant and the next so many solvent particles have collided with the object that we can consider it an stochastic process.
%In \gls{LD} submerged particles experience a random force stemming from the collisions of all the surrounding solvent particles. Additionally, the solvent particles produce a friction that goes against the movement of the submerged particles.
%In this sense \gls{LD} only models the viscous behavior of the solvent, disregarding any electrostatic or hydrodynamic phenomena.
%We reintroduce the ommited degrees of freedom as external forces acting on the particles in a \gls{MD} simulation.

We can write the Langevin \gls{SDE} as
\begin{equation}
  \label{eq:langevin}
  m\, d\vec{\pvel} = \underbrace{\vec{F}}_{\vec{\partial}_{\vec{\ppos}}U(\ppos)}dt - \overbrace{\xi\vec{\pvel}}^{\text{Drag}}dt + \underbrace{\vec{\beta}}_{\text{Fluctuations}},
\end{equation}
which can be interpreted as a form of Eq. \eqref{eq:md} coupled with a thermostat so that noise and drag forces are balanced according to the fluctuation-dissipation relation (see section \ref{sec:fdb}), guaranteeing the correct thermalization of the system (NVT ensemble).

On the other hand, Eq. \eqref{eq:langevin} is an expression of the equivalent \gls{SDE} (Eq. \eqref{eq:fpesde}) for the \gls{FPE} with the particle momenta and positions as the relevant variables. The first term in Eq. \eqref{eq:langevin}, $\vec{F}$, is the sum of the conservative forces acting on particle $i$ (the ones coming from the underlying potential energy landscape), these interactions can be steric, electrostatic, bonded, etc.
The second term represents the drag exerted by the solvent particles in the form of a dissipative force.
Finally, the third term represents the fluctuations produced by the fast and constant collisions of the solvent particles. Here $\vec{\beta}$ is a random increment which must be in fluctuation-dissipation balance with the friction term (see section \ref{sec:fdb}), we will shortly study its value.

%This random term transforms Eq. \eqref{eq:langevin} into an stochastic differential equation, in which special care must be taken when integrating.

The friction coefficient, $\xi$, is related with the damping rate as $\gamma = \xi/m$, which represents the decorrelation time of the velocity, $\tau = m/\xi$. Additionally, $\xi$ can be formally derived from a Green-Kubo relation involving the integral of the solvent-solute force time-correlation\cite{Green1954,Kubo1957}. Its value is often approximated by the Stokes (macroscopic) value $\xi=6\pi\eta a$, of a particle of radius $a$ in a solvent with viscosity $\eta$ (see Eq. \eqref{eq:stokesrel} and discussion in section \ref{sec:einstein}).

When we introduced the Fokker-Planck formalism in chapter \ref{sec:fpe} we saw that when a system dissipates energy, for instance, due to a force like the drag force in Eq. \eqref{eq:langevin}, the fluctuation-dissipation theorem states that there must be an opposite process that reintroduces this energy via thermal fluctuations.

We can use Eq. \eqref{eq:fpefdbal} to elucidate the relation between the drag, $\xi$ and the noise in Eq. \eqref{eq:langevin} by defining the single particle drift coefficient as $\widetilde{\vec{D}}^{(1)} := -\gamma \vec{\pvel}$. The dynamic ``matrix'' (in this case being just a scalar) is then $H = \gamma$. We can now write Eq. \eqref{eq:fpefdbal} in the small time interval limit, which yields
\begin{equation}
  \label{eq:langeD2}
  D^{(2)} = \gamma\sigma_{\text{eq}}.
\end{equation}
% The fluctuation-dissipation balance can be interpreted as arising from the equipartition theorem in the long time limit. We can use the equipartition theorem for a single particle in the absence of any conservative forces to relate $\gamma$ with $B$.
The equipartition theorem can be employed here to obtain the equilibrium covariance $\sigma_{\text{eq}} := \left\langle\vec{\pvel}^2\right\rangle$.
In the absence of dissipative forces, the Hamiltonian is simply $H = \frac{p^2}{2m}$, where $p = m\dot{\ppos}$ is the momentum.
Let's apply these arguments to a one-dimensional system for simplicity, understanding they also hold for the three-dimensional case. The equipartition theorem states that for every degree of freedom in the system the following equation must hold,
\begin{equation}
  \left\langle p \frac{\partial H}{\partial p}\right\rangle = \left\langle p \frac{p}{m}\right\rangle = k_BT.
\end{equation}
Where $k_B$ is the Boltzmann constant and $T$ the temperature. Brackets represent ensemble average.
The previous equation leads to the relation
\begin{equation}
  \label{eq:velautocorr}
  \sigma_{\text{eq}} = \left\langle p^2\right\rangle = m k_BT.
\end{equation}
Which represents the variance of the particle's momemtum. We can now use Eqs. \eqref{eq:langeD2} and \eqref{eq:velautocorr} to identify
\begin{equation}
  \label{eq:langenoiseamp}
  D^{(2)} := m\gamma\kT.
\end{equation}

On the other hand, in order to satisfy Eqs. \eqref{eq:fpesdemean} and \eqref{eq:fpesdefluc} the fluctuating term must have zero mean,
\begin{equation}
  \label{eq:noisemean}
  \left\langle\beta\right\rangle = 0
\end{equation}
and be uncorrelated in time (since we are assuming the time interval is small enough to consider the transport coefficients as constants) with standard deviation given by
\begin{equation}
  \label{eq:noiseautocorr}
  \left\langle\beta(t)\beta(t')\right\rangle = 2D^{(2)}dt\delta(t-t') = 2\xi\kT dt\delta(t-t').
\end{equation}
Finally, we can write the Langevin equation as
\begin{equation}
  \label{eq:langevinfull}
  m\, d\vec{\pvel} = \vec{F}dt - \xi\vec{\pvel}dt +  \sqrt{2\xi\kT}\vec{\noise},
\end{equation}
Where we have defined $\vec{\beta} := \sqrt{2\xi\kT}\vec{\noise}$.
%On the other hand, since there are no privileged directions and Eq. \eqref{eq:velautocorr} should hold, we infer that
%\begin{equation}
%  \label{eq:noisemean}
%  \left\langle F_r\right\rangle = 0
%\end{equation}
%The Langevin model assumes that the noise has no memory (Markov process), so
%\begin{equation}
%  \label{eq:noiseautocorr}
%  \left\langle F_r(t)F_r(t')\right\rangle = B \delta_{t,t'}
%\end{equation}
%This assumption is a consequence of the separation of timescales between the fast processes (solvent collisions) and the slow process (velocity of the larger solute particle). $B$ is some constant which needs to be determined to fulfill Eq. \eqref{eq:velautocorr}. There are several arguments that can be used here to get the value of $B$ \cite{Allen2017,fokker,langevinsolve}. The simplest one is to take Eq. \eqref{eq:langevin} for a single particle in the absence of external forces in one dimension in a time dicretized form
%\begin{equation}
%  p^{n+1} = p^n-\gamma\delta t p^n + \delta t F_r^n
%\end{equation}
%Where $n$ represents an instant in time separated $\delta t$ from the next. Lets compute the variance of the momentum
%\begin{equation}
%  \left\langle (p^{n+1})^2\right\rangle = \left\langle (1-\gamma \delta t)^2(p^n)^2\right\rangle + \delta t \left\langle (F_r^n)^2\right\rangle
%\end{equation}
%In equilibrium the variance of the momemtum must remain constant, so $\left\langle (p^{n+1})^2\right\rangle = \left\langle (p^{n})^2\right\rangle$, leaving
%\begin{equation}
%  \left[ (1-\gamma \delta t)^2 -1 \right]\left\langle (p^{n})^2\right\rangle = \delta t \left\langle (F_r^n)^2\right\rangle
%\end{equation}
%In the limit of $\delta t \rightarrow 0$ and taking into account the relation in Eq. \eqref{eq:velautocorr}
%\begin{equation}
%  \left\langle (p^{n})^2\right\rangle = \frac{\left\langle (F_r^n)^2\right\rangle}{2\gamma} = mk_BT
%\end{equation}
%Which represents the fluctuation-dissipation balance for this system. The value of $B$ in Eq. \eqref{eq:noiseautocorr} is then,
%\begin{equation}
%  B = 2m\gamma k_BT
%\end{equation}
Technically, Eqs. \eqref{eq:noisemean} and \eqref{eq:noiseautocorr} would be compatible with any random distribution for the noise, $\vec{\noise}$, that has zero mean and variance $dt$. However, as the noise term comes from the action of countless independent random variables (collisions with the solvent particles) the central limit theorem\cite{Feller1968} applies. Thus, it is convenient to choose $\vec{\noise}$ as a Gaussian white noise (i.e. a Wiener process)\cite{dunweg1991}.

%
%Finally, we can rewrite Eq. \eqref{eq:langevin} as
%%Multiply it by the position, $r$, and average
%%\begin{equation}
%%  \left\langle r \dot{p}\right\rangle + \xi \left\langle rp\right\rangle = 0
%%\end{equation}
%%Note that the fluctuations have vanished since they are not correlated with the position.
%%We can use a couple of mathematical identities to transform this equation into
%%\begin{equation}
%%  \ddot{\langle r^2\rangle} + \xi \dot{\langle r^2\rangle} = 2\langle p^2 \rangle = 2mk_BT
%%\end{equation}
%%
%%The solution to this differential equation is
%%\begin{equation}
%%  \langle r^2\rangle = 2mk_BT\xi^2\left( \exp{-t/\tau} -1 + t/\tau\right)  
%%\end{equation}
%%And in the long time limit where $t \gg \tau$
%%\begin{equation}
%%  \langle r^2\rangle = 2mk_BT\xi t
%%\end{equation}
%%On the other hand,
%%\begin{equation}
%%  \langle r^2\rangle = \int_0^{t_0}dt\int_0^{t_0}dt'\langle F_r(t) F_r(t')\rangle = Bt_0 = 2mk_BT
%%\end{equation}
%%
%\begin{equation}
%  \label{eq:langevinfull}
%  md\vec{\pvel} = \vec{F_c}dt - \xi \vec{\pvel}dt + \sqrt{2\xi k_BT}\vec{\widetilde{W}}
%\end{equation}
%Where $\vec{\widetilde{W}}$ is a collection of independent Wiener increments.

The presence of the noise term with $\vec{\widetilde{W}}$ makes Eq. \eqref{eq:langevinfull} an \gls{SDE}. Since Eq. \eqref{eq:langevinfull} is not analytic, the assumptions in Eq. \eqref{eq:fdmtaylor} do not hold. In other words, we do not know how to differentiate a random variable. Luckily, we can make assumptions about the integral of a random variable. This allows us to devise a solver for Eq. \eqref{eq:langevinfull} following a strategy  similar to what we did in Eqs. \eqref{eq:eulerftc} and \eqref{eq:eulerriemman} in sec. \ref{sec:euler}.

The challenge in the numerical integration of Eq. \eqref{eq:langevinfull} comes from the discretization of the noise term.
A bad discretization of this term can lead to convergence issues, such as in the case of the commonly used BBK (for Brooks-Brünger-Karplus) scheme\cite{Brunger1984}, which is known to provide the correct temperature with an error of the order of $O(\dt)$\cite{Wang2003}.
We can overcome this issue by solving Eq. \eqref{eq:langevinfull} in integral form. Lets start by studying the one dimensional case of a single particle, since the three directions are uncoupled and particles are independent (beyond the conservative forces) making the jump to 3D and many particles is then trivial. Now we integrate Eq. \eqref{eq:langevinfull} between the interval $t$ and $t+\dt$
\begin{equation}
  \label{eq:langevinriemann}
  m\int_t^{t+\dt}\dot{\pvel} dt' = \int_t^{t+\dt}F_c dt' - \int_t^{t+\dt}\xi\dot{\ppos}dt' + \sqrt{2\xi\kT}d\noise
\end{equation}
Where $d\noise$ is known as a Wienner increment, defined so that
\begin{equation}
  \label{eq:langevinwiennerinc}
  d\noise := \int_t^{t+\dt}\noise(t') dt.
\end{equation}

The key now is to realize that we can compute the last term numerically by using the Riemann sum definition of the integral in the Itô sense.
In the Itô interpretation, this integral is defined as the limit in probability of a Riemann sum, where the Brownian integral of a certain function $f$ between a certain time interval $[0, t]$ is
\begin{equation}
  \label{eq:itonoise}
  \int_0^tf(t')d\noise(t') = \lim_{n\rightarrow\infty}\sum_{i=0}^nf(t^{i-1}) (\noise(t^i) - \noise({t^{i-1}}))
\end{equation}
Where $t_i\in [0, t]$.
It can be shown that this limit converges in probability\cite{Cohen2015}.

%Lets refer to the last term as
%\begin{equation}
%  \label{eq:langenoisint}
%  \noise^{n+1} := d\noise
%\end{equation}
We can use the Riemann sum to transform Eq. \eqref{eq:langevinwiennerinc} into a sum with an arbitrarily large number of terms. Since in this case $f(t) = \sqrt{2\xi\kT}$ we are left with a sum of Gaussian distributed random numbers, which also happens to be a Gaussian random number . We can compute $d\noise$ as Gaussian random numbers with zero mean and standard deviation $\left\langle d\noise^nd\noise^m\right\rangle = 2\xi\kT\dt\delta_{n,m}$ (uncorrelated in time).
This realization now opens the door to a family of Verlet-like integration schemes, in particular we are going to describe the algorithm developed by Grønbech-Jensen and Farago~\cite{Gronbech2013}.

\section{Grønbech-Jensen}\label{sec:gronbechjensen}
Without approximations, we can write Eq. \eqref{eq:langevinriemann} as
\begin{equation}
  \label{eq:gslangevin}
  m\left(v^{n+1}-v^n\right) = \int_t^{t+\dt}F_c dt' - \xi\left(r^{n+1}-r^n\right) + \beta^{n+1}
\end{equation}
On the other hand, making use of the definition of velocity
\begin{equation}
  \int_t^{t+\dt}\dot{\ppos}dt' = r^{n+1} - r^n = \int_t^{t+\dt}vdt'
\end{equation}
Which can be approximated to second order using the Riemann sum with two intervals (similarly as we did for Euler methods in sec. \ref{sec:euler}) as
\begin{equation}
  \label{eq:gsvel}
  r^{n+1} - r^n \approx \frac{\dt}{2}\left(v^{n+1}+v^n\right)
\end{equation}
Replacing $v^{n+1}$ from Eq. \eqref{eq:gslangevin} into \eqref{eq:gsvel} gets us

\begin{equation}
  \label{eq:gspos}
  r^{n+1} - r^n =  b \dt v^n + \frac{b\dt}{2m}\left(\int_t^{t+\dt}F_cdt' + \beta^{n+1}\right)
\end{equation}
Where
\begin{equation}
b := \frac{1}{1+\frac{\xi\dt}{2}}
\end{equation}
The deterministic force term can be approximated to second order using the same strategy as in Eq. \eqref{eq:gsvel}. However, for Eq. \eqref{eq:gspos} only one term of the Riemann sum is needed to make it second order accurate (as it gets multiplied by $\dt^2$). We can then write the final equations for the velocity and position in three dimensions as
\begin{eqnarray}
  \label{eq:gsfinal}
  \vec{\ppos}^{n+1}  &=&  \vec{\ppos}^n + b \dt \vec{\pvel}^n + \frac{b\dt^2}{2m}\vec{F_c}^n + \frac{b\dt}{2m}\vec{\beta}^{n+1}\\
  \vec{\pvel}^{n+1} &=& a\vec{\pvel}^n + \frac{\dt}{2m}\left(a\vec{F_c}^n + \vec{F_c} ^{n+1}\right) +  \frac{b}{m}\vec{\beta}^{n+1}
\end{eqnarray}
Where
\begin{equation}
  a:=b \left(1-\frac{\xi\dt}{2}\right)
\end{equation}
Note that, since we need the force at $t+\dt$ to compute the velocities at $t+\dt$ the forces have to be a function of the positions only (and not velocities).

\section*{Use in UAMMD}
The Grønbech-Jensen algorithm described above is available in \uammd as an Integrator module called \emph{GronbechJensen}, under the namespace \emph{VerletNVT}.

This module will initialize the particle's velocities near the equilibrium Boltzmann distribution.
Alternatively you can instruct it to leave particle velocities untouched. The system's temperature will then tend to the equilibrium value according to the friction coefficient and each particle's mass.

\begin{code2}[Example of the creation of a \emph{VerletNVT} \emph{Integrator} module.]{label=code:verletnvt}
#include<uammd.cuh>
#include<Integrator/VerletNVT.cuh>
using namespace uammd;
//A function that creates and returns a VerletNVT integrator
auto createIntegratorVerletNVT(UAMMD sim){
  using Verlet = VerletNVT::GronbechJensen;
  Verlet::Parameters par;
  par.dt = sim.par.dt; //The time step
  par.friction = sim.par.friction;
  par.temperature = sim.par.temperature; 
  //Optionally, you can force all particles to have the same mass, overriding the one in sim.pd even if it was set.
  //par.mass = sim.par.mass;
  //If set to true, particle velocities will be left untouched at initialization.
  //par.initVelocities = false;
  //If 2D is set to true here the integrator will leave the third coordinate of the particles untouched.
  //par.is2D = true;
  return std::make_shared<VerletNVT>(sim.pd, par);
}
\end{code2}


The Langevin model makes several assumptions to arrive at Eq. \eqref{eq:langevinfull}. At long times, hydrodynamic effects due to momentum propagation of the solvent (leading to long-time tails in the velocity correlation of the particle) are neglected. The dynamics of short times $\tau<\gamma/m$ are also wrong, as memory effects in the noise forces start to matter at fast small scales. While solving these small scale effects requires to explicitly describe the solvent-solute forces using \gls{MD}, we will see in subsequent chapters how to improve the long-time limit introducing the solvent hydrodynamics.

\chapter{Dissipative Particle Dynamics}\label{ch:dpd}
\glsreset{LD}
One of the most popular techniques used to reintroduce some of the degrees of freedom lost with \gls{LD} is \gls{DPD}. This coarse graining technique can be used to go further in the spatio-temporal scale by choosing groups of fluid particles as the simulation unit, sitting inbetween microscopic (as in \gls{MD}) and macroscopic (hydrodynamic) descriptions. In practice \gls{DPD} is a Langevin approach where friction acts by pairs of particles and conserves momentum. \gls{DPD} equations can be defined from the microscopic Hamiltonian using coarse-graining theory\cite{Hijon2010}.
In the standard \gls{DPD}, particles interact via a soft potential, modelling the interaction between two large groups of fluid particles.
In many instances\cite{Sablic2007} \gls{DPD} is used as a momentum-conserving thermostat, which thus permits to include hydrodynamics (contrary to a single Langevin approach). Local momemtum conservation results in the emergence of macroscopic hydrodynamic effects. These momentum conserving forces can then be tuned to reproduce not only thermodynamics, but also dynamical and rheological properties of diverse complex fluids.
The equations of motion in \gls{DPD} have the same functional form as \gls{LD} and can be in fact considered as a momentum-conserving generalization of \gls{LD}. In fact, it can also be derived from the Fokker-Planck formalism\cite{dunweg1991}. The equations of motion for \gls{DPD} read,
\begin{equation}
  \label{eq:dpddyn}
  m\vec{a} = \vec{F^c} + \vec{F^d} + \vec{F^r}.
\end{equation}
Where the three forces are traditionally expressed as\cite{Groot1997,Espanol1995},
\begin{equation}
  \label{eq:dpdforces}
  \begin{aligned}
    \vec{F^c}_{ij} &=&\omega(r_{ij})\hat{\vec{\ppos}}_{ij}\\
    \vec{F^d}_{ij} &=&-\xi\omega^2(r_{ij})(\vec{\pvel}_{ij}\cdot\vec{\ppos}_{ij})\hat{\vec{\ppos}}_{ij}\\
    \vec{F^r}_{ij} &=&\sqrt{2\xi\kT}\omega(r_{ij})\widetilde{W}_{ij}\hat{\vec{\ppos}}_{ij}    
  \end{aligned}
\end{equation}
Where $\vec{\pvel}_{ij} = \vec{\pvel}_j - \vec{\pvel}_i$ is the relative velocity between particles $i$ and $j$. Here $\xi$ represents a friction coefficient (as in sec. \ref{sec:langevin}) and is related to the random force strength via fluctuation-dissipation balance in a familiar way\cite{Espanol1995}. In general $\xi$ can be considered to be a tensorial quantity and even derived from atomistic simulations using dynamic coarse graining theory\cite{Hijon2010}. The factor $\widetilde{W}_{ij}$ is different from the one in sec. \ref{sec:langevin} in that it affects pairs of particles (instead of each individual one), it also represents a Gaussian random number with zero mean and unit standard deviation, but must be chosen independently for each pair while ensuring symmetry so that $\widetilde{W}_{ij} = \widetilde{W}_{ji}$.
The weight function $\omega(r)$ is a soft repulsive force usually defined as
\begin{equation}
  \label{eq:dpdw}
  \omega(r) =
  \begin{cases}
    \alpha\left(1-\dfrac{r}{r_{c}}\right) & r<r_{c}\\
    0 & r\ge r_{c}
  \end{cases}
\end{equation}
Where $r_{c}$ is a cut-off distance. The strength parameter, $\alpha$, can in principle be different for each pair of particles ,$i$-$j$, but for simplicity we will assume it is the same for every pair. In fact, the conservative force, $\vec{F}_c$ can be derived from a bottom-up approach, resulting in the gradient of an effective potential\cite{Hijon2010}.

Numerical integration of the \gls{SDE} \eqref{eq:dpddyn} is tricky for the same reasons as with Eq. \eqref{eq:langevin}. We have already described some methods of solving such an equation in sec. \ref{sec:gronbechjensen}. However, the Grønbech-Jensen method is not valid for \gls{DPD} since in this case the forces depend on the velocities of the particles. A simple modification can be made, sacrificing stability, by approximating the velocity to just first order in Eq. \eqref{eq:gsfinal}, so that the velocity depends only on the force for the current step. Unfortunately, this leads to artifacts in the transport properties and unacceptable temperature drifts. There are several strategies in the literature trying to overcome this, usually presented as modifications of the velocity Verlet algorithm. A good summary can be found in~\cite{Leimhuler2015}. These methods include empirical tweaks to velocity Verlet via a free parameter\cite{Groot1997}, a self-consistent correction to the velocity to avoid the temperature drift\cite{Pagonabarraga1998} or a recomputation of the dissipative forces after the velocity update\cite{Besold2000}.
All of these methods improve the accuracy of the predicted transport properties and response functions in exchange for increased computational cost. 
One popular approach is to simply use velocity Verlet as described in sec. \ref{sec:velocityverlet} with the forces in Eq. \eqref{eq:dpdforces}. This yields ``poor'' stability and presents certain artifacts\cite{Besold2000} due to the mistreatment of the derivative of the noise term incurred by treating Eq. \eqref{eq:dpddyn} as an \gls{ODE} instead of a proper \gls{SDE}. However, it is often good enough and while it might require a smaller time step to recover measurables to an acceptable tolerance it is the fastest approach and trivial to implement in a code already providing the velocity Verlet algorithm.
%\todo{Preguntar a Rafa sobre integradores DPD. Por que nadie lo trata como una SDE y evalua v en $t+1/2$?}
\section*{Use in UAMMD}
In \uammd, \gls{DPD} is encoded as a \emph{Potential} that can be used to specialize the \emph{PairForces} module in sec. \ref{sec:shortrange}, the resulting \emph{Interactor} can then be coupled to a \emph{VerletNVE} \emph{Integrator} (described in sec. \ref{sec:velocityverlet}).

\begin{code2}[Creation of an instance of a \gls{DPD} integrator. In \uammd, \gls{DPD} is encoded as a Verlet integrator coupled with a short range interaction.]  {label=code:dpd}
#include<uammd.cuh>
#include<Integrator/VerletNVE.cuh>
#include<Interactor/PairForces.cuh>
#include<Interactor/Potential/DPD.cuh>
using namespace uammd;
//A function that creates and returns a DPD integrator
auto createIntegratorDPD(UAMMD sim){   
  Potential::DPD::Parameters par;
  par.temperature = sim.par.temperature;
  //The cut off for the weight function
  par.cutOff = sim.par.cutOff;
  //The friction coefficient
  par.gamma = sim.par.friction; 
  //The strength of the weight function
  par.A = sim.par.strength; 
  par.dt = sim.par.dt;  
  auto dpd = make_shared<Potential::DPD>(dpd);
  //From the example in PairForces
  auto interactor = createPairForcesWithPotential(sim, dpd);
  //From the example in the MD section
  // particle velocities should not be initialized
  // by VerletNVE (initVelocities=false)
  auto verlet = createVerletNVE(sim);
  verlet->addInteractor(interactor);
  return verlet;
}
\end{code2}
It is worth mentioning that, following the ``one \emph{Integrator} with many \emph{Interactors}'' philosophy of UAMMD, in addition to the \gls{DPD} potential, one can add any other interactions to the \mintinline{\ucpp}{verlet} \emph{Integrator} returned by the function in source code \label{code:dpd} (such as bonds or an additional steric repulsion just to name a few).

\newpage
%\chapter{Smoothed Particle Hydrodynamics}
%\todo{Describe SPH}
%

\chapter{Brownian Dynamics}\label{sec:bd}
%\todo{Introduccion al origen de BD y que problemas resuelve que LD no}
When the viscous forces are much larger than the inertial forces, i.e. $ |\xi\vec{\pvel}| \gg |m\vec{a}|$, the inertial term in Eq. \eqref{eq:langevinfull} becomes irrelevant at very short time scales.
\gls{BD} takes advantage of such a time scale separation between the particle velocity fluctuations and its displacement and can be interpreted as the overdamped, or non-inertial, limit of \gls{LD}. The decorrelation time of the velocity, defined as $\tau_l = m/\xi$ is much faster than the time needed for a particle to move farther than its own size. \gls{BD} represents the long time limit of the Langevin equation. This is a powerful property of \gls{BD}, since sampling the probability distributions of the underlying stochastic processes (stemming from the rapid movement of the solute particles) does not require sampling their fast dynamics.

In \gls{BD}, the coupling between the submerged particles and the solvent is instantaneous.
Furthermore, since the particle velocities decorrelate instantly, the only remaining relevant variables are the positions of the colloidal particles. We can use the Fokker-Planck formalism to describe the dynamics of the colloidal particles positions. When the \gls{FPE} \eqref{eq:fpe} is restricted to Brownian particles it is known as the Smoluchowski Diffusion Equation.

Using Eq. \eqref{eq:fpesde}, we can derive the \gls{SDE} for the positions of a collection of colloidal particles with positions $\vec{q} =\{\vec{q}_1,\vec{q}_2,\dots, \vec{q}_N\}$ from the Smoluchowski description of the two-body level marginal probability~\cite{Dhont1996} as
\begin{equation}
  \label{eq:bdlange}
  d\vec{\ppos} = \widetilde{\vec{D}}^{(1)}(\vec{\ppos})dt + \delta\vec{\ppos}
\end{equation}
Where $\delta\vec{q}$ is the set of random displacements which satisfy the two particle covariance given by Eq. \eqref{eq:fpesdefluc} 
\begin{equation}
  \label{eq:bdfluc}
  \left\langle \delta\vec{\ppos}\otimes \delta\vec{\ppos}\right\rangle = 2\tens{D} dt
\end{equation}

Following the discussion in section \ref{sec:einstein} we can define the drift vector as
\begin{equation}
  \label{eq:bddrift}
\vec{D}^{(1)} := -\tens{M}\vec{\partial}_{\vec{\ppos}}U(\vec{\ppos}) + \vec{v}_d(\vec{\ppos}),
\end{equation}
Here  $\vec{v}_d(\vec{\ppos})$ represents the drift of a base flow moving the particles along. The potential energy, $U$, acting on the particles can include pairwise interactions, external fields and/or external non conservative force fields (as in optofluidics~\cite{Melendez2019}). In particular, we can define
\begin{equation}
  \vec{F} = \partial_{\vec{q}}U(\vec{q}) + \vec{F}^{\text{ext}}(\vec{q})
\end{equation}
%\todo{Rafa, is this ok?}
Let us focus on the dissipative part of the dynamics, encoded in the mobility tensor. The pairwise mobility tensor, $\tens{M}$, encodes in its elements the hydrodynamic transfer of the force at one point $\vec{q}_i$, to the displacement at another point $\vec{q}_j$. It thus determines the correlations between two particle displacements (see also Eq. \eqref{eq:bdfluc}).
The mobility tensor is then related to the diffusion coefficients via the Einstein relation (see sec. \ref{sec:einstein}),
\begin{equation}
  \label{eq:bdeinstein}
  \tens{D}(\vec{\ppos}) = \kT \tens{M}(\vec{\ppos}).
\end{equation}
The Einstein relation coupled with the condition in Eq. \eqref{eq:bdfluc} hints that mobility tensor should be symmetric and positive semi-definite, so that we can define a tensor $\tens{B}:=\sqrt{\tens{M}}$ such that
\begin{equation}
  \tens{B}\tens{B}^T := \tens{M}.
\end{equation}
If the system is translationally invariant (isotropic)\footnote{Note that there are situations in which this assumption does not hold. Such is the case if a wall is present, where the mobility becomes dependent on the distance to the wall, $\tens{M}=\tens{M}(\vec{q}-\vec{q}', z, z')$.} $\tens{M}(\vec{\ppos}, \vec{\ppos}') = \tens{M}(\vec{\ppos}-\vec{\ppos}')$. To model hydrodynamic couplings, $\tens{M}$ is usually taken to be the Rotne-Prager-Yamakawa tensor (describing up to the second hydrodynamic reflection)~\cite{Dhont1996}. Including further reflections explodes the complexity of the formulation and it is only useful in situations in which particles are really close (e.g. high densities)~\cite{Dhont1996}, in which case the lubrication approximation is preferable.

We can then write the equations of \gls{BD} as
\begin{equation}
  \label{eq:bdfull}
  d\vec{\ppos} = \vec{v}_d(\vec{\ppos}) + \tens{M}\vec{F}dt + \sqrt{2\kT\tens{M}}d\vec{\noise} + \kT\vec{\partial}_{\vec{\ppos}}\cdot\tens{M}dt.
\end{equation}
Where $\vec{F}$ are the forces acting on the particles and the (so-called) Brownian motion, $d\vec{\noise}$, is a collection of Wienner increments with zero mean and
\begin{equation}
\left\langle d\noise_{i}^\alpha(t)d\noise_{j}^\beta(t') \right\rangle = dt\delta(t-t')\delta_{ij}\delta_{\alpha\beta}.
\end{equation}
It is worth noticing that the physical origin of the noise, $d\vec{\noise}$, is no longer just a random force coming from an additive action of many collisions with solvent particles, but the average of a series of stochastic displacements over a period of time which is short compared to the diffusion time of one particle (but long compared with the relaxation time of the solvent velocity fluctuations via sound or vorticity transport).

The last term in Eq. \eqref{eq:bdfull}, known as the thermal drift term, can be non zero in cases where rigid particle constraints are imposed~\cite{Westwood2021} or due to the presence of boundaries~\cite{Pelaez2018,Balboa2016}. However, in isotropic and irrotational hydrodynamic fields, this term is usually zero and can thus be omitted from the description.

In practice, we can obtain the actual form of the mobility matrix by directly measuring correlations between random displacements of different particles~\cite{Panzuela2018} in an experiment, simply by applying Eqs. \eqref{eq:bdfluc} and \eqref{eq:bdeinstein}. However, in the case of solutes in a Newtonian fluid it is possible to obtain analytical expressions for $\tens{M}$ using standard hydrodynamic theory. In future chapters we will study several analytical instances of $\tens{M}$ in various regimes.

For the time being, we will focus on the simple case of the mobility being non-zero only on the diagonal (neglecting hydrodynamic interactions). Furthermore, we assume the self-mobility is equal for all particles so that $\tens{M}(\vec{\ppos}, \vec{\ppos}') = M\mathbb{I} = M_0$. Hydrodynamics will then be reintroduced in chapter \ref{sec:bdhi}. This order allows us to introduce the stochastic numerical integration schemes and their use in \uammd before dealing with the full mobility tensor, which calls for a series of numerical techniques that will be introduced later.

As discussed in sec. \ref{sec:einstein}, in the case of a no-slip sphere of radius $a$ moving inside a fluid with viscosity $\eta$ the bare self-mobility is $M_0 = (6\pi\eta a)^{-1}$.

Since the bare self-mobility is just a number when mutual hydrodynamic interactions are not considered, the thermal drift trivially disappears and Eq. \eqref{eq:bdfull} is greatly simplified, leaving
\begin{equation}
  \label{eq:bd}
  d\vec{\ppos} = M\vec{F}dt + \sqrt{2\kT M}d\vec{\noise},
\end{equation}


%There are several strategies that can be employed here to compute the properties of the stochastic term.
%One way is to integrate the stochastic term over a period of time $\dt\gg\tau_l$ in the Langevin equation. We already made this integral while developing the numerical integration scheme for the Langevin equation in Eq. \eqref{eq:langenoisint}, the same can be applied here to arrive at $\left\langle\Delta\vec{\noise}(t)\Delta\vec{\noise}(0)\right\rangle = t\delta(t)$.
%We are going to do it by studying the long time correlation of the position (the relevant variable in \gls{BD}) in Eq. \eqref{eq:langevinfull}. Lets start by computing the long time correlation of the velocity in the one dimensional case for one particle in the absence of external forces.
%We can neglect the time derivative in Eq. \eqref{eq:langevinfull} in the long time limit so that
%\begin{equation}
%  \label{eq:bdvelcorr}
%  \langle v(t) v(t') \rangle \underset{t-t'\gg\tau_l}{\approx} 2D\langle \noise(t)\noise(t')\rangle = 2D\delta(t-t')
%\end{equation}
%Where we have used the already known correlation for the noise in Eq. \eqref{eq:noiseautocorr}.
%The solution for Eq. \eqref{eq:langevinfull} can be written as
%\begin{equation}
%  v(t) = v_0\exp(-\gamma t) + \int_0^t\exp(-\gamma(t-t'))\noise(t')dt'
%\end{equation}
%We can use the known correlation of the noise to compute the velocity correlation as
%\begin{equation}
%  \begin{aligned}
%&  \left\langle v(t_1)v(t_2)\right\rangle = v_0^2\exp(-\gamma(t_1+t_2)) \\
%  &+\int_0^{t_1}{\int_0^{t_2}{\exp(-\gamma(t_1+t_2+t_1'-t_2'))B\delta(t_1'-t_2')dt_1'dt_2'}}
%\end{aligned}
%\end{equation}
%Where $B :=\frac{2m\kT}{\gamma}$.
%The first term vanishes when $t_1 +t_2 \gg \tau_l$.
%Solving the double integral yields
%\begin{equation}
%  \left\langle v(t_1)v(t_2)\right\rangle = \frac{B}{2\gamma}\left[\exp(-\gamma|t_1-t_2|)-\exp(-\gamma(t_1+t_2))\right]
%\end{equation}
%Where the second exponential vanishes for $t_1 +t_2 \gg \tau_l$, leaving
%\begin{equation}
%  \left\langle v(t_1)v(t_2)\right\rangle = \frac{B}{2\gamma}\exp(-\gamma|t_1-t_2|)
%\end{equation}
%Lets now compute the mean squared displacement of the position
%\begin{equation}
%  \left\langle(r(t) - r_0)^2 \right\rangle = \left\langle\left[ \int_0^{t}v(t_1)dt_1 \right]^2 \right\rangle = \int_0^{t_1}{\int_0^{t_2}{\left\langle v(t_1)v(t_2)\right\rangle}}
%\end{equation}
%Replacing Eq. \eqref{eq:bdvelcorr} and solving the double integral yields
%\begin{equation}
%  \left\langle(r(t) - r_0)^2 \right\rangle \underset{t\gg\tau_l}{\approx}  2Dt % - \frac{B}{\gamma^3}\left(1-\exp(-\gamma t)\right)
%\end{equation}
%Which is the mean square displacement of a Langevin particle at long times.
%%At long times, $t \gg\tau_l$, only the first term survives, finally arriving at the expression for the long time mean square displacement of the displacement
%%\begin{equation}
%%  \label{eq:langeposmsd}
%%  \left\langle(r(t\gg\tau_l) - r_0)^2 \right\rangle = \frac{B}{2\gamma}t = 2Dt
%%\end{equation}
%We can now compute the same for Eq. \eqref{eq:bdlange} with only fluctuations, which must be equal to Eq. \eqref{eq:bdvelcorr} for a time $t \gg \tau_l$.
%
%\begin{equation}
%  \label{eq:bdmsd}
%  \left\langle (r(t)-r_0)^2 \right\rangle = 2D\int_0^{t}{\int_0^{t}{\left\langle\mathcal{\noise}(t_1)\mathcal{\noise}(t_2)\right\rangle dt_1dt_2}} = 2Dt
%\end{equation}
%This allows us to devise the properties of the noise in Eq. \eqref{eq:bdlange}, $\mathcal{\noise}$ is a Gaussian random number with zero mean and standard deviation $\langle\mathcal{\noise}(t)\mathcal{\noise}(t')\rangle = \delta(t-t')$.
%In \gls{BD} displacements are proportional to the forces via the mobility, i.e terminal velocity is reached instantenously.
%Coupled with the thermal fluctuations, the velocity is basically transformed into a random variable. This makes expressing Eq. \eqref{eq:bdlange} in terms of the velocity awkward. The relevant variable is now the position and it is therefore usual to write Eq. \eqref{eq:bdlange} in terms of displacements via the so-called Itô formula
%\begin{equation}
%  \label{eq:bd}
%  d\vec{\ppos} = M\vec{F}dt + \sqrt{2D}d\vec{\noise}
%\end{equation}
where the stochastic term, $d\vec{\noise}$, is the origin of the Brownian motion being a Wiener increment (independent random numbers with zero mean and variance $dt$).
A solution to Eq. \eqref{eq:bd} is one that satisfies
\begin{equation}
  \label{eq:bddis}
  \vec{\ppos}(t) = \vec{\ppos}_0 + \int_0^tM\vec{F}dt + \int_0^t\sqrt{2\kT M}d\vec{\noise}
\end{equation}

\subsection*{Numerical integration}
In order to numerically integrate Eq. \eqref{eq:bd} we must discretize the Brownian motion in \eqref{eq:itonoise}. We can do so by integrating it during a small time interval,$[t_n, t_{n+1}]$, where $t_{n+1}-t_{n} = \dt$. If the time interval is small enough, we can approximate the Riemman sum by only its first term. Using the discrete time notation we can write
\begin{equation}
  \label{eq:itonoise2}
  \int_{t_n}^{t_{n+1}}f(t')d\noise(t') \approx f^n \mathcal{\noise}^n = f^n \left(\noise^{n+1} - \noise^n\right),
\end{equation}
where $\noise^n$ are independent Gaussian random variables with zero mean and standard deviation $\dt$. Naturally, this approximation is better as $\dt \rightarrow 0$.
With this discretization of the noise we can devise several strategies to numerically integrate Eq. \eqref{eq:bd}. The simplest one is the so-called \gls{EM} scheme~\cite{Desmond2001}.

\subsection*{Euler Maruyama}
Using Eq. \eqref{eq:bddis} and \eqref{eq:itonoise2} we can write
\begin{equation}
  \label{eq:eulermaruyama}
  \vec{\ppos}^{n+1} = \vec{\ppos}^n + M\vec{F}^n\dt + \sqrt{2D}\vec{\mathcal{\noise}}^n
\end{equation}
It can be proved that this algorithm is weakly convergent with order $1$ and strongly convergent with order $1/2$~\cite{Kloeden2011}. Thus the \gls{EM} scheme is not particularly accurate, even in the absence of fluctuations.
There are, however, other solvers that yield better accuracy at the same, or similar, computational cost. Let's see some of them.
\subsubsection*{Adams Bashforth}
The \gls{AB} scheme uses the forces from the previous step to improve the prediction for the next. This incurs the overhead of storing the previous forces but its computational cost is marginally larger than \gls{EM}. This algorithm mixes a first order method for the noise with a second order method for the force. It yields better accuracy than \gls{EM}, although this comes from experience since as of the time of writing no formal work has been done on its weak accuracy. An empirical demonstration of its better accuracy can be found in~\cite{Balboa2017}.
\begin{equation}
  \vec{\ppos}^{n+1} = \vec{\ppos}^n + M\left[\frac{3}{2}\vec{F}^n - \half \vec{F}^{n-1}\right]\dt + \sqrt{2D}\vec{\mathcal{\noise}}^n
\end{equation}

\subsubsection*{Leimkuhler}
Another integrator recently developed by Leimkuhler in~\cite{Leimkuhler2015}
 While also a first order method it seems to yield better accuracy than \gls{AB} and \gls{EM}. I am not aware of any formal studies of its accuracy besides the one in~\cite{Leimkuhler2014}.
 The update rule is very similar to \gls{EM} but uses noise from two steps (which are generated each time instead of stored)
 \begin{equation}
  \vec{\ppos}^{n+1} = \vec{\ppos}^n + M\vec{F}^n\dt + \sqrt{\frac{D}{2}}\left[\vec{\mathcal{\noise}}^n + \vec{\mathcal{\noise}}^{n-1}\right]
\end{equation}
Note that, as stated in~\cite{Leimkuhler2015}, while this solver seems to be better than the rest at sampling equilibrium configurations, it does not correctly solve the dynamics of the problem.

\subsubsection*{Midpoint}
Finally, a more sophisticated integrator which we are going to refer to as Mid Point is described in~\cite{Delong2013}. 
It is a two step explicit midpoint predictor-corrector scheme. It has a fourth order convergence scaling at the expense of having twice the cost of a single step method, as it requires to evaluate forces twice per update. Noise has to be remembered as well but in practice it is just regenerated instead of stored.
Midpoint updates the simulation with the following rule
\begin{equation}
  \begin{aligned}
    &\vec{\ppos}^{n+1/2} = \vec{\ppos}^n + \half M\vec{F}^n\dt + \sqrt{D}\vec{\mathcal{\noise}}^{n_1}\\
    &\vec{\ppos}^{n+1} = \vec{\ppos}^n + M\vec{F}^{n+1/2}\dt + \sqrt{D}\left[\vec{\mathcal{\noise}}^{n_1}  + \vec{\mathcal{\noise}}^{n_2}\right]
  \end{aligned}
\end{equation}
Where both instances of the noise ($n_1$ and $n_2$) are uncorrelated and $\langle \mathcal{\noise}^{n_i}\rangle^2 = \dt$.
%\todo{Compare all algorithms by showing a g(r) with different time steps}
\subsection*{Use in UAMMD}
Using the \gls{BD} \emph{Integrators} is quite similar to the rest we have seen so far. An example code is available in code \ref{code:bd}. The same interface joins all the \gls{BD} integrators we saw in this chapter.
\begin{code2}[Example of the creation of a \gls{BD} \emph{Integrator} module.]{label=code:bd}
#include<uammd.cuh>
#include<Integrator/BrownianDynamics.cuh>
using namespace uammd;
//A function that creates and returns a BD integrator
auto createIntegratorBD(UAMMD sim){   
  //Choose the method
  using BD = BD::EulerMaruyama;
  //using BD = BD::MidPoint;
  //using BD = BD::AdamsBashforth;
  //using BD = BD::Leimkuhler;
  BD::Parameters par;
  par.temperature=sim.par.temperature;
  par.viscosity=sim.par.viscosity;
  //If the hydrodynamic radius is not set (or it is equal to -1) the BD Integrator will use the radius of each particle in ParticleData
  par.hydrodynamicRadius=sim.par.hydrodynamicRadius;
  par.dt=sim.par.dt;
  auto bd = std::make_shared<BD>(sim.pd, par);
  return bd;
}
\end{code2}


\section{Metropolis Adjusted Langevin Algorithm (MALA)}
Sometimes we are not interested in the dynamics of a simulation, rather we are looking to sample equilibrium states or simply approaching equilibrium as soon as possible. In these instances the \gls{BD} machinery previously introduced might not be the optimal strategy. We can use Monte Carlo methods to sample equilibrium configurations. Alas Monte Carlo methods often prove to be challenging to parallelize, specially to the degree required in a \gpu. For instance, proposing a new configuration in the standard Metropolis Monte Carlo algorithm consists in moving just one particle at a time and in order to correctly measure the total energy change of the system the rest of the system must not change during the process, virtually serializing the algorithm. One simple parallelization is to sample new configurations by randomly moving all the particles. However, the chances of this new configuration being more favorable or altogether valid are, at most, slim.
The Metropolis Adjusted Langevin Algorithm (MALA) ~\cite{Bou2010} attempts to increase the acceptance chance of moving all the particles in the system by guiding the sampling process using the particle forces in addition to the total system energy change. In this regard, the MALA algorithm can also be referred to as a Force Biased Metropolis Monte Carlo scheme.

 A step in MALA amounts to performing a \gls{BD} simulation step and then using the positions, forces and energies of the old and new configurations to decide whether to accept it or not with a Metropolis-like probability. Let us now study the MALA acceptance rule in detail.

We can interpret the Euler-Maruyama Brownian Dynamics scheme in Eq. \eqref{eq:eulermaruyama} (with unit self mobility, $M=1$) as a Markov chain with a transition kernel following a strictly positive probability transition density defined as
\begin{equation}
  \label{eq:malakernel}
  p( \vec{Q}^n, \vec{Q}^{n+1}) = (4\pi \kT\dt)^{3/2}\exp\left(\frac{\left|\vec{Q}^{n+1} - \vec{Q}^n + \dt\vec{F}^n\right|^2}{4\kT\dt}\right)
\end{equation}
Where $\vec{Q}^n:={\vec{\ppos}_1^n, \vec{\ppos}^n_2, ..., \vec{\ppos}^n_N}$ represents the positions of all particles at time step $n$.
%This is refered to as the unadjusted Langevin algorithm~\cite{Roberts1996}.
On the other hand, the Langevin equation \eqref{eq:bd} at equilibrium presents a probability density given by
\begin{equation}
  \label{eq:malatarget}
  \pi(\vec{Q}) = Z^{-1}\exp\left(-\frac{U(\vec{Q})}{kT}\right)
\end{equation}
Where the entropy is defined as $Z = \int\exp\left(-U(\vec{Q})/\kT\right)$ and $U(\vec{Q})$ is the internal energy (such that $\vec{F} = -\nabla U$).

Once we have a proposed transition kernel (Eq. \eqref{eq:malakernel}) and a target probability density (the equilibrium one in Eq. \eqref{eq:malatarget}) we can use the Metropolis-Hastings method to design an updating scheme, referred to as MALA~\cite{Roberts1996}~\cite{Besag1994}.

Instead of sampling configurations using the Euler-Maruyama scheme (as discussed in sec. \ref{sec:bd}) we now propose a new configuration using Eq. \eqref{eq:eulermaruyama} as
\begin{equation}
  \widetilde{\vec{Q}}^{n+1} = \vec{Q}^n + \dt\vec{F}^n + \sqrt{2\kT\dt}\vec{\widetilde{\mathcal{W}}}^n
\end{equation}

And accept it with a Metropolis probability given by

\begin{equation}
  \alpha(\vec{Q}^n, \widetilde{\vec{Q}}^{n+1}) = \text{min}\left(\frac{p(\widetilde{\vec{Q}}^{n+1},\vec{Q}^{n})\pi(\widetilde{\vec{Q}}^{n+1})}{p(\vec{Q}^{n}, \widetilde{\vec{Q}}^{n+1})\pi(\vec{Q}^{n})}\right)
\end{equation}

Numerically, this can be done by drawing a uniform random number $\xi^n \in [0,1]$ and accepting the new configuration only if $\xi^n<\alpha(\vec{Q}^n, \widetilde{\vec{Q}}^{n+1})$.

In contrast to the naive Metropolis-Hasting algorithm, that blindly proposes configurations, MALA proposes configurations that always have a higher propability in $\pi$, increasing the chances of acceptance.

We are not restricted to using the same value for $\dt$ in all steps\footnote{Theoretically the step size should not be adjusted dynamically for correct sampling (as it violates detailed balance).} since in an MCMC algorithm $\dt$ has lost its meaning as a \emph{time step} and it is now simply a parameter describing the \emph{distance} in phase space between two given configurations. As such, we can tune it to get a desired configuration acceptance ratio.
With MALA we are not solving the dynamics of the system, but rather sampling equilibrium configurations (or moving the system towards equilibrium). We can use the MALA algorithm and its ability to handle any $\dt$ for thermalization or to get a running simulation ``unstuck''. For instance, if a simulation of hard spheres (modeled with some repulsive potential, for instance the WCA in sec. \ref{sec:lj}) has reached an invalid configuration due to thermal fluctuations (i.e. two particles overlapping, which promptly causes the simulation to ``explode'') the MALA algorithm could be used to safely move the simulation away from this state (since MALA will mostly modify the state of the configuration only when a new configuration is more favorable).

Although MALA proves to be useful for thermalising or initializing simulations, testing suggests that hard steric repulsion between particles (like LJ) requires such a small time step to ensure a reasonable acceptance ratio that the algorithm becomes unusable in practice. In these cases using \gls{BD} directly is preferable.

In order to tune the jump step ($\dt$) to ensure a certain acceptance ratio on average, we measure the acceptance ratio every once in a while, if it is lower than the target the jump step is decreased (and increased if it is larger). We increase the jump step slowly (a 2\% increase) and decrease it fast (a 10\%) as it is usually done in optimization algorithms.

Another disadvantage of MALA is that usually the energy change coming from moving all particles in the system grows, on average, with the number of particles. This results in the optimal jump step decreasing with the number of particles. Thus defeating the purpose.
\begin{figure}[H]
  \centering    
  \includegraphics[width=0.8\textwidth]{gfx/malasoft}\\
  \includegraphics[width=0.8\textwidth]{gfx/malalj}
  \caption[ ]{Optimal time step in MALA for several acceptance ratios and number of particles using a soft repulsive potential given by Eq. \eqref{eq:malasoft} (left) and a hard steric WCA interaction (right). Naturally, a higher acceptance ratio (with $1$ corresponding to accepting all configurations and $0$ to never accepting them) results in a lower step size, $\dt$. A by-product of the MALA algorithm comparing the total energy change is that $\dt$ depends on the number of particles so that more particles result in a lower optimal $\dt$. In all cases the number density of particles is kept at $\rho := N/V = 0.1a^{-3}$.}
  \label{fig:malastep}
\end{figure}
In Fig. \ref{fig:malastep} we compare the optimal step size of the MALA algorithm for different acceptance ratios in different scenarios. In particular, we compare results using a hard-repulsion WCA potential (truncated \gls{LJ}, see sec. \ref{sec:lj}) with a soft core repulsive interaction given by the potential

\begin{equation}
  \label{eq:malasoft}
U(r)= U_0
\begin{cases}
  \exp(-(r-d)/b) & r>d\\
  1-(r-d)/b & r\le d
\end{cases}
\end{equation}
Where $U_0$ represents the repulsive strength, $d$ is a typical repulsive radius and we choose the scale factor $b=0.1$.

\subsection*{Use in UAMMD}
The MALA algorithm is available in \uammd as an \emph{Integrator} module. Its usage is similar to the ones we have seen thus far. The MALA module will query its \emph{Interactors} for both force and energy (whereas, for instance, \gls{BD}, will only request forces).
MALA will autotune the step size ($\dt$) to ensure a given acceptance ratio.
\begin{code2}[Example of the creation of a MALA \emph{Integrator} module.]{label=code:mala}
#include<uammd.cuh>
#include<Integrator/MonteCarlo/ForceBiased.cuh>
using namespace uammd;
//A function that creates and returns a MALA integrator
auto createIntegratorMALA(UAMMD sim){   
  MC::ForceBiased::Parameters par;
  //Inverse of temperature
  par.beta = 1.0/sim.par.temperature;	
  //Initial step length.
  //It will be optimized according to the target acceptance ratio
  par.stepSize = sim.par.dt;
  //Desired acceptance ratio 
  par.acceptanceRatio = sim.par.accRatio;
  auto mala = std::make_shared<MC::ForceBiased>(sim.pd, par);
  return mala;
}
\end{code2}

\chapter{Hydrodynamic Interactions}\label{sec:bdhi}
In our discussion about \gls{BD} in chapter \ref{sec:bd} we neglected hydrodynamic interactions by considering only the diagonal terms of the mobility tensor in Eq. \eqref{eq:bdfull} (the so-called self-mobility). We considered the effect of the solvent on the submerged particles without considering the opposite interaction. Particles disturb the fluid around them, which in turn has an effect on the rest. When the hydrodynamic displacements are no longer negligible (for instance, due to the density of colloidal particles increasing), the full form of the mobility tensor must be taken into account. Alternatively, hydrodynamic effects can be taken into account (to some extent) by particle solvers equipped with local momentum conservation (such as \gls{DPD}, see chapter \ref{ch:dpd}). However, \gls{DPD} (and \gls{SPH}) explicitly solve the solvent particle dynamics (or some coarse-grained version of them). Furthermore, the \gls{DPD} and \gls{SPH} methods lie in the inertial regime of the solvent, forcing us to describe a fast time scale that might be of no interest in a given model, in particular the sound and viscous momentum propagation (vorticity). Thus far we have taken a bottom-up approach, starting by modeling the dynamics of the solvent particles themselves and then eliminating degrees of freedom aided by the Fokker-Planck formalism. This enabled us to elucidate Eq. \eqref{eq:bdfull}, giving us some clues about the mobility matrix and, more importantly, relating it with the diffusion via the fluctuation-dissipation balance. It is worth now to take a top-down approach by considering the hydrodynamics of the solvent and then bridge the gap back to \gls{BD}, which will give us more information about the actual form of the mobility.
\section*{Fluid dynamics}
We start by considering the more general incompressible steady Navier-Stokes equations. Then, we will introduce the action of the particles as a force-density on the fluid (propagated by a smoothed delta-like function centered around each particle). We now study how this force propagates to the rest of the fluid.

We will take the non-inertial regime for the dynamics of both the fluid and the particles to connect with \gls{BD} while taking into account hydrodynamic correlations. In section \ref{ch:icm} we consider the fluid inertial regime.

The incompressible Navier-Stokes equations can be written as~\cite{Balboa2014}
\begin{equation}
  \label{eq:navierstokes}
  \begin{aligned}
    \overbrace{\underbrace{\rho\partial_t{\vec{\fvel}}}_{\text{time}} +\underbrace{\rho\nabla\cdot (\vec{\fvel}\otimes\vec{\fvel})}_{\text{convection}}}^{\text{fluid inertia}} + \underbrace{\nabla \pi}_{\text{internal stress}} &= \overbrace{\eta \nabla^2\vec{\fvel}}^{\text{momentum diffusion}} + \underbrace{\vec{f} + \nabla\cdot \mathcal{Z}}_{\text{\parbox{2cm}{\centering external forces\\[-4pt] and\\[-4pt] thermal noise}}}\\
    \nabla\cdot\vec{\fvel} &= 0\quad\mathllap{\underbrace{\phantom{\nabla\cdot\vec{\fvel} = 0}}_{\text{incompressibility}}}
  \end{aligned}
\end{equation}
Where $\vec{\fvel}(\vec{\fpos}, t)$ represents the velocity field of the fluid, $\pi$ the pressure, $\rho$ its density and $\eta$ its viscosity. $\vec{f}$ is some localized force density acting on the fluid (which can arise from the presence of submerged particles).

The fluctuating hydrodynamic description (Landau-Lipshizt) includes a fluctuating stress tensor, $\mathcal{Z}(\vec{\fpos}, t)$, which must comply with the fluctuation-dissipation balance (see sec. \ref{sec:fdb}) according to the following statistical properties~\cite{Zarate2006}
\begin{equation}
  \label{eq:navierstkesnoise}
  \begin{aligned}
&  \langle \mathcal Z_{ij}\rangle = 0\\
&  \langle \mathcal Z_{ik}(\vec{\fpos},t)\mathcal Z_{jm}(\vec{\fpos}',t')\rangle = 2\kT\eta(\delta_{ij}\delta_{km} + \delta_{im}\delta_{kj})\delta(\vec{\fpos}-\vec{\fpos}')\delta(t-t')
\end{aligned}
\end{equation}
Where $i,j,k,m$ represent the different coordinates.
In order to generalize the notation for Eq. \eqref{eq:navierstokes}, we will sometimes introduce the fluctuations into the fluid forcing term, defining $\tilde{\vec{f}} := \vec{f} + \nabla\cdot\mathcal{Z}$.
We can also write Eq. \eqref{eq:navierstokes} as
\begin{equation}
  \label{eq:navierstokes2}
  \rho\partial_t\vec{\fvel} = -\nabla\cdot \tens{\sigma} + \tilde{\vec{f}},
\end{equation}
with
\begin{equation}
  \tens{\sigma} := \pi\mathbb{I} - \eta\left(\nabla \vec{\fvel} + \nabla\vec{\fvel}^T\right) + \rho\left(\vec{\fvel}\otimes\vec{\fvel}\right)
\end{equation}
\section*{Particle dynamics}
To derive the equation of motion of the particle, we impose the kinetic constraint $\vec{u} = \int_{V_p}\vec{\fvel}(\vec{\fpos}, t)d\vec{\fpos}$, where $V_p$ is the particle volume. The kinetic constraint imposes instantaneous coupling between the particles and the surrounding fluid (this condition represents no-slip of the fluid on the ``real'' particle's surface). For the purpose of this derivation, we keep fluctuations out of the description an integrate over the particle volume in Eq. \eqref{eq:navierstokes2},
\begin{equation}
  \rho\int_{V_p}\partial_t\vec{\fvel}d\vec{\fpos} = -\int_{V_p}\nabla\cdot\tens{\sigma}d\vec{r} + \int_V\vec{f}d\vec{\fpos}.
\end{equation}
Note that
$$\rho\int_{V_p}\partial_t\vec{\fvel}d\vec{\fpos} = \rho \partial_t\int_{V_p}\vec{\fvel}d\vec{\fpos} = \rho\partial_t\vec{\pvel}$$
and using the divergence theorem
$$
-\int_{V_p}\nabla\cdot\tens{\sigma}d\vec{\fpos} = \oint_S\tens{\sigma}\cdot\hat{\vec{n}}d\vec{r} = \vec{F}_{\text{drag}}
$$
is the fluid traction on the particle (drag force).
We also note that $\int_{V_p}\vec{f}d\vec{\fpos} = \vec{\lambda}$ is the total force exerted by the particle on the fluid (induced force~\cite{Mazur1974}).
Hence, $m_f\partial_t\vec{\pvel} = \vec{F}_{\text{drag}} + \vec{\lambda}$, with $m_f$ being the mass of the fluid displaced by the particle.
On the other hand, $m_p\dot{\vec{\pvel}} = \vec{F}_{\text{drag}} + \vec{F}$ where $\vec{F}$ is any other force acting on the particle (aside from the fluid drag). Subtracting and introducing the Archimedean excess mass,  $m_e := m_p - m_f$,
\begin{equation}
  m_e\dot{\vec{\pvel}} = \vec{F} - \vec{\lambda}.
\end{equation}
Which implies, for $m_e = 0$, $\vec{\lambda}=\vec{F}$. Thus any force on the particle goes to the fluid. In fact, this is another important assumption in Brownian dynamics; neglect the particle inertia.

%In the spirit of \gls{BD}, we also assume that the friction time $\tau_F = m/\xi$ is quite small compared with any other physical process. In other words, we assume particles are neutrally buoyant and disregard their inertia ($\dot{\vec{\pvel}} = 0$). As demonstrated in ~\cite{Balboa2014}, the inertial effects of the particles due also to a non zero excess mass can be reintroduced via a Lagrange multiplier, $\lambda$, (which enforces $\vec{\pvel} = \vec{\fvel}$). Physically, $\lambda$ is the net force that the particle exerts on the fluid which, in turn (third Newton law) exerts a force $-\lambda$ to the particle. The particle's equation of motion is then
%\begin{equation}
%  \label{eq:inertiallagrande}
%m_e \dot{\vec{\pvel}} = \vec{F} - \lambda.
%\end{equation}
%The particle mass, $m_p$, can be decomposed as $m_p := m + m_e$, where $m=\rho\Delta V$ is the mass of the fluid displaced by the particle and $m_e$ is the so-called excess mass. Note that $m_e=0$ for neutrally buoyant particles, and in this case $\vec{F} = \lambda$, thus any force acting on the particle (external fields, particle-particle interactions, etc) is instantaneously and completely transfered to the fluid.
%
%
%The fluid momentum equation will then be (neglecting convection),
%\begin{equation}
%\rho \partial_t\vec{\fvel} = -\nabla \pi + \eta \nabla^2\vec{\fvel} + \vec{f}.
%\end{equation}
%
%Therefore $\vec{f(\vec{\fpos}-\vec{\ppos})}$ is a force distribution (force per unit volume) related to the particle presence. As a first piece of information, the first moment of such distribution (the so-called stokeslet) is
%$$\int{\vec{f(\vec{\fpos}-\vec{\ppos})}d\vec{\fpos}^3} = \lambda.$$
%Further moments of $f$ can be used to provide torques (rotlet) and stress (stresslet) effects arising from the particles.
%
\section*{The Stokes limit}

If we take the overdamped limit of Eq. \eqref{eq:navierstokes}, where the momentum of the fluid can be eliminated as a fast variable (allowing to neglect the transient term $\partial_t \vec{\fvel}$ as well as the convection\footnote{Neglecting convection is valid for small Reynolds number hydrodynamics, i.e, $\text{Re} = \frac{\eta v}{\rho L} \ll 1$ with $L$ the smallest characteristic length of the system (e.g. particle radius).}\footnote{Moreover, we assume that the Schmidt number is very large, $S_c = \eta/(\rho D_0) \gg 1$, where $D_0 = \kT/(6\pi\eta a)$ is the typical diffusion coefficient of a submerged particle (see sec. \ref{sec:einstein}), which implies that fluid momentum propagates much faster than particle diffusion. For $S_c\gg 1$ the transient term $\rho\partial_t\vec{v}$ can be neglected, which is a sane approximation (even for proteins in water).}) we get the so-called Stokes equations
\begin{equation}
  \label{eq:stokes}
  \begin{aligned}
    \nabla \pi - \eta \nabla^2\vec{\fvel} &=  \tilde{\vec{f}},\\
    \nabla\cdot\vec{\fvel} &= 0.
  \end{aligned}
\end{equation}
We can eliminate the pressure from the description by using the projection method. Let's take the divergence of the first equation
\begin{equation}
  \nabla^2 \pi - \eta \nabla\cdot(\nabla^2\vec{\fvel}) =  \nabla\cdot\tilde{\vec{f}}
\end{equation}
Since the divergence of the velocity is zero the second term vanishes, so we can formally write
\begin{equation}
  \label{eq:stokespressure}
  \pi = \nabla^{-2}(\nabla\cdot\tilde{\vec{f}})
\end{equation}
Replacing the pressure in Eq. \eqref{eq:stokes}
\begin{equation}
  \eta\nabla^2\vec{\fvel} = \nabla\left[\nabla^{-2}(\nabla\cdot\tilde{\vec{f}})\right] - \tilde{\vec{f}} = -\oper{P} \tilde{\vec{f}}
\end{equation}
Where the projection operator, $\oper{P}$, is formally defined as
\begin{equation}
  \label{eq:projectoper}
\oper{P}  :=  \mathbb{I} - \nabla\nabla^{-2}\nabla.
\end{equation}
$\oper{P}$ projects onto the space of divergence-free velocity. $\mathbb{I}$ represents the identity.
In the particular case of an unbounded domain with fluid at rest at infinity, all the differential operators in Eq. \eqref{eq:projectoper} commute in Fourier space, so that

\begin{equation}
  \label{eq:eq:projectoperfou}
  \fou{\oper{P}}(\vec{k}) = \mathbb{I} - \frac{\vec{k}\otimes\vec{k}}{k^2}
\end{equation}
Where $\vec{k}$ are the wave numbers.
Finally, we can identify
\begin{equation}
  \label{eq:stokesoper}
  \oper{L} := -\nabla^{-2}\oper{P}
\end{equation}
as the Stokes solution operator to arrive at
\begin{equation}
  \label{eq:bdhivel}
  \vec{\fvel} = \eta^{-1}\oper{L}\tilde{\vec{f}}
\end{equation}
The Green's function, $\tens{G}$, of Eq. \eqref{eq:bdhivel} in the case of an unbounded domain can be written in Fourier space as
\begin{equation}
  \label{eq:greenstokes}
  \eta^{-1}\oper{L}(\vec{k})\rightarrow \fou{\tens{G}}(\vec{k}) := \eta^{-1}k^{-2}\fou{\oper{P}}(\vec{k})
\end{equation}
The inverse transform of Eq. \eqref{eq:greenstokes} can be computed analytically (with the help of some known mathematical relations~\cite{Lisicki2013}) to get
\begin{equation}
  \label{eq:bdhioseen}
\tens{O}(\vec{r}) := \frac{1}{8\pi\eta r}\left(\mathbb{I} - \frac{\vec{r}\otimes\vec{r}}{r^2}\right)
\end{equation}
This solution is known as the Oseen tensor~\cite{Lisicki2013}, the response of a three dimensional unbounded fluid at rest at infinity to a delta forcing.
Figure \ref{fig:oseen} contains a visual representation of the Oseen tensor.
\begin{figure}
  \centering
  \includesvg[width=0.6\columnwidth]{gfx/oseen}
  \caption[ ]{Representation of the Oseen mobility in 2D. A force acting on a point (green) is propagated to the fluid via the Oseen tensor (blue lines). The fluid behind the source will be dragged onto it, while the fluid in front will be pushed away. Note that the action of the Oseen tensor decays as the inverse of the distance, a behavior this representation does not convey.}
  \label{fig:oseen}
\end{figure}

\section*{Coupling with particles}
Eqs. \eqref{eq:bdhivel} and \eqref{eq:greenstokes} constitute a formalism to translate forces to velocities and permits unveiling their correlated displacements.

In the following derivation we are going to describe the particle effect on the fluid using a generalized delta function, $\delta_a(\vec{r})$. Using a monopolar approximation, we set the force distribution as $\vec{f}(\vec{\fpos}) = \vec{\lambda} \delta_a(\vec{\fpos})$. Here, $\delta_a(\vec{\fpos})$ is a distribution of compact support (usually a smooth smeared delta function). The subscript $a$ refers to the hydrodynamic radius of the particle, related to the typical size of the particle around which the kernel distributes the particle force, or any other extensive quantity (e.g acoustic forces~\cite{Balboa2013}, torques, etc).

The velocity of a certain particle $i\in [1,N]$ is computed as
\begin{equation}
  \label{eq:bdhifvel}
  \vec{\pvel}_i= \oper{J}_{\vec{\ppos}_i}\vec{\fvel} =\int{\delta_a(\vec{\ppos}_i - \vec{\fpos})\vec{\fvel}(\vec{\fpos})d\vec{\fpos}}.
\end{equation}
So that $\delta_a(\vec{\fpos})$ is also used  to interpolate fluid properties to the particle ``language''.
The operator $\oper{J}$ is called interpolation operator. Eq. \eqref{eq:bdhivel} states that the velocity of the particle is equal to the local average of the fluid velocity.

The reverse of interpolation is spreading. As stated, we compute the force density acting on the fluid at a given point $\vec{\fpos}$ in space by spreading the forces of all particles according to
\begin{equation}
  \label{eq:spreadoper}
  \vec{f}(\vec{\fpos}) = \oper{S}(\vec{\fpos})\vec{F} = \sum_i\delta_a(\vec{\ppos}-\vec{\fpos}_i)\vec{F}_i,
\end{equation}
where we have used the supervector notation \cite{Dhont1996}, $\vec{F} := \{\vec{F}_1,\dots,\vec{F}_N\}$.
The benefit of using the same kernel to spread and interpolate is that it makes the operations adjoint to each other, $\oper{J} = \oper{S}^*$, which is required in the following derivation~\cite{Delong2014}.

The physical volume $\Delta V$ of a particle is related to the kernel via
\begin{equation}
  \Delta V = (\oper{J}\oper{S}\ 1)^{-1} = \left(\int\delta_a^2(\vec{r})d\vec{r}\right)^{-1},
\end{equation}
where $\oper{S}$ is applied to the unit constant function\footnote{We abuse the notation here and assume that the coupling operators can be applied to a scalar, a vector or a tensor field, understanding that the same spreading/interpolation operation is applied to each component independently.}\footnote{Note that we applied both $\oper{J}$ and $\oper{S}$ to a system with only one particle.}.
We will discuss spreading and interpolation in a regular grid in more detail in sec. \ref{sec:ibm}.

\section{Connection with Brownian Dynamics}\label{sec:bdhicon}

We can now write an equation for the displacements (velocities) of a group of particles using Eqs. \eqref{eq:bdhivel} and \eqref{eq:greenstokes} as
\begin{equation}
  \label{eq:bdhibdrelation}
  \frac{d\vec{q}_i}{dt} = \vec{u}_i = \eta^{-1}\oper{J}_{\vec{\ppos}_i}\oper{L}(\oper{S}\vec{F} + \nabla\cdot\mathcal Z).
\end{equation}
This equation is reminiscent of Eq. \eqref{eq:bdfull} by defining the mobility as
\begin{equation}
  \label{eq:bdhimobns}
  \tens{M} = \eta^{-1}\oper{J}\oper{L}\oper{S}.
\end{equation}
and its ``square root'' as
\begin{equation}
  \label{eq:bdhimobnsnoise}
  \tens{M}^{1/2} = \eta^{-1/2}\oper{J}\oper{L}\nabla\cdot.
\end{equation}
It can be easily proved, by using that $-\oper{L}\nabla^2\oper{L} = \oper{L}$ and that $\oper{J}^*=\oper{S}$\footnote{The scalar product in the particle and fluid domains are related via $(\oper{J}\vec{\fvel})\cdot\vec{\pvel} = \int{\vec{\fvel}\cdot(\oper{S}\vec{\pvel})d\vec{r}} = \int{\delta_a(\vec{\ppos}-\vec{\fpos})(\vec{\fvel}\cdot\vec{\pvel})d\vec{r}}$.}~\cite{Delong2014}\footnote{In particular, because negative divergence is the adjoint of the gradient and $\nabla^2 = \nabla\cdot\nabla = -\nabla\cdot(\nabla\cdot)^*$, we can write $\left(\oper{J}\oper{L}\nabla\cdot\right)\left(\oper{J}\oper{L}\nabla\cdot\right)^* = \oper{J}\oper{L}\nabla\cdot(\nabla\cdot)^*\oper{L}^*\oper{J}^* = -\oper{J}\oper{L}\nabla^2\oper{L}^*\oper{S} = \oper{J}\oper{L}\oper{S}$.}, that this definition of the square root of the mobility satisfies the fluctuation-dissipation balance\footnote{$\left\langle\delta\vec{\ppos}_i\otimes\delta\vec{\ppos}_j\right\rangle = 2\kT\tens{M}(\vec{\ppos_{ij}})\dt$} with $\mathcal{Z}$ given by Eq. \eqref{eq:navierstkesnoise}.
Naturally, the definition in Eq. \eqref{eq:bdhimobnsnoise} satisfies
\begin{equation}
  \label{eq:mobsqrt}
  \tens{M} = \tens{M}^{1/2}\left(\tens{M}^{1/2}\right)^*.
\end{equation}
%We can now transform Eq. \eqref{eq:bdhibdrelation} into Eq. \eqref{eq:bdfull}, rewritten here for convenience:
%\begin{equation}
%  \label{eq:bdhi}
%  d\vec{\ppos} = \tens{M}\vec{F}dt + \sqrt{2\kT \tens{M}}d\vec{\widetilde{W}} + \kT\left(\vec{\partial}_{\vec{\ppos}}\cdot\tens{M}\right)dt
%\end{equation}
The definition in Eq. \ref{eq:bdhimobns} allows us to realize that Eq. \eqref{eq:bdhibdrelation} and \eqref{eq:bdfull} are, in fact, equivalent.

We are now ready to extract more information about the mobility matrix, which is in general a configuration-dependent tensor. In particular, writing Eq. \eqref{eq:bdhimobns} as the double convolution with the kernel at the position of two particles yields a pairwise mobility, depending only on the positions of the particles as
\begin{equation}
  \label{eq:bdhimob}
  \tens{M}_{ij} = \eta^{-1}\iint{\delta_a(\vec{q}_j-\vec{r})\oper{L}(\vec{r}, \vec{r}')\delta_a(\vec{q}_i -\vec{r}')d\vec{r}d\vec{r}'}.
\end{equation}
Here $\oper{L}$ is the Green's function for Eq. \eqref{eq:stokes} with the particular boundary conditions.

The mobility tensor definition in Eq. \eqref{eq:bdhimob} is general for any geometry. For an unbounded fluid, the Green's function is the Oseen tensor in Eq. \eqref{eq:bdhioseen} and the resulting mobility (when using $\delta_a:=\delta$) is referred to as the Oseen mobility, so that $\tens{M}_{ij} = \tens{O}(\vec{\ppos}_j - \vec{\ppos}_i)$.

The Oseen mobility presents several problems when particles get too close stemming from the point particle assumption. For instance, the tensor becomes ill-formed (and non positive-definite) at close distances, hindering the applicability of the assumption in Eq. \eqref{eq:mobsqrt}. On the other hand, the Oseen mobility diverges at zero distance, which can result in numerical issues in the absence of steric repulsions and requires treating the self mobility separately. The Oseen mobility can be in a way interpreted as the mobility for colloidal particles with size $a\rightarrow 0$.

In practice we would like to take into account the finite size of the submerged particles, which amounts to choosing a different kernel in Eq. \eqref{eq:bdhimob} or, in the purely particle based description of Eq. \eqref{eq:bdfull}, a different (pairwise) approximation to the mobility altogether.

We will see in future sections how to solve the dynamics of the particles without the need for the explicit form for the Green's function by discretizing Eq. \eqref{eq:bdhifvel} directly. This can be useful for special geometries in which the differential operators in the Stokes solution operator do not commute or when Eq. \eqref{eq:bdhimob} cannot be solved analytically. For the moment we will stick to imposing a mobility tensor explicitly.

One of the usual choices for the mobility tensor is the \gls{RPY} tensor (derived around the same time independently in ~\cite{Rotne1969,Yamakawa1970}), describing the hydrodynamic interaction of two spheres of radius $a$. We can compute it by integrating Eq. \eqref{eq:bdhimob} with the Stokes solution operator (the Oseen tensor) on the surface, $S$, of two spheres of radius $a$ centered at the positions of the particles ($\vec{\ppos}_i$ and $\vec{\ppos}_j$),
\begin{equation}
  \label{eq:rpymobfour}
  \begin{aligned}
    \tens{M}_{ij}^{\textrm{RPY}} &= \frac{\eta^{-1}}{(4\pi a^2)^2} \int_{S_i}dS_i\int_{S_j}dS_j \int d\vec{k} \exp(i\vec{k}\cdot\vec{\ppos}_{ij})\hat{\oper{L}}(\vec{k})\\
    &= \eta^{-1} \int \frac{\exp(i\vec{k}\cdot\vec{\ppos}_{ij})}{k^2}\left(\sinc(ka) \right)^2\left(\mathbb{I} - \frac{\vec{k}\otimes\vec{k}}{k^2}\right)d\vec{k}.
\end{aligned}
\end{equation}
Expressing this integral in Fourier space allows us to express it in a compact form and, as we will see later, doing so will facilitate its extension to periodic environments.
A detailed solution for this integral can be found in~\cite{Wajnryb2013}. It is important to note that the integral must be handled separately when the two particles overlap.
Finally, the free-space \gls{RPY} mobility tensor can be written in real space as~\cite{Wajnryb2013}
\begin{equation}
  \label{eq:rpy}
  \tens{M}^{\textrm{RPY}}(\vec{r} = \vec{q}_i-\vec{q}_j) = M_0\left\{
  \begin{aligned}
    &\left( \frac{3a}{4r} + \frac{a^3}{2r^3} \right)\mathbb{I} + \left(\frac{3a}{4r} - \frac{3a^3}{2r^3}\right)\frac{\vec{r}\otimes\vec{r}}{r^2}  & r > 2a\\
    &\left(1 - \frac{9r}{32a} \right)\mathbb{I} + \left( \frac{3r}{32a} \right)\frac{\vec{r}\otimes\vec{r}}{r^2} & r \le 2a
  \end{aligned}\right.
\end{equation}
The self mobility, $M_0 := \tens{M}(0) = (6\pi\eta a)^{-1}$ is, by no coincidence, equal to the drag for a sphere moving through a Stokesian fluid as given by the Stokes-Einstein relation~\cite{Dhont1996} (see sec. \ref{sec:einstein}).

This approximation is valid for dilute suspensions (and) to leading order in the far field. The \gls{RPY} tensor has been proven to accurately capture hydrodynamics even when particles overlap beyond their hydrodynamic radius~\cite{Ermak1978}~\cite{Wajnryb2013}.
Given its popularity lots of works attempting to generalize or adapt the \gls{RPY} tensor for different geometries and use cases can be found~\cite{Wajnryb2013,Liang2013,Guan2018,Fiore2017}. In particular, we will make use of the generalization in~\cite{Zuk2014}, which allows for each particle to have a different hydrodynamic radius.

Notice that, as revealed by Eqs. \eqref{eq:bdhioseen} and \eqref{eq:rpy}, the approximations for the pairwise mobility usually come under the form
\begin{equation}
  \label{eq:bdhimobgeneral}
  \tens{M}(\vec{r}) = f(r)\mathbb{I} + g(r)\frac{\vec{r}\otimes\vec{r}}{r^2}.
\end{equation}
%Being $M_0 := \tens{M}(0)$ refered to as the self mobility.
This is an expression of the fact that the mobility is usually isotropic and translationally invariant.
%The only piece missing now is the fluctuating term. However, we already saw back in sec. \ref{sec:langevin} the relation between the noise and the drag via the fluctuation-dissipation balance.
%\begin{equation}
%  \tens{M} = \tens{M}^{1/2}\left(\tens{M}^{1/2}\right)^T
%\end{equation}

The mobility tensors elucidated thus far have zero divergence (due to the incompressibility of the three-dimensional flow). However, it is important to take the thermal drift term in Eq. \eqref{eq:bdfull} into account since even for these cases, certain spatio-temporal discretizations could result in the mobility having a spurious thermal drift. Furthermore, divergence will be non-zero in other geometries (like the confined systems in sec. \ref{sec:q2D}).
Although in the use cases throughout this manuscript thermal drift does not pose a significant computational challenge, certain solvers (for instance, when computing hydrodynamic correlations of rigid bodies~\cite{Westwood2021}), for which the application of the Stokes solution operator (or alternatively the evaluation of the mobility) is particularly expensive can result in the thermal drift term having a non-trivial cost.
One of the most widespread generic approaches for computing the thermal drift term is to use the so-called Random Finite Differences (RFD)~\cite{Delong2014}, where the divergence of the mobility is computed as
\begin{equation}
  \label{eq:rfd}
    \frac{1}{\delta}\left\langle \tens{M}\left(\vec{\ppos}+\frac{\delta}{2}\vec{\noise}\right)\vec{\noise} - \tens{M}\left(\vec{\ppos}-\frac{\delta}{2}\vec{\noise}\right)\vec{\noise}\right\rangle
    = \vec{\partial}_{\vec{\ppos}}\cdot\tens{M}(\vec{\ppos}) + O(\delta^2).
\end{equation}
Here $\vec{\noise}$ is a vector of random Gaussian numbers with zero mean and unit standard deviation and $\delta$ is a small parameter (generally chosen as small as possible without incurring in numerical issues).
Note that evaluating Eq. \eqref{eq:rfd} requires two Stokes solves (or mobility evaluations). It is possible to skip one of them by evaluating the derivative of the kernel only once~\cite{Delong2014}.

\chapter{Hydrodynamics in open boundaries}
In the following chapters, we will discuss numerical algorithms for hydrodynamic interactions in domains with different periodicities based on the mathematical framework laid out in chapter \ref{sec:bdhi}. Let us start by examining systems with fully open boundaries. In this instance, we will describe fully Lagrangian algorithms based on Green's functions in which the whole computation takes place in real space (the standard approach in \gls{BD}).

Although we are not limited to it, thus far the mobility matrices we have computed explicitly (in Eqs. \eqref{eq:rpy} and \eqref{eq:bdhioseen}) correspond to systems with open boundaries (since they come from applying the unbounded Green's function in Eq. \eqref{eq:greenstokes}). Let's first see how to solve Eq. \eqref{eq:bdfull} (rewritten here for the readers convenience),
$$  d\vec{\ppos} =\tens{M}\vec{F}dt + \sqrt{2\kT\tens{M}}d\vec{\noise} + \kT\vec{\partial}_{\vec{\ppos}}\cdot\tens{M}dt,$$
by using the unbounded \gls{RPY} mobility and then explore other strategies that can be used when assuming periodic boundary conditions.

The fluctuating term in Eq. \eqref{eq:bdfull} involves finding the square root of the mobility as defined in Eq. \eqref{eq:mobsqrt}.
This requires that the mobility matrix is positive definite. Note that the \gls{RPY} tensor is positive definite for all particle configurations, whereas the Oseen tensor becomes non positive definite at short distances.
Solving Eq. \eqref{eq:bdfull} numerically by direct evaluation results in an algorithm with a complexity of at least $O(N^2)$ due to the matrix-vector multiplication of the mobility tensor and the forces. Furthermore, finding the square root of the mobility will typically dominate the cost of the algorithm. Even so, there might be situations in which such an algorithm could be the best option.
Temporal integration can be achieved with any of the methods described in chapter \ref{sec:bd}. However, it is important to consider the extreme cost of computing and/or storing both terms in Eq. \eqref{eq:bdfull} when choosing an algorithm. As such, in \uammd we usually stick to the Euler-Maruyama method.

The deterministic term can be computed via a library call to a matrix-vector multiplication function. However, this incurs $O(N^2)$ storage. If the full mobility matrix is not needed for a later stage, it can be computed on the fly by using the \emph{NBody} algorithm in sec. \ref{sec:nbody}.
\section{Cholesky}\label{sec:chol}
The classic strategy for computing the square root of the mobility, originally proposed by Ermak~\cite{Ermak1978}, is by direct Cholesky factorization. This operation requires $O(N^3)$ operations, rendering this algorithm unsuitable for large numbers of particles (above $10^4$). Additionally, it has $O(N^2)$ storage requirements, since the full mobility matrix has to be stored.
However, the sheer raw power of the \gpu can make this a valid option. In \uammd the Cholesky factorization is accomplished via a single library call to NVIDIA's cuSolver function \emph{potrf}~\cite{cusolver}.
On the other hand, since the mobility matrix has to be stored anyway, the rest of the algorithm can be coded via a few function calls to a linear algebra library. In particular, the Cholesky module in \uammd uses the matrix-vector multiplications in the \emph{cuBLAS} library~\cite{cublas}. Taking into account the symmetric form of the mobility matrix, only the upper half needs to be computed and stored, \emph{cuBLAS} (and most linear algebra libraries) provide subroutines that leverage this. In this regard, there is not much possibility for optimization.

\subsection*{Use in UAMMD}\label{sec:uammdchol}
In \uammd, \gls{BDHI} algorithms are separated between temporal integration schemes and strategies for computing the deterministic and stochastic displacements. Both pieces are joined to form an \emph{Integrator} that can be used as usual.
Here is an example of the Euler-Maruyama integration scheme being specialized with the Cholesky decomposition algorithm for the fluctuations.
\begin{code2}[Example code for the Cholesky \gls{BDHI} method]{label=code:chol}
#include<uammd.cuh>
#include<Integrator/BDHI/BDHI_EulerMaruyama.cuh>
#include<Integrator/BDHI/BDHI_Cholesky.cuh>
using namespace uammd;
//A function that creates and returns a BDHI integrator
// using Cholesky decomposition for the noise
auto createIntegratorBDHICholesky(UAMMD sim){   
  //A strategy is mixed with an integration scheme
  using Cholesky = BDHI::EulerMaruyama<BDHI::Cholesky>;
  Cholesky::Parameters par;
  par.temperature = sim.par.temperature;
  par.viscosity = sim.par.viscosity;
  //For Cholesky the radius is optional.
  //If not selected, the module will use the individual radius of each particle.
  //par.hydrodynamicRadius = sim.par.hydrodynamicRadius;
  par.dt = sim.par.dt;
  auto bdhi = std::make_shared<Cholesky>(sim.pd, par);
  return bdhi;
}
\end{code2}


\section{Lanczos}\label{sec:lanczos}
Fixman proposed a method based on Chebyshev polynomials~\cite{Fixman1986} to compute the square root of the mobility. This method requires approximating the extremal eigenvalues of the mobility. Many strategies can be employed to find out these eigenvalues, with complexities ranging from $O(N^3)$ (thus beating the purpose) to $O(N^{2.25})$~\cite{Jendrejack2000}. More recently, a family of iterative algorithms based on Krylov subspace decompositions (using the Lanczos algorithm) have emerged~\cite{Ando2012,Saadat2014} showcasing algorithmic complexities in the order $O(kN^2)$, being $k$ the number of required iterations (which is usually around the order of $10$ depending on the desired tolerance). In \uammd the technique developed in ~\cite{Ando2012} is implemented.

Another benefit of this method over Cholesky is that it is not required to store the full mobility matrix in order to compute the fluctuations. The product of the mobility tensor by a vector (the forces in the deterministic term and a random noise in the fluctuating one) can be computed by recomputing the necessary terms. This will be particularly useful later, when most elements in the mobility tensor become zero, reducing the complexity of the computation for both terms. In particular, \uammd's implementation of the Lanczos iterative method is templated for any object capable of providing the product of any given vector with the mobility matrix. In the current instance we use the NBody algorithm (chapter \ref{sec:nbody}) coupled with a \emph{Transverser} because the mobility is a dense matrix. However, in section \ref{sec:pse}, we only need to compute the square root of a sparse mobility matrix (because the pairwise mobility is short ranged) and thus we can couple the same \emph{Transverser} with a neighbour list instead.


%\todo{Some description of the algorithm might be cool... Explain at least why its called Lanczos}

\subsection*{Use in UAMMD}
Using the Lanczos strategy in \uammd is similar to using Cholesky (see sec. \ref{sec:uammdchol}). With the difference that now, being an iterative algorithm, a tolerance can be selected.
Code \ref{code:lanczos} contains an example of the Euler-Maruyama integration scheme being specialized with the Lanczos iterative algorithm for the fluctuations.
\begin{code2}  [Example code for the Lanczos \gls{BDHI} method.]  {label=code:lanczos}
#include<uammd.cuh>
#include<Integrator/BDHI/BDHI_EulerMaruyama.cuh>
#include<Integrator/BDHI/BDHI_Lanczos.cuh>
using namespace uammd;
//A function that creates and returns a BDHI integrator
// using Lanczos decomposition for the noise
auto createIntegratorBDHILanczos(UAMMD sim){   
  //A strategy is mixed with an integration scheme
  using Lanczos = BDHI::EulerMaruyama<BDHI::Lanczos>;
  Lanczos::Parameters par;
  par.temperature = sim.par.temperature;
  par.viscosity = sim.par.viscosity;
  //For Lanczos the radius is optional.
  //If not selected, the module will use the individual radius of each particle.
  //par.hydrodynamicRadius = sim.par.hydrodynamicRadius;
  par.dt = sim.par.dt;
  //The tolerance for the stochastic term computation
  par.tolerance = sim.par.tolerance;
  auto bdhi = std::make_shared<Lanczos>(sim.pd, par);
  return bdhi;
}
\end{code2}

\newpage
\chapter{Triply periodic hydrodynamics}
It is often convenient to impose \gls{PBC} in our complex fluids simulations. However, as the hydrodynamic interactions are long ranged in nature, special considerations are required to elaborate efficient algorithms. When discussing open boundary hydrodynamics in the past chapter the most efficient algorithm had a complexity of $O(N^2)$. Intuitively, it would seem that imposing \glspl{PBC} would but make things even worse. Luckily, we can leverage the mathematical machinery laid out in chapter \ref{sec:bdhi} to devise a family of triply periodic, Eulerian-Lagrangian, pseudo-spectral algorithms on regular meshes with complexities that will, in fact, scale linearly with the number of particles. In the following sections we will go through a bunch of GPU-aware algorithms that allow to compute hydrodynamic displacements (owing to the different terms in Eq. \eqref{eq:bdfull}) in triply periodic environments. In particular, we will solve Eq. \eqref{eq:stokes} (or Eq. \eqref{eq:navierstokes} in sec. \ref{ch:icm}) directly. As we saw in chapter \ref{sec:bdhi}, sampling the local fluid velocity to compute hydrodynamic displacements of each particle is equivalent to solving the particle's dynamics with Eq. \eqref{eq:bdfull} (or equivalently Eq. \eqref{eq:bdhibdrelation}).

All the algorithms we will learn about share some common themes, mainly: They are variations of the Immersed Boundary Method~\cite{Peskin2002} that take the gross of the computation to Fourier-space in some way (pseudo-spectral algorithms). Many complex operations (differentiations, convolutions\dots) become simple algebraic ones in Fourier space. One of the main motivations to devise pseudo-spectral algorithms is the existence of the \gls{FFT} algorithm which can take a signal, evaluated at equidistant points (i.e. a regular grid), to Fourier space in just $O(N\log(N))$ operations. It would be pointless to develop a spectral algorithm because it reduces the complexity of certain operations if we need $O(N^2)$ operations to transform a signal.

In order to numerically solve Eq. \eqref{eq:stokes} (via the Green formalism, see Eq. \eqref{eq:greenstokes}) in Fourier space we need to evaluate the fluid velocity on a grid.
A grid poses an immediate challenge when we want to compute the displacements of a group of arbitrarily located particles because we have to transform the forces acting on them to a smooth force density field defined in the same grid as the fluid. In chapter \ref{sec:bdhi} we saw how this conversion is carried out by the spreading and interpolation operators via a smoothed delta function. We will describe how to efficiently apply the spreading and interpolation operators on a \gpu in chapter \ref{sec:ibm}.



\section{Force Coupling Method (FCM)}\label{sec:fcm}
The \gls{FCM}~\cite{Keaveny2014} is an Immersed-Boundary-like Eulerian-Lagrangian pseudo-spectral method initially devised for the computation of the hydrodynamic displacements of a colloidal suspension in a triply periodic environment. In future chapters, we will see how the \gls{FCM} is quite general and can be applied outside of its originally intended applications.

Both \emph{Cholesky} and \emph{Lanczos} methods use the explicit form of an open boundary mobility. This makes them open boundary algorithms since they do not take into account the periodic images of the system in any way. Furthermore, the computational complexity of these methods is restrictive. Luckily, we can manage to do it in $O(N)$ operations if we consider periodic boundary conditions. In particular by solving Eq. \eqref{eq:bdhibdrelation} directly in Fourier space via the Force Coupling Method~\cite{Keaveny2014}. In doing so, we get the added benefit (and disadvantage) of not imposing a specific mobility tensor, which will arise naturally according to the convolution between the Green's function and the spreading kernel.

In order to do this we first spread particle forces to the fluid ($\oper{S}\vec{F}$ in Eq. \eqref{eq:bdhibdrelation}) and transform them to Fourier space. Then the Green's function (like Eq. \eqref{eq:greenstokes}) for periodic boundary conditions) is used to obtain the velocities in the fluid in Fourier space via multiplication. The fluid velocity is then transformed back to real space and Eq. \eqref{eq:bdhifvel} is applied to get the particle displacements by interpolation.
The kernel used in the original description of the \gls{FCM} is the Gaussian kernel in Eq. \eqref{eq:gaussiankernel}, in particular
\begin{equation}
  \label{eq:fcmkernel}
  \delta_a(r = ||\vec{\ppos}_i - \vec{\fpos}||) := \frac{1}{(2\pi\sigma_a)^{3/2}}\exp\left(\frac{-r^2}{2\sigma_a^2}\right)
\end{equation}
Where $\sigma_a = a/\sqrt{\pi}$. This naturally regularizes the Oseen tensor at short distances, yielding \gls{RPY}-like behavior. In fact, the mobility tensor for an unbounded three dimensional domain can be computed analytically in this case by solving the double convolution in Eq. \eqref{eq:bdhimob}. In particular, we can write the real space expressions for $f(r)$ and $g(r)$ in Eq. \eqref{eq:bdhimobgeneral}
\begin{equation}
  \label{eq:fcmmob}
  \begin{aligned}
f(r) & =  \frac{1}{8\pi\eta\,r}\left[\left(1+2\,\frac{a^{2}}{\pi\,r^{2}}\right){\erf}\left(\frac{r\sqrt{\pi}}{2a}\right)-2\frac{a}{\pi\,r}\exp\left(-\frac{\pi\,r^{2}}{4a^{2}}\right)\right]\\
g(r) & =  \frac{1}{8\pi\eta\,r}\left[\left(1-6\,\frac{a^{2}}{\pi\,r^{2}}\right){\erf}\left(\frac{r\sqrt{\pi}}{2a}\right)+6\frac{a}{\pi\,r}\exp\left(-\frac{\pi\,r^{2}}{4a^{2}}\right)\right]
  \end{aligned}
\end{equation}

The fluctuating term, $\nabla\cdot\mathcal{Z}$, is computed directly in Fourier space in the fluid via \emph{ik} differentiation. This makes evaluating the Brownian motion effectively cost free.
The computation of the fluid velocities is independent of the number of particles (since the first step is to spread the forces to the fluid). The complexity of the spreading and interpolation operations grow linearly with the number of particles. Thus the overall complexity of the algorithm is $O(N)$.

Summarizing, we can express the fluid velocity in Fourier space as
\begin{equation}
  \label{eq:fcmvel}
  \fou{\vec{\fvel}}(\vec{k}) = \eta^{-1}\hat{\tens{G}}\left(\vec{k}\cdot\fou{\mathcal{Z}} + \fou{\vec{f}}\right)
\end{equation}
Where
\begin{equation}
  \fou{\tens{G}} = B(k)\left(\mathbb{I} - \frac{\vec{k}\otimes\vec{k}}{k^2}\right)
\end{equation}
is the fourier representation of the Stokes Green's function\footnote{This separation is made here to emphasize the generality of this method for any Green's function as long as its Fourier transform is available.}.
Here $B(k)$ is a factor in Fourier space, i.e $B := \frac{1}{k^2}$ for the Oseen tensor in Eq. \eqref{eq:greenstokes}.
%\todo{Spectral splitting as in PSE can also be done here, which would allow to reduce the support by restricting the kernel to be a Gaussian, probably not worth it because of it.}
Note that we have defined the divergence of the noise as $\vec{k}\cdot\fou{\mathcal{Z}}$, without the complex $i$. We can do this given the fact that if we define $\fou{\mathcal{Z}} := i\fou{\mathcal{Z}}_1$ (being $\fou{\mathcal{Z}}_1$ a different noise with the same properties as $\fou{\mathcal{Z}}$) we still get white noise with identical properties.

The \gls{FCM} was originally developed for a Gaussian kernel, however, note that any smooth, closely-supported kernel can be used, which will regularize differently the near field hydrodynamics. In particular, any of the kernels described in sec. \ref{sec:ibm} are valid here.

\subsection*{Spatio-temporal discretization}
Assuming a cubic box (the description can be then generalized to non-cubic boxes easily) we solve the velocity of the fluid on a grid with size $h$, with a number of cells in each size $N_c = L/h$.
We use \gls{FFT} to discretize the Fourier transform, this requires us to evaluate the properties of the fluid in a grid. This grid must be fine enough to correctly describe the Gaussian interpolator in Eq. \eqref{eq:fcmkernel} (similarly to the discrete description in sec. \ref{ch:tppoisson}). On the other hand, the kernel in Eq. \eqref{eq:fcmkernel} has an infinite range and to make the overall spreading/interpolation have a constant cost for each particle (independent of the size of the domain) it is necessary to truncate it at a certain distance, $r_c$. In particular, the author in ~\cite{Keaveny2014} suggests $r_c=m \sigma_a$, with $m=3\sqrt{\pi}\approx 5.3$ being the number of standard deviations of the Gaussian that are taken into account (thus making $\delta_a(r>r_c) = 0$), and $\sigma_a/h > 1.86$, suggesting that the error is less than machine precision for $\sigma_a/h = 14.89$. We can then set a number of support cells for the kernel as $n_s = 2\ \textrm{ceil}(r_c/h)+1$.
We will later study these tolerance considerations.

The noise can be computed in Fourier space by generating Gaussian random numbers with zero mean and standard deviation given by
\begin{equation}
  \langle \fou{\mathcal{Z}}_{ik}(\vec{k})\fou{\mathcal{Z}}^*_{jm} (-\vec{k})\rangle = \frac{2\kT\eta N_c^3}{ h^3 \dt}(\delta_{ij}\delta_{km} + \delta_{im}\delta_{kj})
\end{equation}
Special care must be taken to enforce this condition, in particular by ensuring the uncoupled modes, also known as Nyquist points, are real (equal to their conjugate). Appendix \ref{ch:appendixa} provides a summary of dealing with spectral methods in numerical implementations, including these details.

Making an analogy with Eq. \eqref{eq:bdhibdrelation}, the whole process of going from forces acting on the particles to particle displacements can the be summarized as follows:
\subsubsection*{Force Coupling Method}
\begin{enumerate}
\item Spread particle forces to the grid: $\vec{f} = \oper{S}\vec{F}$
\item Transform fluid forcing to Fourier space: $\fou{\vec{f}} = \mathfrak{F}\oper{S}\vec{F}$
\item Multiply by the Green's function to get $\eta^{-1}\fou{\tens{G}}\mathfrak{F}\oper{S}\vec{F}$
\item Sum the stochastic forcing in Fourier space: $\fou{\vec{\fvel}} = \eta^{-1}\fou{\tens{G}}(\mathfrak{F}\oper{S}\vec{F} + \vec{k}\fou{\mathcal{Z}})$
\item Transform back to real space: $\vec{\fvel} = \eta^{-1}\mathfrak{F}^{-1}\fou{\tens{G}}(\mathfrak{F}\oper{S}\vec{F} + \vec{k}\fou{\mathcal{Z}})$
\item Interpolate grid velocities to particle positions: $\vec{\pvel} = \oper{J}\vec{\fvel}$
\end{enumerate}
Here $\mathfrak{F}$ represents the Fourier transform operator.
Spreading and interpolation can be done with the algorithms and kernels described in chapter \ref{sec:ibm}. 

It is important to note that the steps above constitute quite a general protocol whose range of applicability extends far beyond triply periodic hydrodynamics by reinterpreting/bending the different terms and operators. As a matter of fact, we will employ a reimagination of the Force Coupling Method to compute electrostatics in chapter \ref{ch:tppoisson}. In another instance, when discussing the \gls{ICM} (chapter \ref{ch:icm}), we will see how the Green's function, $\tens{G}$, does not have to be analytic and can, in fact, be computed numerically. For the time being, in the following chapters we will describe several modifications to the \gls{FCM} to compute hydrodynamics in different regimes.

Once the particle velocities are computed, the dynamics can be integrated using, for instance, the Euler-Maruyama scheme devised for \gls{BD} in sec. \ref{sec:bd}\footnote{The arguments used to arrive at Eq. \eqref{eq:bdhimobns} can also be employed here to interpret $\vec{\pvel}=\oper{J}\vec{\fvel}$ as the equations of Brownian Dynamics.}. The update rule in the case of Euler-Maruyama is

\begin{equation}
  \label{eq:fcmupdate}
  \vec{\ppos}^{n+1} = \vec{\ppos}^n + \vec{\pvel}^n\dt,
\end{equation}
where the particle velocities already include the stochastic displacements.

\subsection*{Regarding torques}

Torques acting on the particles can easily be introduced in the algorithm by adding them as an external force on the fluid. We can redefine the fluid forcing to include torques
\begin{equation}
\vec{f} = \oper{S}\vec{F} + \half\nabla\times(\oper{S}_\tau\vec{T})
\end{equation}
Where $\oper{S}_\tau$ is the kernel used to spread the torques, $\vec{T}$. If a Gaussian is used (see Eq. \eqref{eq:fcmkernel}), its width is related to the hydrodynamic radius as $\sigma_\tau = a/(6\sqrt{\pi})^{1/3}$.

The curl of the spread torques can easily be computed in Fourier space, where it is transformed into a vector product

\begin{equation}
\fou{\vec{f}} = \mathfrak{F}\oper{S}\vec{F} + \half i\vec{k}\times(\mathfrak{F}\oper{S}_\tau\vec{T})
\end{equation}

Similarly, once the fluid velocities are obtained in Fourier space, the angular velocities, $\vec{\omega}$ can be computed from the local fluid vorticity
\begin{equation}
  \vec{\omega} = \half\oper{J}_\tau\mathfrak{F}^{-1}\left(i\vec{k}\times\fou{\vec{\fvel}}\right)
\end{equation}


\subsection*{Use in UAMMD}
Using the \gls{FCM} strategy in \uammd is similar to using Cholesky (see sec. \ref{sec:uammdchol}). Being a non-exact algorithm\footnote{Due to the spatial discretization and the quadrature errors incurred when spreading/interpolating with a truncated kernel.}, a tolerance has to be specified and since now the domain is periodic, a domain size is also needed.

In contrast to the open boundary algorithms in sections \ref{sec:chol} and \ref{sec:lanczos} this implementation does not allow to set a different hydrodynamic radius for each particle.
Here is an example of the Euler-Maruyama integration scheme being specialized with \gls{FCM}.
\begin{code2}[Usage example of the \gls{FCM} module] {label=code:fcm}
#include<uammd.cuh>
#include<Integrator/BDHI/BDHI_EulerMaruyama.cuh>
#include<Integrator/BDHI/BDHI_FCM.cuh>
using namespace uammd;
//A function that creates and returns a BDHI integrator
// using Force Coupling Method
auto createIntegratorBDHIFCM(UAMMD sim){   
  //A strategy is mixed with an integration scheme
  using FCM = BDHI::EulerMaruyama<BDHI::FCM>;
  FCM::Parameters par;
  par.temperature = sim.par.temperature;
  par.viscosity = sim.par.viscosity;
  par.hydrodynamicRadius = sim.par.hydrodynamicRadius;
  par.dt = sim.par.dt;
  par.tolerance = sim.par.tolerance;
  par.box = sim.par.box;
  auto bdhi = std::make_shared<FCM>(sim.pd, par);
  return bdhi;
}
\end{code2}

\newpage

\section{Positively Split Ewald (PSE)}\label{sec:pse}
In \gls{FCM} the size of the grid is tied to the hydrodynamic radius of the particles. Hindering its ability to simulate either large domains or small particles.
%Furthermore, the \gls{FCM} does not imposes a mobility explicitly, making it impossible to control the near field hydrodynamic effects without modifying the shape of the spreading kernels.
We can apply an Ewald splitting strategy to overcome this limitation as described in~\cite{Fiore2017}.

In particular, we will solve Eq. \eqref{eq:bdfull} using the \gls{RPY} mobility with periodic boundary conditions. We can write the Stokes solution operator in Eq. \eqref{eq:stokesoper} for a periodic system (with $\{\vec{k}\}$ allowed wavevectors) in real space using the same strategy as in~\cite{Hasimoto1959}
\begin{equation}
  \label{eq:psestokesoper}
  \oper{L}(\vec{\fpos}) = \frac{1}{V}\sum_{\vec{k}} \frac{\exp(i\vec{k}\vec{r})}{k^2}\left(\mathbb{I} - \frac{\vec{k}\otimes\vec{k}}{k^2}\right)
\end{equation}
Where $V = L^3$ is the volume of the periodic domain.

We can now write the periodic \gls{RPY} tensor, which amounts to replacing the integral in Eq. \eqref{eq:rpymobfour} by a sum~\cite{Fiore2017}\footnote{The trick here is to take the open-boundaries \gls{RPY} tensor and sum over the system images in Fourier space, which can then be interpreted as the definition of the discrete Fourier transform.}.
\begin{equation}
  \label{eq:pserpyfou}
  \tens{M}_{ij}^{\textrm{RPY}}(\vec{r})= \frac{1}{\eta V} \sum_{\vec{k}} \frac{\exp(i\vec{k}\vec{r})}{k^2}\left(\sinc(ka) \right)^2\left(\mathbb{I} - \frac{\vec{k}\otimes\vec{k}}{k^2}\right)
\end{equation}

In \gls{PSE}, the \gls{RPY} tensor is expressed a sum of two symmetric, positive definite, operators. The first of them is spatially local and can be evaluated exactly using the algorithm in sec. \ref{sec:lanczos} (with the added benefit that most elements in the mobility tensor will be zero). The second operator is non local and its contribution can be taken into account using the same spectral algorithm as in sec. \ref{sec:fcm}.
Thus we are looking for a mobility tensor in the form
\begin{equation}
  \label{eq:psemobsep}
  \tens{M}_{ij}^{\textrm{RPY}} = \tens{M}_{ij}^{\far} + \tens{M}^{ \near}_{ij}
\end{equation}
We use the Ewald sum splitting of Hasimoto~\cite{Hasimoto1959} for Eq. \eqref{eq:pserpyfou} so that
\begin{equation}
  \label{eq:pserpyfar}
  \tens{M}_{ij}^{\far}= \frac{1}{\eta V} \sum_{\vec{k}} \frac{\exp(i\vec{k}\vec{r})}{k^2}\left(\sinc(ka) \right)^2H(k,\xi)\left(\mathbb{I} - \frac{\vec{k}\otimes\vec{k}}{k^2}\right)
\end{equation}
Where the Hasimoto splitting function is defined as
\begin{equation}
  \label{eq:psehasimoto}
  H(k,\xi) = \left(1 + \frac{k^2}{4\xi^2}\right)\exp\left(-\frac{k^2}{4\xi^2}\right)
\end{equation}
Here $\xi$ is the splitting parameter (see sec. \ref{ch:tppoisson}), an arbitrary parameter larger than zero that allows to shift the weight of the mobility from one term to the other. Thus, the far field contribution to the mobility decays rapidly in Fourier space.

On the other hand, the integral for the inverse transform of the near part can be computed analytically, this term can be written as
\begin{equation}
  \label{eq:pserpynear}
  \tens{M}_{ij}^{\near}= F(r,\xi)\mathbb I - G(r,\xi)\left(\frac{\vec{r}\otimes\vec{r}}{r^2}\right)
\end{equation}
Where the functions $F$ and $G$ are two rapidly decaying functions with really convoluted expressions that can be found in Appendix A of~\cite{Fiore2017} or in the \uammd source code\footnote{These expressions are really long and convoluted. The corresponding code evaluating the near field in Eq. \eqref{eq:pserpynear} is located in the \uammd source file \emph{RPY\_PSE.cuh}, which is written to facilitate its copy-pasting for other implementations.}. In essence, this means that the near field contribution to the mobility decays rapidly, i.e it is a compactly supported interaction.

It is worth mentioning that both contributions to the mobility in \eqref{eq:pserpynear} and \eqref{eq:pserpyfar} are guaranteed to be positive definite for all particle configurations, which is the main contribution of the original work describing the \gls{PSE}~\cite{Fiore2017}.
For our purposes this means that we can use the techniques we have already devised in previous sections to evaluate each term. The deterministic term (the multiplication of the mobility by the forces acting on the particles) can simply be computed separately (in the far and near fields) and then added.

The fluctuations pose more of a challenge if we want to refrain from computing and storing the square root of the mobility. Nonetheless, we can split the fluctuating contribution in Eq. \eqref{eq:bdfull} into a far and near field ones by defining
\begin{equation}
  \label{eq:psenoise}
  (\tens{M}^{\textrm{RPY}})^{1/2} d\vec{\noise} :=  (\tens{M}^{\near})^{1/2} d\vec{\noise}_1 + (\tens{M}^{\far})^{1/2} d\vec{\noise}_2,
\end{equation}
Where $\vec{\noise}_{1,2}$ are independent Wiener processes. It can be easily proven (by studying the covariance of the noise) that this expression does indeed provide the correct fluctuation-dissipation balance with the mobility in Eq. \eqref{eq:psemobsep}, since both Wiener processes in the right hand side are uncorrelated. 

Let's see in more detail how to adapt our previous algorithms to solve each part of the problem

\subsection*{The far field}

The far field computation is identical to the previously laid out \gls{FCM} with a couple of trivial changes.
First, the Green's function has to be modified, in particular, we need to redefine the Fourier factor in Eq. \eqref{eq:fcmvel}
\begin{equation}
  B(k) := \frac{1}{k^2V}H(k,\xi)\exp\left(\frac{-k^2\lambda}{4\xi^2}\right)\sinc(ka)^2,
\end{equation}
which describes the Green's function associated with the far field \gls{RPY} mobility in Eq. \eqref{eq:pserpyfar}. For the \gls{FCM} based on a Gaussian kernel (see Eq. \eqref{eq:fcmkernel}) we have introduced a second splitting parameter, $\lambda$, to split the exponential in $H$ (a common strategy~\cite{Lindbo2011}~\cite{Wang2016}). Thus, this corresponds to a Gaussian (\gls{FCM}) kernel with variance $\sigma_a := \frac{\sqrt{\lambda}}{2\xi}$ (in real space). On the other hand, this restricts the far field spreading kernel to a Gaussian, without the possibility of using other more closely supported kernels easily, since the splitting requires knowledge of the Fourier transform of the kernel.

The cell size has to be chosen according to a certain cut-off wave number in Eq. \eqref{eq:pserpyfar}. The authors in ~\cite{Lindbo2010} give an upper bound for the truncation error of the fourier sum as $E_f \approx \exp(-k_{\textrm{cut}}^2/4\xi^2)$. Thus we can set $E_f$ to be less than the tolerance, $\epsilon$, and set
\begin{equation}
  k_{\textrm{cut}} = 2\xi\sqrt{-\log(\epsilon)}
\end{equation}
From here we can compute the necessary grid cell size as $h = \pi/k_{\textrm{cut}}$.

As evidenced by this new definition of $\sigma_a$, the spreading kernel is now uncoupled from the hydrodynamic radius. The width and support of the kernel can now be chosen to minimize quadrature and truncation errors. In particular the authors in ~\cite{Lindbo2011} suggest choosing $m = C\sqrt{\pi n_s}$, where $C = 0.976$ is an empirical parameter. The number of standard deviations, $m$, can be computed by making the quadrature error, $E_q \approx \textrm{erfc}(m/\sqrt{2})$, less than a certain tolerance, $E_q<\epsilon$(see sec. 3.2 in ~\cite{Lindbo2011}). We can then set the splitting parameter as $\lambda = \left(\frac{hn_s\xi}{m}\right)^2$.

As demonstrated in section \ref{sec:bdhicon}, there is a one-to-one relation between the purely Lagrangian Brownian Dynamics representation (via the mobility tensor) and the Green formalism used by the \gls{FCM}. Naturally, this equivalence also includes the fluctuations. We compute the far field fluctuating contribution via the stochastic stress tensor, $\mathcal{Z}$, (see, for instance, Eq. \eqref{eq:fcmvel} and related discussion) which is already included in the \gls{FCM} in a natural way. The treatment of the fluctuating stress tensor in \gls{FCM} holds for any hydrodynamic Green's function as long as the divergence operator operator ($\nabla\cdot$) can be applied (which is the case in \gls{PSE} since we are using triply periodic boundary conditions).
%\begin{equation}
%  \tilde{H}(k,\xi,\tau) :=  \left(1 + \frac{k^2}{4\xi^2}\right)\exp\left(-\frac{k^2}{4\xi^2}\right)
%\end{equation}
\subsection*{The near field}

For the near field, we can use the infrastructure in the \emph{Lanczos} algorithm (see sec. \ref{sec:lanczos}) with the mobility in Eq. \eqref{eq:pserpynear}. We can leverage here the fact that only local terms in the mobility matrix are non-zero (i.e. elements owing to close particles) to accelerate the matrix-vector multiplication. In \uammd, this is achieved by using a neighbour list with a \emph{Transverser} that takes the forces acting on the particles and computes the product with the mobility for each particle (see sec. \ref{sec:transverser}). The same \emph{Transverser} is then used to compute the fluctuations via the \emph{Lanczos} algorithm.

The functions $F$ and $G$ in Eq. \eqref{eq:pserpynear} are evaluated in double precision to reduce numerical errors and then tabulated\footnote{\uammd provides an infrastructre for tabulating functions called \emph{TabulatedFunction}. Check its documentation for more information.}. Both of them are truncated up to a certain distance, $r_{\textrm{cut}}^{\textrm{nf}}$. According to ~\cite{Lindbo2011} the real space truncation error is bounded by $E_n\approx \exp(-\xi^2r_{\textrm{cut}}^{\textrm{nf}})$. Thus, we can choose
\begin{equation}
r_{\textrm{cut}}^{\textrm{nf}} = \frac{\sqrt{-\log(\epsilon)}}{\xi}
\end{equation}

Although the cost of computing the near field can sometimes negate the potential gains of using \gls{PSE} over \gls{FCM} (mostly due to the overhead of using an iterative Lanczos algorithm for the noise), it is important to note that if the near field cut off radius is less than the minimum distance allowed for a pair of particles this part of the algorithm can be skipped altogether (with the exception of the self terms in the mobility matrix). For instance, in the presence of some kind of steric repulsion (a la \gls{LJ} or WCA).

Finally the optimal Hasimoto splitting parameter, $\xi$, must be tuned on a case by case basis. In general, lower values of $\xi$ will work better for systems with low density (and the other way around). However, this will also depend on the specific implementation of the near and far field components besides particle configuration. In \uammd, choosing $\xi a \approx 0.6$ is usually a good default.

\subsubsection*{Use in UAMMD}

Usage is similar to \gls{FCM}, with the difference that now a splitting parameter ($\xi$ in Eq. \eqref{eq:psehasimoto}) can be selected to shift the weight of the algorithm between the near and far field. In particular, lower values $\xi a$ give more weight to the near field, while higher values give more importance to the far field.

\begin{code2}[Example of the creation of the \gls{PSE} module.]  {label=code:pse}
#include<uammd.cuh>
#include<Integrator/BDHI/BDHI_EulerMaruyama.cuh>
#include<Integrator/BDHI/BDHI_PSE.cuh>
using namespace uammd;
//A function that creates and returns a BDHI integrator
// using Positively Split Ewald
auto createIntegratorBDHIPSE(UAMMD sim){   
  //A strategy is mixed with an integration scheme
  using PSE = BDHI::EulerMaruyama<BDHI::PSE>;
  PSE::Parameters par;
  par.temperature = sim.par.temperature;
  par.viscosity = sim.par.viscosity;
  par.hydrodynamicRadius = sim.par.hydrodynamicRadius;
  par.dt = sim.par.dt;
  par.tolerance = sim.par.tolerance;
  par.box = sim.par.box;
  //The Ewald splitting parameter
  par.psi = sim.par.psi;
  auto bdhi = std::make_shared<PSE>(sim.pd, par);
  return bdhi;
}
\end{code2}

\newpage
\section{Spatial discretization with staggered grids}\label{sec:staggered}

Thus far our spatial discretization has assumed a regular cartesian grid with all properties (fluid velocity and forcing) defined in the centers of the cell. This is known as a collocated grid. In non trivial geometries (such as in the presence of walls) the discretization of the different differential operators in Eq. \eqref{eq:navierstokes} could lead to the projection operator, $\oper{P}$ not being exactly idempotent, so that $\oper{P}^2 \ne \oper{P}$. This can lead to inaccuracies in the temporal discretization and ultimately to the discrete fluctuation-dissipation balance not being satisfied. Additionally, a collocated grid can lead to numerical artifacts in the presence of boundaries (such as walls) due to the fluid properties not being defined exactly on them and other aliasing issues that can affect high frequency phenomena~\cite{Ferziger2002,Meier1999}.

Furthermore, when studying transport phenomena in compressible fluids, collocated grids present spurious dependencies with the wave vector~\cite{BalboaTesis}.

An alternative is to use a so-called MAC (Marker and cell), or staggered, grid~\cite{Balboa2012}.

\begin{figure}[h]
  \centering
  \includesvg[width=\columnwidth]{gfx/staggered}
  \caption[ ]{Representation of a staggered grid. Each quantity is defined on its own grid, with origin (circles) shifted by $h/2$ between them. Crosses mark cell centers in the various grids. The $x$ coordinates of vectors are defined on the red grid, $y$ coordinates on the green. Tensors are defined on the orange one and finally, scalars are defined on the black grid.}
  \label{fig:staggered}  
\end{figure}

Although it can be tricky to work with a staggered grid in a numerical implementation, the key is to interpret that each quantity is defined on a different grid, shifted $h/2$ with respect to the others (see Fig. \ref{fig:staggered}). When we need to work on quantities defined on different grids, we simply use linear interpolation.
From the point of view of the ``main'' grid (in which scalars are defined at the centers), vector quantities are defined on cell faces, while tensors are located at cell corners. If we need to multiply, for instance, a scalar, $s$, and a vector, $\vec{v}$, we interpolate the scalar at the same location as each coordinate of the vector. For example, in the $x$ direction:
\begin{equation}
  g^x_{i,j} = s_{i-1/2,j} v^x_{i,j} = \half( s_{i,j} + s_{i-1,j})v^x_{i,j}
\end{equation}
Where $i$,$j$ represent the indexes in the relevant ``subgrid'' (the one for the $x$ direction in this case, marked as red in Fig. \ref{fig:staggered}).
We can use this to discretize the different operators in Eq. \eqref{eq:stokesoper} via finite differences.
The divergence of a vector is a scalar, defined in the centers of the main grid (black in Fig. \ref{fig:staggered})
\begin{equation}
  \label{eq:staggereddiv}
  (\nabla\cdot\vec{v})_{\vec{i}} = \frac{1}{h}\sum_\alpha(v^\alpha_{\vec{i} + \vec{\hat{\alpha}}} - v^\alpha_{\vec{i}})
\end{equation}
Where $\vec{i}:=(i,j,k)$ and $\vec{\hat{\alpha}}:=(\delta_{x\alpha}, \delta_{y\alpha}, \delta_{x\alpha})$ is only non-zero in the direction $\alpha$.

The gradient of a scalar is a vector, in which each component is defined on a different cell face
\begin{equation}
  \label{eq:staggeredgrad}
  (\nabla s)^\alpha_{\vec{i}} = \frac{1}{h}(s_{\vec{i}} - s_{\vec{i}- \vec{\hat{\alpha}}})
\end{equation}
Joining these two operators we can compute the Laplacian
\begin{equation}
  (\nabla^2\vec{v})^\alpha_{\vec{i}} = \nabla\cdot(\nabla v_{\vec{i}}^\alpha) = \frac{1}{h^2}\sum_\beta\left(v^\alpha_{\vec{i} +\vec{\hat{\beta}}}  - 2v^\alpha_{\vec{i}} + v^\alpha_{\vec{i} -\vec{\hat{\beta}}} \right)
\end{equation}
And the gradient of the divergence
\begin{equation}
  \label{eq:staggeredlap}
\left[\nabla(\nabla\cdot \vec{v})\right]^\alpha_{\vec{i}} = \frac{1}{h^2}\sum_\beta\left(v^\beta_{\vec{i} +\vec{\hat{\beta}}}  - v^\beta_{\vec{i}} + v^\beta_{\vec{i} -\vec{\hat{\alpha}} + \vec{\hat{\beta}}} - v^\beta_{\vec{i} -\vec{\hat{\alpha}}}\right)
\end{equation}
Where we recall that each component of a vector is defined on its own grid (see Fig. \ref{fig:staggered}) (so a component of a vector with index $i$ is defined at a different location from a scalar with the same index). The specific index notation here, assigning the same indexes $(i,j)$ to the different quantities defined on the grid, aims to facilitate a computer implementation. With this notation everything can be stored as if it were defined on a collocated grid. Then, when applying the operators above, each element (vector or scalar) can be accessed with the same indexes as in these equations.

%\todo{I do not really know why a staggered grid is better for our uses}

%In general, \gls{BC}s in the previous methods have to be introduced via the Green's function in Eq. \eqref{eq:bdhimob}. However, this Green's function might be unknown or simply non analytic. We can discretize and solve Eq. \eqref{eq:bdhibdrelation} directly to have more fine control over the \gls{BC}s.

\section{Fluctuating Immersed Boundary (FIB) in triply periodic systems}\label{sec:fib}
Recently, a new spatio-temporal solver for the Navier-Stokes equation has been presented, based on the \gls{IBM} and similar to the \gls{FCM}~\cite{Delong2014}. This framework, refered to as \gls{FIB}, is generic for any geometry (via direct discretization of the differential operators in Eq. \eqref{eq:stokesoper}). However, here we will only take a few key ideas from it to implement another triply periodic solver for equation \eqref{eq:stokes}. In particular, we will make use of the new temporal integrators introduced in~\cite{Delong2014}. Another difference between this algorithm and \gls{FCM} (as presented in sec. \ref{sec:fcm}) is the use of a staggered grid (see sec. \ref{sec:staggered}).

Using \gls{PBC} greatly simplifies the algorithm in ~\cite{Delong2014} to something very similar to the \gls{FCM}, since, as discussed in sec. \ref{sec:bdhi} the Stokes operator has a straightforward form in Fourier space (see Eq. \eqref{eq:greenstokes}).

Summarizing, for our purposes, we can describe the \gls{FIB} as a variation of the \gls{FCM} for staggered grids and a more sophisticated temporal integration algorithm. The authors of ~\cite{Delong2013} developed the \gls{FIB} for a three point Peskin kernel (see Eq. \eqref{eq:peskin3}) but, similarly to what was discussed in sec. \ref{sec:fcm}, any of the kernels in sec. \ref{sec:ibm} can be used.

The discretization of the Stokes operator in Fourier space is straight-forward for a collocated grid by using the Fourier representation of the differential operators in Eq. \eqref{eq:stokesoper} (a technique known as \emph{ik} differentiation). However, for a staggered grid we have defined the differential operators using finite differences (see Eqs. \eqref{eq:staggereddiv} and \eqref{eq:staggeredgrad}). It is then necessary to redefine the discretized Fourier operators. We can leverage a property of the Fourier transform for this; a shift in space results in a shift in phase in Fourier space. So from the Fourier transform of some scalar quantity (defined in the centers of the main grid in Fig. \ref{fig:staggered}), we can compute the Fourier transform of the gradient by shifting the scalar to $x+h$ and then shifting the result back to $x+h/2$ (where vectors like the gradient are defined).

So by taking the Fourier transform of Eq. \eqref{eq:staggeredgrad}
\begin{equation}
  \label{eq:staggeredgradfou}
  \fou{\nabla s} = \frac{\exp(-i2\pi \vec{k} h/2)}{h}\left[\fou{s}_{\vec{k}} - \exp(i2\pi \vec{k} h)\fou{s}_{\vec{k}}\right] = \frac{2i}{h}\sin\left(\frac{\vec{k} h}{2}\right)\fou{s}_{\vec{k}} = i\vec{k}_{\text{eff}}\fou{s}_{\vec{k}}
\end{equation}
Where the effective wave vector
\begin{equation}
  \vec{k}_{\text{eff}} = \frac{2}{h}\sin\left(\frac{\vec{k}h}{2}\right)
\end{equation}
Allows to make an equivalence with the discrete differential operators for a collocated grid.

Note that if the gradient of a vector, like the velocity, is to be computed this way, each element must first be shifted to the scalar grid (by applying a phase of $\pm h/2$ in Fourier space).

The divergence can be computed using a similar procedure:

\begin{equation}
  \label{eq:staggereddivfou}
  \fou{\nabla\cdot\vec{v}} = \frac{2i}{h}\sum_\alpha\sin\left(\frac{k_\alpha h}{2}\right)\fou{v}_{\vec{k}}^\alpha = i \sum_\alpha k^\alpha_{\text{eff}}\fou{v}_{\vec{k}}^\alpha
\end{equation}

And finally the Laplacian:
%\todo{Check this}
\begin{equation}
  \label{eq:staggeredlapfou}
  \left(\fou{\nabla^2\vec{v}}\right)^ \alpha = \frac{-4}{h^2}\sin^2\left(\frac{k_\alpha h}{2}\right)\fou{v}_{\vec{k}}^\alpha = - \left(k^\alpha_{\text{eff}}\right)^2\fou{v}_{\vec{k}}^\alpha
\end{equation}

For convenience, we can now write the discrete version of the projection operator in Eq. \eqref{eq:projectoper} in Fourier space for a staggered grid in a triply periodic system as

\begin{equation}
  \label{eq:staggeredprojection}
  \fou{\oper{P}}(\vec{k}) = \left(\mathbb{I} - \frac{\vec{k}_{\text{eff}}\otimes\vec{k}_{\text{eff}}}{k_{\text{eff}}^2}\right)
\end{equation}
\subsection*{Temporal integration}
The \gls{FIB}, as presented in ~\cite{Delong2014}, comes with two new temporal integrator schemes particularly aimed at easing the discretization of thermal drift term. However, in our triply periodic implementation the thermal drift term is strictly zero. Nonetheless we will describe these integration schemes since they can come in handy in the future. On the other hand, even when the thermal drift is mathematically zero in the continuum limit it is possible that our discretization causes some spurious drift, in which case it should also be included in our description.
Taking into account that there are other sources of error it might be worth avoiding the extra work involved in the thermal drift computation (which amounts to two extra spreading operations).\uammd's implementation acknowledges this by computing the thermal drift using random finite differences (as described in section \ref{sec:bdhicon}) and providing a parameter to switch its inclusion on or off.

\subsubsection*{Simple predictor corrector}
This first-order weak accurate midpoint scheme requires a single Stokes solve and interpolating fluid velocities twice (at the current time, $t=n$, and the mid point, $t=n+1/2$).
\begin{equation}
  \begin{aligned}
    &\vec{\fvel} = \eta^{-1}\mathfrak{F}^{-1}\fou{\nabla^{-2}}\fou{\oper{P}}(\mathfrak{F}\oper{S}^{n}\vec{F}^{n} + \nabla\cdot\mathcal{Z}^{n})\\
    &\vec{\ppos}^{n+\half} = \vec{\ppos}^n + \frac{\dt}{2}\oper{J}^n\vec{\fvel}\\
    &\vec{\ppos}^{n+1} = \vec{\ppos}^n + \dt\oper{J}^{n+\half}\vec{\fvel}
  \end{aligned}
\end{equation}
Note that we are again abusing the operator notation by writing the Fourier representation of the inverse of the Laplacian as $\fou\nabla^{-2}$. Being the inverse Laplacian an operator that returns the solution to the equation $\nabla^2\vec{x} = \vec{f}$. Since we are describing a triply periodic algorithm the Laplacian is self-adjoint and this equation has a trivial solution in Fourier space as $\fou{v}^\alpha = \left(k_{\text{eff}}^\alpha\right)^{-2}\fou{f}^\alpha$ (following the discrete definition of the Laplacian in a staggered grid from Eq. \eqref{eq:staggeredlapfou}).
%\todo{Something more about it...}
\subsubsection*{Improved predictor corrector}
We can achieve second-order accuracy in exchange for an additional Stokes solve by improving the estimation of the velocity at the midpoint. The update rule in this case is 
\begin{equation}
  \begin{aligned}
    &\vec{\fvel} = \eta^{-1}\mathfrak{F}^{-1}\fou{\nabla^{-2}}\fou{\oper{P}}\mathfrak{F}(\oper{S}^{n}\vec{F}^{n} + \nabla\cdot\mathcal{Z}^{n_1})\\
    &\vec{\ppos}^{n+\half} = \vec{\ppos}^n + \frac{\dt}{2}\oper{J}^n\vec{\fvel}\\
    &\tilde{\vec{\fvel}}= \eta^{-1}\mathfrak{F}^{-1}\fou{\nabla^{-2}}\fou{\oper{P}}\mathfrak{F}\left[\oper{S}^{n+\half}\vec{F}^{n+\half} + \half\nabla\cdot\left(\mathcal{Z}^{n_1} +  \mathcal{Z}^{n_2}\right)\right]\\
    &\vec{\ppos}^{n+1}= \vec{\ppos}^n + \dt\oper{J}^{n+\half}\tilde{\vec{\fvel}}
  \end{aligned}      
\end{equation}
The divergence operator, whose application is required in real space to compute the noise term, is discretized according to the spatial discretization (see for instance Eq. \eqref{eq:staggereddiv} in a staggered grid). Similarly for the rest of the differential operators which are applied in Fourier space.

Since in a staggered grid each component of the velocity is defined on a different grid but with the exact same geometry there is no need for special considerations when Fourier transforming the fluid forcing or velocities using the \gls{FFT} (see Appendix \ref{ch:appendixa}).
\subsection*{Use in UAMMD}
Usage of the \gls{FIB} is reminiscent of other \gls{BD} \emph{Integrators} we have seen thus far. In the current case there is an additional parameter, \mintinline{\ucpp}{scheme}, which allows to choose between either of the two integration schemes described in the previous section.

\begin{code2}[Example of the creation of a \emph{FIB} \emph{Integrator} module.]{label=code:fib}
#include<uammd.cuh>
#include<Integrator/BDHI/FIB.cuh>
using namespace uammd;
//A function that creates and returns an FIB integrator
auto createIntegratorFIB(UAMMD sim){   
  BDHI::FIB::Parameters par;
  par.temperature = sim.par.temperature;
  par.viscosity = sim.par.viscosity;
  par.hydrodynamicRadius = sim.par.hydrodynamicRadius;
  par.dt = sim.par.dt;  
  par.box = sim.par.box;
  //The integration scheme
  par.scheme = BDHI::FIB::IMPROVED_MIDPOINT;
  //par.scheme = BDHI::FIB::MIDPOINT;
  auto fib = std::make_shared<BDHI::FIB>(sim.pd, par);
  return fib;
}
\end{code2}





\section{Inertial Coupling Method (ICM)}\label{ch:icm}
Thus far we have neglected the inertial terms in Eq. \eqref{eq:navierstokes} and focused on the Stokes level (in what we call \gls{BDHI}). In this last chapter about triply periodic hydrodynamics we will explore an algorithm, refered to as \gls{ICM}~\cite{Balboa2014}, that allows to reintroduce inertia. In particular, we will consider both the temporal and convective inertial terms in Eq. \eqref{eq:navierstokes}.

Note, however, that we will not consider particle inertial terms coming from an excess mass of the particles, i.e we consider neutrally buoyant particles with $m_e := m - \rho\Delta V = 0$. This implies that the particles still follow the local fluid velocity exactly and particles reach terminal velocity instantaneously. As demonstrated in ~\cite{Balboa2014} particle excess mass (inertial) effects can be reintroduced as a constraint via a Lagrange multiplier.

We will use the same staggered grid spatial discretization as in \gls{FIB} (see chapter \ref{sec:staggered}).
Using the proyection method descibed in chapter \ref{sec:bdhi}, we can write Eq. \eqref{eq:navierstokes} as
\begin{equation}
  \dot{\vec{\fvel}} = \rho^{-1} \oper{P}\left(\vec{\mathfrak{f}} + \tilde{\vec{f}}\right).
\end{equation}
Where we have introduced a new fluid forcing,
\begin{equation}
  \vec{\mathfrak{f}} = -\rho\nabla\cdot (\vec{\fvel}\otimes\vec{\fvel}) + \eta\nabla^2\vec{\fvel},
\end{equation}
that includes the advective and diffusive terms to simplify the notation.
We apply the projection operator in Fourier space, as we did in, for instance, sec. \ref{sec:fcm}. Since we now have to solve the temporal variation of the velocity and we have non-linear terms, the diffusive and advective terms will be evaluated in real space. In the \gls{ICM}, the divergence of the noise is also evaluated in real space.

We use a second-order accurate\footnote{A non-zero excess mass will demote the scheme to first-order accuracy.} predictor-corrector scheme for temporal discretization. We can discretize the coupled fluid-particle equations as
\begin{equation}
  \label{eq:icmalgo}
  \begin{aligned}
    &\vec{\ppos}^{n+\half} = \vec{\ppos}^n + \frac{\dt}{2}\oper{J}^n\vec{\fvel}^n,\\
    &\rho\frac{\vec{\fvel}^{n+1} - \vec{\fvel}^n}{\dt} = \oper{P}\left(\vec{\mathfrak{f}}^{n+\half} + \tilde{\vec{f}}^{n+\half} \right),\\
    &\vec{\ppos}^{n+1} = \vec{\ppos}^n + \frac{\dt}{2}\oper{J}^{n+\half}\left(\vec{\fvel}^{n+1} + \vec{\fvel}^{n}\right).
  \end{aligned}
\end{equation}
Which requires evaluating the non-linear fluid forcing terms at mid step (i.e advection and diffusion).
This discretization is similar to the midpoint predictor-corrector schemes in \gls{FIB} but the convective term is discretized using a second order explicit Adams-Bashforth method (Eq. 35 in ~\cite{Balboa2014}),
\begin{equation}
  \nabla\cdot (\vec{\fvel}\otimes\vec{\fvel})^{n+\half} = \frac{3}{2} \nabla\cdot (\vec{\fvel}\otimes\vec{\fvel})^n - \half \nabla\cdot (\vec{\fvel}\otimes\vec{\fvel})^{n-1}.
\end{equation}
Advection is therefore stored each step to be reused in the next.
The diffusive term is similarly discretized to second-order by
\begin{equation}
  \nabla^2\vec{\fvel}^{n+\half} = \half\nabla^2\left(\vec{\fvel}^{n+1} + \vec{\fvel}^{n}\right).
\end{equation}
Replacing both equations into \eqref{eq:icmalgo} and solving for the velocity at $n+1$ leads to the full form of the velocity solve, depending only on the velocity from previous time steps
\begin{equation}
  \label{eq:icmfluidvel}
  \begin{aligned}
    &\vec{\fvel}^{n+1} = \tilde{\oper{P}}\vec{g}^n =\tilde{\oper{P}}\Big[    \left(\frac{\rho}{\dt}\mathbb{I} + \frac{\eta}{2}\nabla^2\right)\vec{\fvel}^n- \\
    & \frac{3\dt}{2} \nabla\cdot (\vec{\fvel}\otimes\vec{\fvel})^n - \frac{\dt}{2} \nabla\cdot (\vec{\fvel}\otimes\vec{\fvel})^{n-1}+\\
    &\oper{S}\vec{F}^{n+\half} + \nabla\cdot\mathcal{Z}^n \Big],
  \end{aligned}
\end{equation}
where the modified projection operator is defined as
\begin{equation}
  \tilde{\oper{P}} :=\left(\frac{\rho}{\dt}\mathbb{I} - \frac{\eta}{2}\nabla^2\right)^{-1}\oper{P}
\end{equation}
and is applied in Fourier space.
The full algorithm can be summarized as follows:
\begin{enumerate}
\item Take particle positions to time $n+\half$: $\vec{\ppos}^{n+\half} = \vec{\ppos}^n + \frac{\dt}{2}\oper{J}^n\vec{\fvel}^n$.
\item Spread forces on particles to the staggered grid: $\oper{S}\vec{F}^{n+\half}$.
\item Compute and store advection: $\nabla\cdot (\vec{\fvel}\otimes\vec{\fvel})^n$.
\item Compute the rest of the terms in $\vec{g}$ in Eq. \eqref{eq:icmfluidvel}, using the advective term just computed in addition to the one stored in the previous step.
\item Take $\vec{g}$ to Fourier space and apply $\tilde{\oper{P}}$: $\fou{\vec{\fvel}}^{n+1} = \fou{\tilde{\oper{P}}}\fou{\vec{g}}$.
\item Take $\fou{\vec{\fvel}}^{n+1}$ back to real space.
\item Evaluate particle positions at $n+1$ by interpolating: $\vec{\ppos}^{n+1} = \vec{\ppos}^n + \frac{\dt}{2}\oper{J}^{n+\half}\left(\vec{\fvel}^{n+1} + \vec{\fvel}^{n}\right)$.
\end{enumerate}
Since we are describing a triply periodic algorithm, we can use the discrete form of the differential operators for a staggered grid devised in previous sections (see sec. \ref{sec:staggered} and \ref{sec:fib}).

%\todo{Some test of the algorithm, maybe the MSD of a particle? or a high Reynolds plot of the fluid velocity around a sphere}

\subsection*{Use in UAMMD}
Usage of the \emph{ICM} \emph{Integrator} requires a list of the familiar parameters for hydrodynamics thus far plus the fluid density, which for the first time plays a role.

\begin{code2}[Example of the creation of a \emph{ICM} \emph{Integrator} module.]{label=code:icm}
#include<uammd.cuh>
#include<Integrator/Hydro/ICM.cuh>
//A function that creates and returns an ICM integrator
auto createIntegratorICM(UAMMD sim){
  Hydro::ICM::Parameters par;
  par.temperature = sim.par.temperature;
  par.viscosity = sim.par.viscosity; //Fluid viscosity
  par.density = sim.par.density;   //Fluid density
  par.hydrodynamicRadius = sim.par.hydrodynamicRadius;
  par.dt = sim.par.dt;
  par.box = Box(sim.par.L);
  return std::make_shared<Hydro::ICM>(sim.pd, par);
}
\end{code2}

All the algorithms laid out in this chapter make use of the spreading and interpolator operators first introduced in chapter \ref{sec:bdhi}. For this matter, let us delve into the Eulerian-Lagrangian coupling found in the \gls{IBM} and how to efficiently encode it in a \gpu.

\chapter{The Immersed Boundary Method (IBM) kernels}\label{sec:ibm}
If we want to directly solve Eq. \eqref{eq:bdhibdrelation} instead of imposing a specific mobility (perhaps because we do not know the analytical form of the Green's function for a particular geometry), we need to discretize the spreading and interpolation operators in Eqs. \eqref{eq:spreadoper} and \eqref{eq:bdhifvel}. However, we cannot directly discretize deltas and we would like to take into account the size of the particles. We could smear the delta into a Gaussian, but the Gaussian kernels come with some inconveniences. In particular, their infinite range forces us to truncate them at a certain length. We would like to reduce the support of our kernels as much as possible without damaging accuracy. We can borrow some lessons from the \gls{IBM} to help us in this regard.

The \gls{IBM}~\cite{Peskin1977}~\cite{Peskin2002} is a mathematical framework for simulation of fluid-structure interaction. It is commonly used to discretize the spreading and interpolation operations we have seen in sec. \ref{sec:bdhi}\footnote{Which incidentally happen to be identical to the ones for electrostatics (sec. \ref{ch:tppoisson}).}. In \gls{IBM} the forces acting on a certain marker (particle) are distributed (spread) to the nearby fluid grid points via some smeared delta function (see figure \ref{fig:ibm}). \gls{IBM} offers us a way to discretize the spreading (Eq. \eqref{eq:spreadoper}) and interpolation (Eq. \eqref{eq:bdhifvel}) operators via some sophisticated $\delta_a$ kernels often referred to as Peskin kernels\footnote{In honor of their creator~\cite{Peskin1977}.}.
Furthermore, the spreading and interpolation algorithms that we are going to devise for its implementation will also be applicable to other situations outside its intended purpose. For instance, to spread charges and interpolate electric fields when solving the Poisson equation (see sec. \ref{ch:tppoisson} or \ref{ch:dppoisson}). In general, we can use the spreading and interpolation algorithms here for transforming between a Lagrangian (particles) and an Eulerian (grid) description.

We will discretize the interpolation operator in Eq. \eqref{eq:bdhifvel} on a grid as

\begin{equation}
  \label{eq:interpdiscrete}
  \oper{J}_{\vec{\ppos}_i}\vec{\fvel} = \sum_j{\delta_a(\vec{\ppos}_i - \vec{\fpos}_j)\vec{\fvel}_jdV(j)}
\end{equation}

Where the sum goes over all the cells, $j$, in the grid, with centers at $\vec{\fpos}_j$. The function $dV(j)$ are the quadrature weights (i.e the volume of the cell). For a regular grid with cell size $h$, the quadrature weights are simply $dV(j) := h^3$, but we will see other geometries in chapter \ref{ch:dppoisson}.


\begin{figure}[h]
  \centering
  \includesvg[width=0.7\columnwidth]{gfx/ibm}
  \caption[ ]{A representation of the Immersed Boundary. The blue circle represents a particle (with the green cross marking its center). Some quantity (i.e. the force) acting on it will be spread to the grid points inside its radius of action(red crosses).}
  \label{fig:ibm}  
\end{figure}

A thorough description of the whole \gls{IBM} framework can be found at~\cite{Peskin2002}, including the equations of motion for an arbitrary structure submerged in an incompressible fluid. However, we will only make use of the properties devised for the kernels.

In particular, Peskin kernels must abide by a series of postulates that intend to maximize computational efficiency (which translates to closer support) while minimizing the discretization effect of the grid (such as translational invariance).

For the sake of simplicity, the first postulate consists in assuming the kernel can be separated as
\begin{equation}
  \label{eq:peskinseparable}
  \delta_a(\vec{r}=(x,y,z)) =\frac{1}{h^3}\phi\left(\frac{x}{h}\right)\phi\left(\frac{y}{h}\right)\phi\left(\frac{z}{h}\right)
\end{equation}
This allows to state the postulates regarding the one-dimensional function, $\phi$. Additionally, this form yields $\delta_a\rightarrow\delta$ as $h\rightarrow 0$.
The second postulate is that $\phi(r)$ must be continuous for $r\in\mathbb R$, avoiding jumps in the quantities spread to, or interpolated from, the grid. The close support postulate says that $\phi(r>r_c) = 0$, being $r_c$ a cut off radius. This is our main means for seeking computational efficiency, since reducing the support of the kernel by one cell reduces dramatically the required operations. In particular, if the kernel has a support of $n_s$ cells in each direction, spreading or interpolating requires visiting $n_s^3$ nearby cells, so a support of $n_s=5$ requires $125$ cells while a support of $3$ requires just $27$. Note that the support, $n_s$ must be large enough to include all cells within $r_c$ of the point to spread. In the case of a regular grid, this can be achieved by choosing $n_s \ge 2r_c/h+1$.

The last basic postulate is required for the kernel to conserve the communicated quantities and it is simply a discrete expression of the fact that the kernel must integrate to unity.
\begin{equation}
  \label{eq:peskinunity}
  \sum_j \phi(r-j) = 1 \textrm{ for } r\in\mathbb R
\end{equation}
Where $j$ are the centers or the cells inside the support.
The next postulate intends to enforce the translational invariance of the distributed quantities as much as possible.
\begin{equation}
  \label{eq:peskinsumsquares}
  \sum_j\left(\phi(r-j)\right)^2 = C \textrm{ for any } r\in\mathbb R
\end{equation}
Where $C$ is some constant to be determined. Eq. \eqref{eq:peskinsumsquares} is a weaker version of the condition for exact grid translational invariance
\begin{equation}
  \sum_j\phi(r_1-j)\phi(r_2-j) = \Phi(r_1-r_2)
\end{equation}
Which states that the coupling between any two points must be a function of their distance. However, it can be shown that satisfying this condition is incompatible with a compact support~\cite{Peskin2002}. Eq. \eqref{eq:peskinsumsquares} attempts to guarantee some degree of translational invariance by imposing a condition on the point with maximum coupling, $r_1 = r_2$.

Finally, we can impose conditions on the conservation of the first $n$ moments to get increasingly higher order accuracy interpolants (at the expense of wider support)
\begin{equation}
  \sum_j(r-j)^n\phi(r-j) = K_n
\end{equation}
Where $K_n$ are some constants. Note that the zeroth moment condition corresponds to Eq. \eqref{eq:peskinunity} with $K_0 = 1$.
By solving the system of equations given by these conditions, different kernels can be found. 

\subsubsection*{3-point Peskin kernel}
In particular, enforcing only the condition for the first moment (with $K_1=0$) we arrive at the so-called 3-point Peskin kernel.

\begin{equation}
  \label{eq:peskin3}
  \phi_{p_3}(|r|) =  \left\{
  \begin{aligned}
    & \frac{1}{3}\left( 1 + \sqrt{1-3r^2}\right)& r < 0.5\\
    & \frac{1}{6}\left(5-3r-\sqrt{1-3(1-r)^2}\right)& r < 1.5\\
    & 0 & r>1.5 
  \end{aligned}\right.
\end{equation}
Where the argument $|r|$ represents the fact that the above expression must be evaluated for the absolute value of the separation (since the kernel is symmetrical).

\subsubsection*{4-point Peskin kernel}
We can add a more restrictive condition on the integration to unity postulate
\begin{equation}
  \label{eq:evenodd}
  \sum_{j \textrm{ even}} \phi(r-j)  =  \sum_{j \textrm{ odd}} \phi(r-j) = \half
\end{equation}
Which smooths the contributions of the kernel when using a central difference discretization for the gradient operator.
Solving for $\phi$ with this extra condition yields the classic 4-point Peskin kernel

\begin{equation}
  \label{eq:peskin4}
  \phi_{p_4}(|r|) =  \left\{
  \begin{aligned}
    & \frac{1}{8}\left( 3 - 2r + \sqrt{1+4r(1-r)}\right)& r < 1\\
    & \frac{1}{8}\left(5-2r-\sqrt{-7+12r-4r^2}\right)& r < 2\\
    & 0 & r>2
  \end{aligned}\right.
\end{equation}
The main advantage of this kernel is that it interpolates linear functions exactly, and smooth functions are interpolated to second order accuracy.

\subsubsection*{6-point Peskin kernel}
Recently, a new 6-point kernel has been developed that satisfies the moment conditions up to $n=3$ for a special choice of $K_2$~\cite{Bao2016}. Additionally, it also satisfies the even-odd condition in Eq. \eqref{eq:evenodd}, it is three times differentiable and offers a really good translational invariance compared to similarly supported kernels.

This kernel sets $K_1= K_3 = 0$ and
\begin{equation}
K_2 = \frac{59}{60} - \frac{\sqrt{29}}{20}
\end{equation}
Solving for $\phi$ using these conditions, and defining the following
\begin{equation}
  \begin{aligned}
    &\alpha = 28\\
    &\beta(r) = \frac{9}{4} - \frac{3}{2} (K_2 + r^2) + (\frac{22}{3}-7K_2)r - \frac{7}{3})r^3\\
    &\gamma(r) = -\frac{11}{32}r^2 + \frac{3}{32}(2K_2+r^2)r^2 +
    \frac{1}{72}\left((3K_2-1)r+r^3\right)^2 +\\
    &\qquad+\frac{1}{18}\left((4-3K_2)r -r^3\right)^2\\
    &\chi(r) = \frac{1}{2\alpha}\left( -\beta(r) + \textrm{sgn}(\frac{3}{2} - K_2)\sqrt{\beta(r)^2 - 4\alpha\gamma(r)}\right)
\end{aligned}
\end{equation}
We get the expression for the 6-point kernel
\begin{equation}
  \label{eq:peskin6}
  \phi_{p_6}(|r|) =  \left\{
    \begin{aligned}
      & 2\chi(r) + \frac{5}{8} + \frac{1}{4}(K_2 + r^2)& r < 1\\[8pt]
      & -3\chi(r-1) + \frac{1}{4} - \frac{1}{6}\left((4-3K_2) + (r-1)^2\right)(r-1) & r < 2\\[8pt]
      & \chi(r-2) - \frac{1}{16} + \frac{1}{8}\left(K+(r-2)^2\right) - \\
      &\qquad-\frac{1}{12}\left((3K_2-1) - (r-2)^2\right)(r-2)& r<3\\[8pt]
      &0 &r>3
  \end{aligned}\right.
\end{equation}

Given its complexity it is advisable to tabulate $\phi_{p_6}$.
\subsubsection*{Barnett-Magland (BM) kernel}
A new kernel, called ``exponential of the semicircle''(ES) and here referred to as BM, has been recently developed to improve the efficiency of non-uniform \gls{FFT} methods~\cite{Barnett2019}. This kernel has been used for electrostatics~\cite{Shamshirgar2021}, but we will also apply it to the Stokes equation. This kernel has a simple mathematical expression
\begin{equation}
  \label{eq:bmkernel}
  \phi_{BM}(r,\{\beta, w\}) = \left\{
  \begin{aligned}
    &\frac{1}{S}\exp\left[\beta(\sqrt{1-(r/w)^2}-1)\right] & |r|/w\le 1\\
    & 0 & \textrm{otherwise}
  \end{aligned}\right.
\end{equation}
Where $\beta$ and $w$ are parameters related to the shape and support (width) of the kernel. The parameter $S(\beta, w)$ is the necessary normalization to ensure that Eq. \eqref{eq:bmkernel} integrates to unity. Since Eq. \eqref{eq:bmkernel} does not have an analytic integral, this factor must be computed numerically. One advantage of BM kernel is that it decays faster than a Gaussian in Fourier space, which is beneficial in spectral methods~\cite{Barnett2019}.

One disadvantage of the kernels above is that we do not know their analytical Fourier transform (in the case of the BM kernel this stems from it not having an analytical integral), which hinders our ability to elaborate analytical expressions for the mobility in Eq. \eqref{eq:bdhimob}.
For our purposes, this means that we have to numerically estimate the relation between the width of the kernels and the hydrodynamic radius, which will sometimes subject also to the size of the grid.
On the other hand, all of them will present Oseen-like behavior at long distances and will be regularized in a similar way to the \gls{RPY} mobility at short distances.
We will investigate this in more detail shortly.
\subsubsection*{Gaussian kernel}
Finally, we can include here for completeness the Gaussian kernel, which can be defined as
\begin{equation}
  \label{eq:gaussiankernel}
  \phi_G(r,\{\sigma\}) = \frac{1}{(2\pi\sigma)^{3/2}}\exp\left(\frac{-r^2}{2\sigma^2}\right)
\end{equation}
Where $\sigma$ is the width of the Gaussian.
In this case it is possible to compute the double convolution in Eq. \eqref{eq:bdhimob} analytically, allowing to relate the hydrodynamic radius, $a$, with the width of the Gaussian as $\sigma := a/\sqrt{\pi}$.
Other Peskin-like kernels can be found by enforcing other conditions, see for example~\cite{Yang2009}.
Section \ref{sec:kernelcomp} presents a comparison of the different kernels showcased in this section.

\section{Spreading and interpolation algorithms}
In this chapter we will see how to efficiently spread and interpolate in a \gpu.
Let us consider that the communication will take place between a set of $N$ particles (aka markers) with positions $\{\vec{\ppos}_0,\dots,\vec{\ppos}_{N-1}\}$ inside a cubic box of size $L$ and a discrete regular grid with dimensions $n$ in each direction (representing the field). Therefore, each cell in the grid is a cube of size $l := n/L$. The kernel is truncated at some distance $r_c$, which translates to a number of support cells $n_s:=r_c/h$, ensuring that each particle only has to communicate with $n_s^3$ cells in its vicinity. Note that the algorithms described in the following sections can be generalized then to non-cubic boxes, non-regular grids, etc in a straightforward way.

For convenience we will assume that particles will spread forces, represented with $\vec{F}:=\{\vec{F}_0,\dots,\vec{F}_{N-1}\}$, into a force density field, $\vec{f}_j$ (where $j$ represents a given cell), in the grid. Similarly particles will interpolate a velocity field, $\vec{\fvel}_j$, from the grid into a set of per-particle velocities, $\vec{\pvel}:=\{\vec{\pvel}_0,\dots,\vec{\pvel}_{N-1}\}$. Naturally, the algorithms will be generic for any per-particle and per-cell quantity.

We want to devise algorithms for two purposes:
\begin{enumerate}
\item Spreading (as defined in Eq. \eqref{eq:spreadoper}) for a discrete set of points:
  \begin{equation}
    \vec{f}_j = \oper{S}\vec{F} = \sum_i \delta_a(\vec{\fpos}_j-\vec{\ppos}_i) \vec{F}_i.
  \end{equation}
\item Interpolation (as defined in Eq. \eqref{eq:interpdiscrete}) from a discrete set of points:
  \begin{equation}
    \label{eq:interpdiscrete}
    \vec{u}_i = \oper{J}_{\vec{\ppos}_i}\vec{\fvel} = \sum_j{\delta_a(\vec{\ppos}_i - \vec{\fpos}_j)\vec{\fvel}_jdV}
  \end{equation}
  Where, owing to the assumption of a regular grid, the quadrature weights, $dV$, are assumed to be the same for all cells in the grid.
\end{enumerate}


In a regular grid, we can find the cell in which a given particle with position $\ppos^{(\alpha)} \in [-L/2, L/2)$ (being $\alpha$ any direction) lies using $c^{(\alpha)} = \floor((\ppos^{(\alpha)}/L + 0.5)N_\alpha)$, where $N_\alpha$ is the number of cells in the direction $\alpha$. From there, it is straightforward to find the coordinates of the $n_s^3$ cells in the support of the particle, which are given by
$$\vec{c}_s \in [\vec{c}_q - \vec{P}, \vec{c}_q - \vec{P} + n_s].$$
Where $\vec{c}_q$ represents the cell in which a particle with position $\vec{q}$ is located.
The factor $\vec{P}$ is defined, in each direction, as
$$
P^{(\alpha)} := n_s/2 + s^{(\alpha)}
$$
The shift, $s^{(\alpha)}$, has to be defined depending on whether the support, $n_s$, is even or odd (see Fig. \ref{fig:ibm2}). In particular, we have
\begin{equation}
s^{(\alpha)} := \left \{
\begin{aligned}
  &0\quad&\text{if}\quad n_s\text{ is odd}\\
  &\floor(\ppos^{(\alpha)}/h - c_q^{(\alpha)} + 0.5) \quad&\text{if}\quad n_s\text{ is even}
\end{aligned}\right.
\end{equation}
Note that this rule for computing $s^{(\alpha)}$ only works for a regular grid. In general, we will need to find a rule that returns $0$ if a particle is in one side with respect to the cell center and $1$ otherwise.

It is important to note that if the support is even, the value of the factor $\vec{P}$ in each direction will depend on the specific position of the particle inside the cell (see Fig. \ref{fig:ibm2}). On the other hand the definition of $\vec{P}$ works seamlessly even if \gls{PBC} are used, in that case we would simply need to fold the neighbour cells to the $[0, \vec{N})$ range (for instance, the rightmost neighbour cell in the $x$ direction would be $c_j^{(x)} = mod(c_q^{(x)} + P, N_x)$).

\begin{figure}[h]
  \centering
  \includesvg[width=\columnwidth]{gfx/ibm2}
  \caption[ ]{Two particles (red and blue circles, centered at the green circles) in a one dimensional grid ($N_x=5, N_y=N_z=1$). The kernel has a support of $n_s=3$ cells (top) and $4$ cells (bottom). Dashed lines indicate the middle of each cell and crosses represent the cells that need to be visited by the spreading or interpolation operations for each particle. If the support is even, depending on where a particle is inside the cell, some set of neighbours or another will be visited for spreading/interpolation. In this particular example, the shift, $P$, on the bottom (even) support case will be $2$ for the red particle and $1$ for the blue one.}
  \label{fig:ibm2}  
\end{figure}


The interaction between a cell $j$ and a particle $i$ is mediated via the kernel $\delta_a(\vec{\ppos}_i-\vec{\fpos}_j)$, in principle, this would incur $n_s^3$ evaluations of the kernel for spreading or interpolation. However, if the kernels are separable (so that $\delta_a(\vec{r}) = \phi_x(r_x)\phi_y(r_y)\phi_z(r_z)$ by the first postulate in Eq. \eqref{eq:peskinseparable}) we can reduce it to just $3n_s$ by precomputing and storing $\phi_X(\ppos^i_x-\fpos^j_x)$ (and similarly for the other directions) for each direction before spreading/interpolating each particle. We can then just multiply the stored evaluations of $\phi_{x/y/z}$ in each direction to get $\delta_a$. Algorithm \ref{alg:precomputeKernel} shows how to take advantage of this optimization via two functions: The first one takes a position inside the unit box and its corresponding cell shift $P$ and stores (in some arrays with an undetermined location) the $3n_s$ necessary kernel evaluations. The second function takes the 3D index of a neighbour cell ($[0, n_s)$ in each direction) and uses the stored kernel evaluations (which assumes the first function has been called already) to compute and return the weight for that neighbour cell. 
\begin{algorithm}[H]
  \caption[ ]{Precomputing the kernel and accessing it via linear indexes. We used a similar strategy when discussing the cell list, see alg. \ref{alg:visitneigh}.}
  \label{alg:precomputeKernel}
  \begin{algorithmic}[1]
    \Function{precomputeKernel}{$\vec{\ppos} \in [0, L)_{\mathbb R^3}$, $\vec{P}_s$}
    \State $\vec{c} \gets$ cell of $\vec{\ppos}$
    \For{$i=0$ until $n_s$}
    \State $\vec{c}_s \gets \vec{c} - \vec{P}_s + (i,i,i)$
    \State $\vec{r}_s \gets \text{ center of } \vec{c}_s$ \Comment{Center position of cell $\vec{c}_s$ in each direction}
    \State $\vec{r} \gets \text{dist}(\vec{r}_s, \vec{\ppos})$ \Comment{Distance to cell $\vec{c}_s$ in each direction}
    \State Store $\phi_X[i] \gets \phi(\vec{r}_x/h)/h$
    \State Store $\phi_Y[i] \gets \phi(\vec{r}_y/h)/h$
    \State Store $\phi_Z[i] \gets \phi(\vec{r}_z/h)/h$
    \EndFor
    \EndFunction    
    \Function{fetchKernel}{$i_x \in [0, n_s), i_y  \in [0, n_s), i_z  \in [0, n_s)$}
    \State \Return $\phi_X[i_x]\phi_Y[i_y]\phi_Z[i_z]$
    \EndFunction
  \end{algorithmic}
\end{algorithm}


In a serial implementation of spreading, we can simply sum the contribution, $\delta_a(\vec{\ppos}_i - \vec{\fpos}_j)\vec{F}_i$, of each particle, $i$, to each of the cells, $j$, in its support by looping. In the case of interpolation a similar strategy can be employed, where each particle accumulates the quantity $\delta_a(\vec{\ppos}_i - \vec{\fpos}_j)\vec{v}$ from its nearby cells. Algorithm \ref{alg:visitneigh} summarizes the strategy to spread or interpolate a given particle thus far.
\begin{algorithm}[H]
  \caption[ ]{Visiting the neighbour cells of a particle, $i$, located at $\vec{\ppos}_i \in [0, L)_{\mathbb R ^3}$ for spreading or interpolation. We define $\vec{u}:=(1,1,1)$ for convenience.}
  \label{alg:visitneigh}
  \begin{algorithmic}[1]
    \State $\vec{c}_i \gets$ cell of particle $i$ \label{alg:visitneighgetcell}
    \If{$n_s$ is even}
    \LeftComment{Shift for even supports. Can be 0 or 1 in each direction}
    \State $\vec{s} \gets \floor(\vec{\ppos}_i/h - \vec{c}_i + 0.5\vec{u})$ \Comment{Valid for regular grids}
    \Else
    \State $\vec{s} \gets (0,0,0)$ 
    \EndIf
    \State $\vec{P}_s\gets n_s/2\vec{u} -\vec{s}$ \label{alg:visitneighshift}
    \State precomputeKernel($\vec{\ppos}_i, \vec{P}_s$) \Comment{See alg. \ref{alg:precomputeKernel}}
    \For{ $j=0$ until $n_s^3$} \label{alg:visitneighloop}
    \State $i_x \gets \text{mod}(j, n_s)$\label{alg:visit_ix}
    \State $i_y \gets \text{mod}(j/n_s, n_s)$
    \State $i_z \gets j/n_s^2$
    \State $\delta\gets$ fetchKernel($i_x, i_y, i_z$) \Comment{$\delta_a(\vec{q}_i - \vec{r}_j)$, see alg. \ref{alg:precomputeKernel}.}
    \State $\vec{c}_j \gets \text{fold}(\vec{c}_i + (i_x, i_y, i_z) - \vec{P}_s)$ \Comment{Neighbour cell $\in [0, \vec{n})$.}
    \If{Spreading}
    \State cellQuantity[$\vec{c}_j$] += $dV\delta$ particleQuantity[$\vec{\ppos}_i$] \label{alg:visitspread}
    \ElsIf{Interpolating}
    \State particleQuantity[$\vec{p}_i$] += $\delta$ cellQuantity[$\vec{c}_j$]\label{alg:visitinterp}
    \EndIf
    \EndFor
  \end{algorithmic}
\end{algorithm}
In a serial version, we can simply apply algorithm \ref{alg:visitneigh} to each particle to get a fairly efficient algorithm. However, a direct parallelization of this approach by assigning a worker to each particle is not possible for spreading. In this case two particles could write at the same time to the same cell, resulting in a race condition. This race condition can be circumvented by ensuring that line \ref{alg:visitspread} in alg. \ref{alg:visitneigh} is an \emph{atomic} operation\footnote{In a parallel environment, an \emph{atomic} operation enforces that only one thread performs the operation a time. Allowing several workers (threads) to modify (for instance by reading, modifying and then writing) the same memory location at the same time can result in some of the information being lost (for instance when a second thread reads while the first is still modifying). CUDA comes with a great GPU atomic library, which has become more and more efficient with time. CUDA atomics have come to a point in which, in the absence of collisions (two threads trying to access the same memory location), the overhead of using atomics is almost negligible.}.

Another solution is to traverse per cell instead of per particle, usually referred to as mesh-based approaches. Each cell is assigned to a worker, which then traverses all nearby particles. This requires to construct a cell list. While this approach is free of atomic operations (which can hurt performance if lots of collisions happen) it makes the algorithm scale with the number of cells and it will be inefficient in low density systems (or in configurations with large density disparities), where many cells are empty. On the other hand, CUDA atomic operations have come a long way, presenting almost no overhead when there are no collisions. Furthermore, a per-cell approach does not allow to take advantage of Eq. \eqref{eq:peskinseparable} easily, increasing the number of kernel evaluations to $n_s^3$.
We usually have to spread quasi uniform configurations in 3D and we typically deal with a few million number of particles, which makes a per-particle approach the overall best option. Note, however, that there is not a best-for-all algorithm regarding spreading and a mesh based approach will probably have the upper-hand in some situations (probably when highly dense systems are considered).
For our typical use, the few corner cases in which the per-cell approach is more optimal are not worth the effort to consider it as an option. A review of several mesh-based methods is available at~\cite{Guo2015}.

It is worth mentioning the specific geometry of the loop in line \ref{alg:visitneighloop} of algorithm \ref{alg:visitneigh}. When looping through the cells inside the support of a particle, using the strategy in algorithm \ref{alg:visitneigh} instead of the naive three nested loops results in more integer operations, but reduces the operation to a single loop, reducing conditional logic pressure and facilitating loop unrolling, also giving the compiler more chances for optimization. Furthermore, the traversal order can be tweaked easily by modifying the relation between the linear neighbour index and the cell offsets in lines \ref{alg:visit_ix} and below from algorithm \ref{alg:visitneigh}.

\subsubsection*{Spreading algorithm}
Regarding \gpu friendly parallelizations of particle-based spreading algorithms, we will consider three main strategies that we will refer to as Block Per Domain (BPD), Block Per Particle (BPP) and Thread Per Particle (TPP). After describing the different algorithms we will compare their performance.
\subsubsection*{Thread Per Particle (TPP)}
Let us start with the \emph{naive} version of the algorithm in which we assign a thread to each particle and then simply apply algorithm \ref{alg:visitneigh}, summing the particles contributions atomically to the grid data as previously discussed. Each thread must then evaluate the kernel as many times as needed for its assigned particle. We deal with the kernel evaluations in two different ways:
\begin{enumerate}
\item \textbf{TPP (register)}: Store the $3n_s$ kernel evaluations (making use of the optimization in Algorithm \ref{alg:precomputeKernel}) in each thread's \emph{register} memory. Given the scarcity of register memory, storing $3n_s$ values in it could easily result in register spilling\footnote{In a given CUDA kernel, when more register memory than available is requested, the compiler stores the excess in global memory. This is known as \emph{register spilling}. Naturally, register spilling should be avoided at all costs, since accessing global memory is orders of magnitude slower than register memory.}. This algorithm is referred to as \emph{GM} in ~\cite{Shih2021}.
\item \textbf{TPP (recompute)}: We do not take advantage of the kernel separation at all and simply recompute the kernel whenever needed (in particular $n_s^3$ times). This naturally increases arithmetic pressure compared with the register version. However, as a general rule, GPUs are much more forgiving to extra arithmetic operations than they are to extra memory traffic. Thus exchanging memory for arithmetics is usually a good call.
\end{enumerate}
While assigning a thread to a particle sounds like a reasonable separation of tasks, in doing so we are missing out on the intra-thread-block collaborative capabilities of the GPU (such as the shared memory space, see sec. \ref{ch:gpuintroduction}). Furthermore, in TPP threads in the same block are dealing with different particles, so there is no mechanism in place reducing the chance of atomic collisions. If the particles assigned to two adjacent threads happen to be close in space, there will be a high chance that at some point these two threads try to write to the same cells in the grid. Luckily there is a yet more fine-grained parallelization hidden in this algorithm that happens to be perfectly suited for the GPU architecture. This parallelization emerges when assigning an entire thread block to a particle as we will discuss now.

\subsubsection*{Block Per Particle (BPP)}
We suggest a particle-based approach, presenting subtle differences with the existing ones~\cite{Shih2021}~\cite{Guo2015}~\cite{Fiore2017}, specifically tailored for the \gpu. We assign a thread block to each particle and then make each thread in the block spread to a different cell atomically. Since all threads in the block handle the same particle, all the per-particle information can be stored in shared memory. This includes the $3n_s$ evaluations of the kernel, which can be precomputed into shared memory collaboratively by all threads in the same block (see algorithm \ref{alg:precomputeKernelSM}, a slight modification to algorithm \ref{alg:precomputeKernel} that takes this parallelization into account). The available shared memory is much larger than register memory so in this instance storing the $3n_s$ evaluations is not an issue. We refer to this method as block-per-particle (BPP)\footnote{Changing algorithm \ref{alg:visitneigh} into a block-per-particle geometry amounts to assigning different elements of the cell loop to different threads, in the same way as in passing from algorithm \ref{alg:precomputeKernel} to \ref{alg:precomputeKernelSM}.}.
%\todo{improve alg so a different thread is asigned to each dir} 
\begin{algorithm}[h]
  \caption[ ]{Precomputing the kernel in GPU shared memory collaboratively. This algorithm is quite similar to alg. \ref{alg:precomputeKernel}, but each element in the loop is asigned to a different thread in the block. The arrays $\phi_{X/Y/Z}$ are located in shared memory.}
  \label{alg:precomputeKernelSM}
  \begin{algorithmic}[1]
    \Function{precomputeKernelSM}{$\vec{\ppos} \in [0, L)_{\mathbb R^3}$, $\vec{P}_s$}
    \State $\vec{c} \gets$ cell of $\vec{\ppos}$
    \For{$i=$threadIdx.x until $n_s$ \textbf{by} blockIdx.x}    
    \State $\vec{c}_s \gets \vec{c} - \vec{P}_s + (i,i,i)$
    \State $\vec{r}_s \gets \text{center of} \vec{c}_s$ \Comment{Center position of cell $\vec{c}_s$ in each direction}
    \State $\vec{r} \gets \text{dist}(\vec{r}_s, \vec{\ppos})$ \Comment{Distance to cell $\vec{c}_s$ in each direction}
    \State Store $\phi_X[i] \gets \phi(\vec{r}_x/h)/h$
    \State Store $\phi_Y[i] \gets \phi(\vec{r}_y/h)/h$
    \State Store $\phi_Z[i] \gets \phi(\vec{r}_z/h)/h$
    \EndFor
    \State \_\_syncthreads()
    \EndFunction    
  \end{algorithmic}
\end{algorithm}

In BPP, only one of the threads (typically the first one) needs to fetch the particle position and quantity to spread and compute its information (lines \ref{alg:visitneighgetcell} to \ref{alg:visitneighshift} in alg. \ref{alg:visitneigh}).
One benefit of assigning a block per particle is that it is guaranteed that no atomic collisions will occur for threads in the same block. Since the execution order of blocks is not guaranteed the chances of atomic collisions between blocks are reduced on average.
In a thread-per-particle approach, if the particles are sorted randomly in memory chances of atomic collision are reduced at the expense of a worse access pattern (both when reading particles and writing to cells).

We might also consider sorting particles in memory by spatial hash (so that close in space also means close in memory) which results in faster fetching of the grid data, but increased chance of atomic overlap\footnote{In \uammd, particles can be sorted using the Z-Morton hash (see chapter \ref{sec:celllist}) easily.}. In general, there will be a trade-off between these two effects.

However, our block-per-particle algorithm hides these issues (effect of particle sorting and atomic overlap) to an extent, since only one thread in each block will fetch a particle and atomic overlap is reduced by design.

Still, testing suggests that particle sorting via spatial hashing dramatically improves performance (an overall speedup of $\times 7$ for TPP and $\times 2$ for BPP). Suggesting that atomic overlap is far less important than a cache-friendly memory access pattern\footnote{Which goes to show how far have CUDA atomics have come.}.
The storage layout of the grid data can also influence the performance of the algorithm. In particular, using a space-filling curve (as the one in sec. \ref{sec:celllist}) could improve the access pattern in both spreading and interpolation~\cite{Shih2021}. However, this has not been considered in this work, where a linear index is always used (so that the data for cell with indexes $(i,j,k)$ is located at element $i+(j+kn_y)n_x$ in the relevant array)\footnote{Note, however, that the software interface for this module allows for an arbitrary ordering of the grid data, as we will later see.}. This ordering comes imposed by the usual relation between spreading/interpolation and spectral methods in fluctuating hydrodynamics, which requires using \gls{FFT} libraries that benefit from (or are restricted to) it.


\subsubsection*{Block Per Domain (BPD)}
Recently, a new \gpu-friendly algorithm has been developed~\cite{Shih2021} that uses a hybrid strategy, lying in between particle and grid based approachs, by breaking the domain into subdomains (called subproblems) and assigning each one to a thread block. Subproblems with more than a certain number of particles are broken again until each thread block deals with a subproblem with up to a maximum number of particles (see Fig. 1 in ~\cite{Shih2021}) to aide with load-balancing. Subproblems are then spread into shared memory copies of the subdomains, which are then reduced and added atomically into global memory. Since overlap between subproblems is reduced to a shell of ghost cells atomic collisions are minimized. One particular advantage of this method is that its generalization to multi-gpu via domain decomposition comes naturally, as the algorithm already works by subdividing the domain. Testing suggest that, for a single \gpu, the new strategy in ~\cite{Shih2021} is only worth it for large and dense systems (with more than $~2$ million particles) and 2D grids (see Fig. \ref{fig:ibmcomp}). Although it is arguably the best algorithm at the time of writing if one wants to take advantage of several GPUs.
We refer to this algorithm as Block Per Domain (BPD), although the authors of ~\cite{Shih2021} refer to it as \emph{SM} (from shared memory). It is worth noting that the authors of the BPD algorithm do take into account the previously introduced separable-kernel optimization.

\subsection*{Performance comparison of the spreading algorithms}
It is worth mentioning that, although particle sorting usually takes a negligible amount of the runtime, in the \uammd implementation sorting is carried out by \emph{ParticleData} independently of whether spreading/interpolation is present (since sorting is beneficial in several other sections of the codebase, like neighbour traversing). In this sense, the overhead of sorting is hidden. Nonetheless, for the sake of fairness, sorting time is included as part of the runtime in the performance figures when applicable.

Finally, the \uammd implementation presents another benefit when compared to others seen in the wild. In particular when referring to spreading/interpolating several quantities at once (such as forces/velocities). The general strategy when dealing with vectorial quantities (or simply several scalars) is to run the algorithm as many times as there are components. This is related to the AoS (array-of-structures) vs SoA (structure-of-arrays) debate. In general terms, it is best to take an SoA approach in the GPU, which calls for the usual spread-many-times strategy. However, there is one crucial caveat to this rule worth taking into account: In complex fluid simulations we usually deal with vectorial quantities with $3$ or $4$ elements (i.e. positions, forces+energies...), which happen to be special in NVIDIA GPUs\footnote{And probably in the GPU architecture in general.}. In particular, CUDA presents special load/store instructions that fetch/write $128$ bits from memory (the space for $4$ floats). In SoA we would store and treat each component separately, so that reading the (for instance) position of a particle would take $3$ instructions. If we store the positions in an array composed by groups of $4$ floats\footnote{A series of vectorial types names \emph{realX} (with X being 2,3 or 4) are available in \uammd. For instance, the type \emph{real4} containing $4$ scalars.} (an AoS approach) we only need a single instruction to load all three coordinates (and we get a fourth number to leverage for free). Using vector types can potentially make the cost of spreading/interpolation $4$ values at the same time much cheaper than repeating the operation $4$ times. \uammd's spreading/interpolation implementation interface is templated for any random access iterator\footnote{See Appendix \ref{sec:cpp} for more information on C++ iterators.} containing the per-particle and per-cell data as long as the type of the elements follows a certain set of sane rules (in particular, that the values of the iterator can be added together and that the per-particle type can be converted to the per-cell type when required\footnote{\uammd's vector types come with a large set of algebraic overloads. In general, any operation that can be used in a scalar also works for vector types in a per-element manner. For instance, the result of multiplying two \emph{real4} values $a$ and $b$ will be, $c = (a_xb_x, a_yb_y, a_zb_z, a_wb_w)$.}).

We compare the TPP and BPP algorithms with the hybrid BPD (or \emph{SM} in ~\cite{Shih2021}) in figures \ref{fig:ibmcomp} and \ref{fig:ibmcompsorted}, measuring the performance of spreading one scalar per particle for a uniform distribution of positions at different densities (an average of $1$ (left) and $0.1$ (right) particles per cell). The Gaussian kernel is used in Fig. \ref{fig:ibmcomp} for TPP and BPP, while ~\cite{Shih2021} uses the \emph{BM} kernel. However, as the kernel is precomputed with $3n_s$ evaluations which are then used $n_s^3$ times, the cost of evaluating the kernel is negligible in any case. In the case of recomputing the kernel our implementation could have an unfair advantage here, but since it happens to be the slower option in general, we will not consider it.
In any case, if the kernel evaluation becomes a problem due to it being expensive, it is always a possibility to tabulate the kernel in a global memory array\footnote{The \uammd utility \emph{TabulatedFunction} can be employed here. This tool mimics the old CUDA textures, allowing to tabulate a function inside a range and then interpolate it at any point in between.}. By doing so, the cost of computing the kernel becomes constant regardless of its mathematical expression, amounting to fetching some value from memory.

The following remarks about performance trends should be taken with a grain of salt, since with time the \gpu architecture tends to become more and more forgiving about things like the memory access pattern, atomic operations, etc. The compiler also grows smarter with time, being able to better optimize the code. However, it is worth mentioning here at least some of the trends that will probably hold. Performance measurements were carried out using a NVIDIA RTX 2080Ti \gpu, which has the \emph{sm\_75} architecture.

The TPP strategies particularly benefit from the AoS approach, where testing suggest that spreading $4$ numbers at once is overall $5\times$ faster than spreading a single number. Alas, the overall more performant BPP strategy does not seem to benefit from this particular optimization and, in fact, the cost of spreading $4$ numbers in an AoS manner is slightly higher than spreading $4$ times separately.
\begin{figure}
  \centering
  \includegraphics[width=0.75\linewidth]{gfx/ibm_comp_dens1_2080ti}\\
  \includegraphics[width=0.75\linewidth]{gfx/ibm_comp_dens0.1_2080ti}
\caption[ ]{Performance comparison between the different spreading schemes considered for two different densities. Particles are distributed uniformly on a cubic grid and spread with a support $n_s=8$ using a Gaussian kernel. A single scalar is spread per particle. The considered algorithms are: A block-per-particle (BPP), the algorithm used in \uammd. A thread-per-particle (TPP) precomputing the kernel to register memory (register, similar to \emph{GM-sort} in Shih2021~\cite{Shih2021}) and recomputing it (recompute). Finally, timing for the BDP algorithm (\emph{SM} in ~\cite{Shih2021}) is also presented. For these tests, particle sorting was not employed, i.e. the particles are randomly distributed in memory with respect to their physical positions. Performance data was gathered in an RTX2080Ti \gpu using single precision. The crossover between BPP and SM happens at around $2$ million particles in both cases.}
\label{fig:ibmcomp}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.75\linewidth]{gfx/ibm_comp_dens1_sorted_2080ti}\\
  \includegraphics[width=0.75\linewidth]{gfx/ibm_comp_dens0.1_sorted_2080ti}
\caption[ ]{The same situation as in Fig. \ref{fig:ibmcomp}, but in this case particles are presorted to increase spatial locality in memory (particles close in space are placed close in memory). Both the positions and the spread quantity are sorted. Sorting is carried out using the same Morton hash technique laid out in chapter \ref{sec:celllist}. Sorting takes a neglegible amount of time and, moreover, it does not need to be done every time spreading occurs. Typically particles are sorted at the start of a given simulation and then every few diffusive times.}
\label{fig:ibmcompsorted}
\end{figure}

Spatial hashing and sorting dramatically improves performance for all of our algorithms\footnote{We observe a speedup of up to $10\times $ for BPP and as high as $400\times $ for TPP.} for medium to large problem sizes (for BPP, the performance crossover between unsorted and sorted happens at around $n_x=n_y=n_z=100$ regardless of density). For small sizes, we elucidate the increased atomic overlap caused by sorting hurts the internal atomic handling system in the particular architecture at hand. Furthermore, the performance of the sorting algorithm (radix sort) increases with the number of particles. Figure \ref{fig:ibmcompsorted} presents the same situation as figure \ref{fig:ibmcomp} but with particle pre-sorting.

It is worth mentioning that, in the case of BPP, the optimal number of threads per block will in general depend on the number of support cells ($512$ cells in total for the showcased examples), besides other hardware considerations. For the data presented in figures \ref{fig:ibmcomp} and \ref{fig:ibmcompsorted} the optimal block size was $32$, the minimum meaningful block size (since a warp, the minimum hardware computing unit in CUDA GPUs, is composed of $32$ threads).
Finally, if sorting is used, the performance of BPP appears to be robust to changes in density. Whereas the TPP will typically be more performant with a higher density. Without sorting, neither of the algorithms seems to be affected by density.

According to the carried out tests, the overall better choice is therefore the BPP algorithm with particle sorting.
Sizes beyond $512$ cells per direction (in a cubic domain) start to suffer from memory issues ($n=512$ with density $1$ takes up at least $4GB$ of memory) and the sheer number of particles involved, $N\sim 10^8$ for density $1$, render them impractical for most purposes.


\subsubsection*{Interpolation algorithm}
Interpolation is inherently a more efficient operation than spreading, since no atomic operations are required and, moreover, we only need to read the grid data (see alg. \ref{alg:visitneigh}). The worker assigned to a particle can accumulate the result of the interaction with nearby particles into a local variable and then write it to the global result array. Furthermore, interpolation requires only $N$ write operations, while spreading requires to perform $n_s^3N$ atomic writes.
The same TPP vs BPP arguments from spreading can be broadly employed here, prompting a BPP approach. In this case, each thread in the block assigned to a given particle will compute the interaction with one (or several) cells, storing the result in a thread local register.
When all threads in the block have finished, the local results are reduced to a final result, which is then written to global memory by a single thread (usually the first one).
A naive block reduction involves the first thread fetching the local values from the rest of the threads (via shared memory or with CUDA shuffle intrinsics) and then performing a simple addition. However, using \emph{cub}'s~\cite{cub} \emph{BlockReduce} utility proves to be a vastly superior alternative\footnote{With \emph{cub} the reduction algorithm will be different depending on a series of factors (GPU architecture, data type, number of elements...).}.

Figure \ref{fig:ibmcompinterp} shows the performance of BPP interpolation in the same situation as Fig. \ref{fig:ibmcomp}. Timings present a good linearity with the number of particles and are almost independent of the density. 
%\todo{Differences maybe due to block size?, should be faster I would say}
\begin{figure}[h]
  \centering
\includegraphics[width=0.75\linewidth]{gfx/ibm_comp_interp_2080ti}
\caption[ ]{Interpolation performance for different densities. Particles are distributed uniformly in a cubic grid and one value is interpolated with a support $n_s=8$ using a Gaussian kernel. Performance data was gathered in an RTX2080Ti \gpu using single precision. All data lies near the 9ns mark.}
  \label{fig:ibmcompinterp}
\end{figure}


\subsection*{Use in UAMMD}

The \emph{IBM} module in \uammd exposes the spreading and interpolation algorithms described in the previous sections.

This interface is quite generic, allowing to spread or gather in a wide variety of situations. The geometry of the grid is abstracted via the \emph{Grid} object (see Appendix \ref{ch:online}) so non-regular grids can be used. By default, the module will assume that data for cell $(i,j,k)$ is located at index $i+(j+kn_y)n_x$, but any indexing can be used by providing the module with a functor whose parenthesis operator takes $(i,j,k)$ and returns the index in the grid data array.

The types of the grid and particle data arrays are templated. In fact any random access iterator can be provided as long as the value type of the grid data is convertible to the corresponding variable in the particle data (and vice-versa) and any two given elements can be summed. So one can, for instance, using a \emph{transform iterator},  compute the quantity to spread on the fly without storing or reading anything. For additional information on C++ templates and iterators, see Appendix \ref{sec:cpp}.

The gather operation can take an additional functor whose parenthesis operator takes the coordinates of a given cell and a grid object and returns the quadrature weights for that cell.

The default for each cell is taken from the cell volume via the \emph{Grid} object (\mintinline{\ucpp}{dV=grid.getCellVolume(cell);}).

The kernel can also be specified as a template argument. The kernel interface allows to set $\phi$ in each direction separately and can also be different depending on the particle. The support can also be set per particle.

An implementation of all the kernels in sec. \ref{sec:ibm} can be found at file \emph{IBM\_kernels.cuh}.

A lot of extra functionality (omitted here for simplicity) is laid out in \uammd's online documentation, here we provide an example function that spreads and/or gathers an isotropic Gaussian kernel into a regular Cartesian grid. Representing the simplest, yet most common, use case.

%\todo{This interface is super big and complex, what do?}
\begin{code2}[Usage example of the spreading and interpolation module. Some data (stored in \emph{dataAtParticlePositions}) located at some positions (stored in the \emph{positions} array) can be spread to a grid (for which the values of each cell will be stored in \emph{dataAtCellPositions}) using the function \emph{spreadWithIBM}. Similarly, the function \emph{interpolateWithIBM} can be used for the interpolation operation. As an example, the \gls{FCM} module (chapter \ref{sec:fcm}) uses code similar to this one in order to spread particle forces into a grid and, after solving the Stokes equation, interpolate the velocities on a grid back to the particle positions.]  {label=code:ibm}
#include<uammd.cuh>
#include<misc/IBM.cuh>
using namespace uammd;

//A simple Gaussian kernel compatible with the IBM module.
class Gaussian{
  const real prefactor;
  const real tau;
  const int support;
public:
  Gaussian(real width, int support):
    prefactor(pow(2.0*M_PI*width*width, -0.5)),
    tau(-0.5/(width*width)),
    support(support){}

  __device__ int3 getSupport(real3 pos, int3 cell){
    return {support, support, support}
  }

  __device__ real phi(real r, real3 pos) const{
    return prefactor*exp(tau*r*r);
  }
};

template<class Iter1, class Iter2>
void spreadWithIBM(Grid grid, real4* positions,
                   Iter dataAtCellPositions,
                   Iter2 dataAtParticlePositions,
                   int numberParticles){
  const real width = 1; //An arbitrary width
  const int support = 8;//An arbitrary support
  auto kernel = std::make_shared<Gaussian>(width, support);
  IBM<Gaussian> ibm(kernel, grid);
  //Spreads dataAtParticlePositions into dataAtCellPositions
  ibm.spread(positions, dataAtParticlePositions, dataAtCellPositions, numberParticles);
}

template<class Iter1, class Iter2>
void interpolateWithIBM(Grid grid, real4* positions,
                        Iter dataAtCellPositions,
                        Iter2 dataAtParticlePositions,
                        int numberParticles){
  const real width = 1; //An arbitrary width
  const int support = 8;//An arbitrary support
  auto kernel = std::make_shared<Gaussian>(width, support);
  IBM<Gaussian> ibm(kernel, grid);
  //Interpolates dataAtCellPositions into dataAtParticlePositions
  ibm.gather(positions, dataAtParticlePositions, dataAtCellPositions, numberParticles);
}

\end{code2}




\section{Comparing the different spreading kernels for BDHI}\label{sec:kernelcomp}
As explained in previous sections using different kernels will yield slightly different regularizations of the near field hydrodynamics. In particular, the specific shape of the kernel will have an effect on the hydrodynamic radius when performing the convolution in Eq. \eqref{eq:bdhimob}. In this section we will explore the relation between the different kernels showcased in section \ref{sec:ibm} and the resulting hydrodynamic radius. %\todo{dont really like this intro}

The spatial discretization (collocated vs. staggered grid) will also influence the hydrodynamic radius, so we will make a separation between them.
Finally, some kernels must be truncated at a certain distance damaging accuracy.
Since the domain is periodic in the three directions it is straightforward to see that any possible effect of the kernels on the grid will be the same for any cell. Thus, we just need to measure in one of the cells, or more appropriately, fold our results to a unit cell and average.
In particular, we will study the variance of the hydrodynamic radius inside a cell to determine both the accuracy (translational invariance) and the relation between the grid size, $h$, and the hydrodynamic radius, $a$\footnote{We are assuming a cubic domain in which cells in all directions have the same length.}. It is possible that the following relations are slightly renormalized in the case of non-cubic cells. However, the self mobility is not well-defined anyway in non-cubic periodic domains. As a matter of fact, if a particle is being pulled on the plane in a box with increasingly large size in the perpendicular direction its in-plane self mobility monotonically increases towards infinity~\cite{Vogele2016}\footnote{An effect that is well captured by all the periodic methods we have seen thus far.}.

The self mobility of a single particle being pulled inside a periodic cubic domain of size $L$ is computed using the well known periodic correction for the Stokes drag of a sphere~\cite{Hasimoto1959},
\begin{equation}
  \label{eq:selfmobpbc}
  M_{0}(a,L) = \frac{1}{6\pi\eta a}\left[1-b\frac{a}{L} + \frac{4\pi}{3}\left(\frac{a}{L}\right)^3 - c \left(\frac{a}{L}\right)^6\right].
\end{equation}
Where the factors $b$ and $c$ are
\begin{equation}
  \begin{aligned}
  b &:= 2.83729748,\\
  c &:= \frac{16\pi^2}{45} + 23.85.
  \end{aligned}
\end{equation}
We test accuracy by measuring the translational invariance through the self mobility of a particle being pulled in an arbitrary direction. In particular, we measure the variance of the hydrodynamic radius (computing $a = 6\pi\eta M_{0}$) inside a grid cell.
We define the error as
\begin{equation}
  \label{hydroerr}
  E_a(\vec{r}) = \left|1 - \frac{a(\vec{r})M_{0}}{6\pi\eta}\right|.
\end{equation}


\subsection*{Regular grids}
We will test using the \gls{FCM} with the different kernels since the \gls{PSE} is tied to the Gaussian kernel.

Being defined for specific support cells, the Peskin kernels are tied to the grid size, $h$. When performing the double convolution with the hydrodynamic Green's function (see Eq. \eqref{eq:bdhimob}) the resulting self mobility will be regularized differently for each one and will be dependent on $h$.
Since we cannot perform the double convolution in Eq. \eqref{eq:bdhimob} analytically in the case of Peskin kernels we measure the self mobility (either dynamically as explained in the previous section or by looking at fluctuations) and then renormalize the hydrodynamic radius so that $M_{0}(L\rightarrow\infty) = \frac{1}{6\pi\eta a}$.
In particular, for the Peskin kernels introduced in section \ref{sec:ibm}, the resulting hydrodynamic radii are:
\begin{itemize}
\item Peskin 3pt gives $a=(1\pm 0.01) h$
\item Peskin 4pt gives $a=(1.31\pm 0.001) h$
\item Peskin 6pt gives $a=(1.5195 \pm 0.0002) h$
\end{itemize}
It is important to note that the Peskin kernels prevent us from tweaking the translational invariance of the hydrodynamic radius (as evidenced by the error ranges in the above list).

In contrast to the Peskin kernels for the Gaussian we are free to choose how fine the grid is (which we refer to as upsampling) and a truncation distance. Both parameters will affect the overall accuracy of the spreading and interpolation. The Gaussian gives $a=h \sqrt{\pi} g_u$, where we set the upsampling, $g_u$, large enough to satisfy a given tolerance. In particular we set
\begin{equation}
  \label{eq:gaussianupsampling}
  g_u(\epsilon) := \text{min}\left(0.55 -0.11 \log_{10}(3\epsilon), 1.65\right)
\end{equation}
Which is just an empirical fit.
We truncate the Gaussian kernel at a distance $r_c$ so that $\phi_G(r_c) < \epsilon$. The author of ~\cite{Keaveny2014} provides an in-depth analysis of the discretization errors of the Gaussian kernel.
Figure \ref{fig:ibm_hydrovar} shows the average error inside a fluid cell for the Peskin kernels, compared to similarly supported Gaussian kernels. At first sight, it might seem like the error is not much better for the Peskin kernels as compared with the Gaussian ones. However, the Gaussian kernels require a greater upsampling (i.e more cells, see Eq. \eqref{eq:gaussianupsampling}) than the Peskin ones to achieve the same tolerance and thus the latter are in general a better choice. Figure \ref{fig:ibmvar} shows the hydrodynamic radius variance inside a cell for some kernels, but in a slice of the cell (at the center height).
\begin{figure}[H]
\includegraphics[width=0.8\textwidth]{gfx/ibm_hydrodeviation}
\caption[ ]{Deviation from the average hydrodynamic radius inside a cell in the $x$ direction. Shown here is the error computed by evaluating Eq. \eqref{hydroerr} inside a range $x=[0, 1]h$ with the mean subtracted. Since the signal is symmetrical only the range $[0.5, 1]h$ is presented.}
\label{fig:ibm_hydrovar}
\end{figure}
Finally, in the case of BM the quadrature error and hydrodynamic radius are different for each $n_s$, $\beta$ and $h$. In a way, a new kernel is obtained for each support (which relates to the tolerance) and $\beta$ must be tweaked to achieve a particular hydrodynamic radius. The actual relations are still being tested and thus we refrain from laying them out here for the time being. It is worth mentioning that, provided the same support, the BM kernel tends to greatly outperform the Gaussian (by yielding a lower quadrature error). Still, there are some considerations that must be taking into account when using the BM kernel for immersed boundary, in particular coming from the fact that it is not separable. A more in depth discussion about the BM kernel can be found in ~\cite{Barnett2019}.
%\todo{Put here the actual relation from the work in DPStokes}
\subsection*{Staggered grids}
We perform the same tests using the \gls{FIB}, which uses a staggered grid.
\begin{itemize}
\item Peskin 3pt gives $a=(0.910\pm 0.005)h$
\item Peskin 4pt gives $a=(1.265\pm 0.002)h$
\item Peskin 6pt gives $a=(1.4830 \pm 0.0001 )h$
\end{itemize}
\begin{figure}[H]
  \centering
  \subcaptionbox{Gaussian 3pt}{\includegraphics[width=0.49\textwidth]{gfx/fcm_gauss_3pt}}\label{fig:ibm_gauss3pt}
  \subcaptionbox{Peskin 3pt}{\includegraphics[width=0.49\textwidth]{gfx/fcm_peskin_3pt}}\label{fig:ibm_peskin3pt}
  \subcaptionbox{BM 3pt}{\includegraphics[width=0.49\textwidth]{gfx/fcm_bm_3pt}}\label{fig:ibm_bm3pt}
  \subcaptionbox{Gaussian 10pt}{\includegraphics[width=0.49\textwidth]{gfx/fcm_gauss_10pt}}\label{fig:ibm_gauss10pt}
  \caption[ ]{Variance of the hydrodynamic radius inside a cell (2D slice at $z=L/2+h/2$ of a 3D system) for different kernels in \gls{FCM}. Tests were carried out on a cubic box of size $L=32a$.} \label{fig:ibmvar}
\end{figure}

In general using a staggered grid requires a smaller grid to get the same hydrodynamic radius (with a similar translational invariance) than a collocated one. On the other hand, the access patterns in a staggered grid are usually more complex and can result in less performant code, even when the cost of the FFT will be slightly inferior. In the case of a triply periodic system, using a staggered grid via the \gls{FIB} is not worth it in general as compared to the \gls{FCM}.
All tests were carried out with single precision (to measure the error in a ``real-life'' setting) using an RTX3080 \gpu.
%\gls{PSE} always uses a Gaussian, and the tolerance is chosen along with the splitting parameter so it is meaningless to talk about accuracy vs. kernel support in that case.


%\begin{itemize}
%\item Finishing words about methodology
%%\item A word about all things implemented in UAMMD thus far
%%\item Putting it all together
%%\item An outline of all the phisics that can be done with the algorithms and implementations thus far
%%\item Future direction and additions to the algorithms in this part
%%\item Introduce next part, novel algorithms and physics.
%\end{itemize}
%

\newpage
\cleardoublepage

\ctparttext{}
\part{Novel algorithms and physics for complex fluids}\label{pt:algo}

In this part of the manuscript we will go through several novel, \gpu-focused, algorithms that have been developed during this thesis.
In particular, we will discuss four new algorithms; two of them solving the Stokes equation (see Eq. \eqref{eq:stokes}) for hydrodynamics and the other two solving the, not yet introduced, Poisson equation (see Eq. \eqref{eq:ttpoisson}) for electrostatics.
In chapter \ref{sec:q2D} we introduce a novel algorithm for solving the Stokes equation in quasi 2D geometries, in which particles are restricted to move in a periodic plane while embedded in a fluid that is either open (quasi 2D) or non existent (true 2D) in the third direction.
Later, in chapter \ref{sec:dpstokes} we solve the Stokes equation again but this time letting the particles move freely inside a doubly periodic domain (periodic in the plane and open in the third direction), introducing the possibility of placing walls at the domain limits, creating slit or walled geometries.
The doubly periodic Stokes algorithm will be the last methodology for hydrodynamics in this manuscript. We will use the two subsequent chapters in this part to introduce two algorithms for electrostatics (by solving the Poisson equation)\footnote{The algorithms for electrostatics make use of the toolset developed for \gls{FCM} and \gls{PSE} in previous chapters, which is the reason why its introduction was delayed up to this point.}. We will start by describing a method for computing the electrostatic interactions of a group of charged particles in a triply  periodic environment in chapter \ref{ch:tppoisson}. Finally, in a similar way as done with Stokes, we will compute electrostatic interactions in a doubly periodic environment (with the possibility of placing surface charged walls at the domain limits).
% Hydrodynamics in slit channels are key in several contexts of technological and biological significance.

\chapter{Hydrodynamics under quasi two-dimensional confinement}\label{sec:q2D}
\begin{figure}[h]
  \centering
  \includesvg[width=\columnwidth]{gfx/q2d}
  \caption[ ]{Representation of a colloidal system under soft confinement in the perpendicular ($z$) direction. Particles are confined via some external potential that distributes them near $z=0$ with a typical width $\delta$. The confining forces acting on the particles are propagated to the plane via the Oseen tensor (orange lines).}
  \label{fig:q2dsch}
\end{figure}

In this section we will develop algorithms to study the dynamics of a group of colloidal particles whose movement is constrained in the $z$ direction by some external force (see Fig. \ref{fig:q2dsch}) while submerged in an otherwise unbounded three-dimensional fluid. We refer to this geometry as quasi 2D (q2D for short), as opposed to a situation in which both the particles \emph{and} the fluid are restricted to the plane (true 2D or t2D) or when both are unbounded in the three directions (true 3D or t3D). In the limit when the confining force is infinitely stiff, particles move in a strictly two-dimensional plane. This can be used to model, for instance, the transport the properties of systems at fluid-fluid or air-fluid interfaces. 

Besides being common in subcellular biology, there are several examples of these constrained dynamics systems in a wide variety of industrial applications, like food, creams, or crude oil (e.g., asphaltenes near water interfaces).
On the other hand, the transverse diffusion of proteins embedded in lipid bilayers controls their biological function~\cite{Brown20112}.
Note that although confined to the membrane plane, these proteins will be subject to relatively large spatial fluctuations.

In general, diffusion of colloidal particles confined to two-dimensional surfaces is a key transport mechanism in several contexts of technological and biological significance.

As to the physical phenomenon that actually constrains the particles to a plane we have, for instance, the so-called Pickering emulsions, which are stabilized by the spontaneous absorption of colloidal particles to the fluid-fluid interface.

Colloids might also be trapped in fluid-fluid interfaces and interact via capillary~\cite{Dominguez2010} or electrostatic~\cite{Gao2016} forces. By placing walls, particles can be forced to diffuse in a plane~\cite{Lin2014}. 

A standing pressure wave with hundreds of megahertz or more creates an ultrasound potential which moves heavy colloidal particles towards two-dimensional traps formed at the nodes of the pressure wave (or the valleys if particles are lighter than the fluid)~\cite{Settnes2012,Balboa2013}. The resulting ultrasound potential is harmonic, leading to a Gaussian colloidal dispersion around the pressure node~\cite{Balboa2013}.

Similar harmonic traps can be obtained by laser tweezers~\cite{Ding2012}.
% \todo{Nature Communications 2015}
Other forms of (non-Gaussian) traps can be prepared using electric fields (maybe leading to barometric-like density profiles).

In future sections, we will study diffusive phenomena arising in this special geometry. For now, let's lay out the algorithmic machinery necessary to simulate it.

Let's start by considering a suspension of colloidal particles confined near $z=0$ by a strong confining potential submerged in an otherwise unbounded three-dimensional incompressible fluid. As we saw in chapter \ref{sec:bdhi}, forces acting on the particles are propagated to the whole fluid, due to its incompressibility, via the Oseen tensor. This results in part of the momemtum introduced in the fluid by the confining force acting on one particle (which acts in the normal direction) being propagated over the plane. As we will soon see, the resulting flow source tends to expel other particles around it (see Fig.~\ref{fig:q2dsch}). This can be interpreted as the flow in the plane having a positive divergence, as seen from within the plane. Indeed, we will see that this effect naturally arises in a particle-based description (\gls{BDHI}) through an effective plane mobility with non-zero divergence~\cite{Pelaez2017}.

There are several ways to gradually transform a perfect 2D confinement into an isotropic 3D distribution and, as illustrated in Fig. \ref{fig:q2dsch}, we model the case where the 2D interface of colloids becomes Gaussianly blurred. In particular, we consider a suspension of colloids confined in the $z$ direction around some height, $z=0$, by a harmonic potential of the form
\begin{equation}
  \label{eq:q2Dconf}
U_{\text{conf}}(z)=\half K_sz^2
\end{equation}
applied to each individual particle.
The spring constant $K_s$ controls the width, $\delta$, of the Gaussian distribution of particles in the $z$ direction.
\begin{equation}
  \delta = \left(\frac{\kT}{K_s}\right)^{1/2}
\end{equation}
We can use this as a slider to go from t3D, in the limit $K_s\rightarrow 0$, to q2D when $K_s\rightarrow\infty$.
The dynamics of the system are then governed by Eq. \eqref{eq:bdhibdrelation} with an Oseen-like mobility (like the \gls{RPY} one). Note that solvent inertial effects can also be incorporated by using the \gls{ICM} in sec. \ref{ch:icm}.

Thus far, all the algorithms we have developed for solving \eqref{eq:bdhibdrelation} (or more generally Eq. \eqref{eq:navierstokes}) deal with either fully open or triply periodic boundary conditions. However, now the perpendicular direction should be open while leaving the plane periodic.

To be fair, our mathematical infrastructure allows to discretize the Stokes operator in any geometry to accommodate any boundary conditions. But since our implementations rely on the \gls{FFT} to be efficient and \gpu-friendly these modifications are not straightforward. 

One approach to deal with this is to simply use the triply periodic algorithms with a box with width, $L_z\gg \delta$, large enough to neglect finite-size effects. However, since hydrodynamics are long ranged in nature we will be forced to perform a full finite-size analysis to ensure the convergence of the measured properties.

In future sections, we will describe a family of pseudo-spectral algorithms capable of dealing with non periodic boundary conditions. For the moment, let's adapt the already developed \gls{FCM} to the limit of strict confinement (q2D). Let's see how we can compute the Fourier expression of a Green's function specific for q2D.

\section{The Force Coupling Method in quasi 2D}

Suppose that all particles are subject to a force coming from the potential in Eq. \eqref{eq:q2Dconf}. We want to take the mathematical limit of Eq. \eqref{eq:bdhibdrelation} (using $\vec{F} = -\vec{\partial}_{\vec{\ppos}} U_{\text{conf}}$) when $K_s\rightarrow\infty$ (hence particles move strictly in the plane $z=0$). Although it is possible to take this limit formally the general theory for it is quite complex. Luckily we can elaborate some simple arguments and assumptions to make this in a trivial way (we provide a more in depth analysis in \cite{Pelaez2018}).

%We start with the realization that, given that the particles are restricted to a plane, the mobility tensor can be reordered in a diagonal blocked arregement 
%
%\begin{equation}
%\tens M=\left[\begin{array}{cc}
%\tens M^{\parallel} & \tens M^{\parallel,\perp}\\
%\left(\tens M^{\parallel,\perp}\right)^{T} & \tens M^{\perp}
%\end{array}\right]=\left[\begin{array}{cc}
%\tens M^{\parallel}\\
% & \tens M^{\perp}
%\end{array}\right]
%\end{equation}
%Where the parallel mobility (in the $x,y$ plane) is decoupled from the perpendicular one (in $z$). We can see this by understanding that a perpendicular force cannot induce motion on the plane, since that would break symmetry. Equivalently, a parallel force cannot take a particle out of plane. On the other hand, the confinement does not cause a free energy gradient in the plane (since it is uniform in it).

To understand the enhancement of collective diffusion in confined geometries, we first consider the Smoluchousky equation for the particle concentration field $\rho(\vec{\fpos}, t) = \sum_i\delta(\vec{\fpos} - \vec{\ppos}_i)$ of ideal (i.e, non-interacting via potential forces) colloids,

\begin{equation}
  \label{eq:smoludensq2D}
  \partial_t \rho = -\vec{\partial}_{\vec{\fpos}}\cdot\left[-D_0\vec{\partial}\rho + \rho\vec{\fvel}_d \right].
\end{equation}
Here $D_0$ is the bare diffusion coefficient and $\vec{\fvel}_d$ is the drift velocity, given by
\begin{equation}
  \vec{\fvel}_d(\vec{\fpos}, t) = \int\tens{M}(\vec{\fpos}-\vec{\fpos}') f(\vec{\fpos}') d\vec{\fpos}'.
\end{equation}
Where $f(\vec{\fpos})$ is the confining force density, which can be expressed as the gradient of an osmotic pressure. For an ideal gas of colloids, the pressure is $\pi := \kT \rho(\vec{\fpos})$, then
\begin{equation}
  \vec{f}(\vec{\fpos}') = -\vec{\partial}_{\vec{\fpos}}\pi = -\kT \vec{\partial}_{\vec{\fpos}}\rho = -\kT \sum_i \partial_z\delta(\vec{\fpos}-\vec{\ppos}_i) \hat{\vec{e}}_z,
\end{equation}
where the last equality particularizes the equation for the in-plane confinement. The drift velocity is then,
\begin{equation}
  \label{eq:driftq2d}
  \begin{aligned}
    \vec{\fvel}_d(\vec{\fpos}) &= -\int\tens{M}(\vec{\fpos}-\vec{\fpos}') \vec{\partial}_{\vec{\fpos}}\pi d\vec{\fpos}' \\
    &=-\kT\int\tens{M}(\vec{\fpos}-\vec{\fpos}') \sum_i \partial_z\delta(\vec{\fpos}-\vec{\ppos}_i) d\vec{\fpos}' \overset{\text{(By parts)}}{=}\\
    &=\kT\int \left[\partial_{z'}\tens{M}(\vec{\fpos}-\vec{\fpos}')\right] \sum_i \delta(\vec{\fpos}-\vec{\ppos}_i) d\vec{\fpos}'\\
    &= -\kT\sum_i\int \left[\partial_z \tens{M}(\vec{\fpos}-\vec{s}')\right]|_{z=0} \delta(\vec{s}' - \vec{\ppos}_i)d\vec{s}'.
  \end{aligned}
\end{equation}
Where we introduced the plane coordinates, $\vec{s}$, such that $\vec{\fpos} = \vec{s} + z\hat{\vec{e}}_z$. Here $\delta$ represents the Dirac delta.
Now, the flow is incompressible in 3D, which implies
\begin{equation}
  \label{eq:q2dtotalflow}
  (\vec{\partial}_{\vec{s}} + \partial_z)\tens{M} = 0 \rightarrow \partial_z\tens{M} = -\vec{\partial}_{\vec{s}}\tens{M},  
\end{equation}
thus, substituting $\partial_z\tens{M}$ in Eq. \eqref{eq:driftq2d} with Eq. \eqref{eq:q2dtotalflow},
\begin{equation}
  \label{eq:flowq2Ddivergence}
  \vec{\fvel}_d(\vec{s}) = \kT\sum_i \int \left( \vec{\partial}_{\vec{s}}\tens{M} \right)_{z=0} \delta(\vec{s}' - \vec{\ppos}_i)d\vec{s}' = \kT\left(\vec{\partial}_{\vec{\ppos}}\cdot\tens{M}\right)_{z=0},
\end{equation}
which indicates that $\left(\vec{\partial}_{\vec{\ppos}}\cdot\tens{M}\right)_{z=0}$ is acting as a flow source term in the plane\footnote{Note that we have restored back to supervector notation, i.e, $\vec{\partial}_{\vec{\ppos}}\cdot\tens{M} = \sum_i\vec{\partial}_{\vec{\ppos}_i}\cdot\tens{M}(\vec{s}-\vec{q}_i)$.}. Notably, the resulting hydrodynamic interaction between two particles $1$ and $2$ is similar to an electrostatic repulsion,
\begin{equation}
  \label{eq:q2Deffrepulsion}
  \vec{\fvel}_d(\vec{q}_1) = \kT\left(\vec{\partial}_{\vec{q}_2}\right)_{z_1=z_2=0}\tens{M} \approx \frac{1}{8\pi\eta q_{12}^3}\vec{q}_{12},
\end{equation}
where we have used the Oseen mobility for $\tens{M}$\footnote{This interaction could be equivalent to an effective electrostatic potential $U_{\text{eff}} \approx \frac{3a}{4r}\kT$. However, unlike a potential interaction, the thermal drift $\vec{\partial}_{\vec{s}}\cdot\tens{M}$ does not induce any distortion in the equilibrium distribution. On the contrary, the addition of the drift term $\kT\vec{\partial}_{\vec{s}}\cdot\tens{M}$ preciesly ensures that the equilibrium distribution is consistent with the Boltzmann presumption $P(\vec{\ppos}) \propto \exp({-\beta}H(\vec{\ppos}))$; given by the effective Hamiltonian of the system (see chapter \ref{sec:fpe}).}.

The collective effect on the colloidal density $\rho(\vec{\fpos},t)$ can be analyzed from eq. \eqref{eq:smoludensq2D}.
\begin{equation}
  \partial_t \rho = \vec{\partial}_{\vec{\fpos}}\cdot\left[D_0\vec{\partial}_{\vec{\fpos}}\rho + \kT \bar{\rho}\int\tens{M}(\vec{\fpos}-\vec{\fpos}')\vec{\partial}_{\vec{\fpos}}\rho d\vec{\fpos}'\right]
\end{equation}
Where we have approximated $\rho$ by its average, $\bar{\rho}$, in the second term (linear perturbation approximation).
Taking the Fourier transform in space,
\begin{equation}
  \partial_t \hat{\rho}(\vec{k},t) = \left[D_0k^2 + \kT \bar{\rho}\vec{k}\cdot\tens{M}\cdot{\vec{k}}\right]\hat{\rho},
\end{equation}
which solution is given by
\begin{equation}
  \hat{\rho}(k,t) = \hat{\rho}(k,0)\exp\left(-k^2D_c(\vec{k}) t\right), 
\end{equation}
where $D_c$ is the short time collective diffusion coefficient\footnote{The collective diffusion coefficient determines the decorrelation time $\left(D_c(k)k^2\right)^{-1}$ of a density fluctuation of typical wavenumber $k$.},
\begin{equation}
  D_c(\vec{k}) = D_0 + \bar{\rho}\kT \frac{\vec{k}\cdot\tens{M}\cdot{\vec{k}}}{k^2}.
\end{equation}

In 3D, $D_c = D_0$ (since the divergence of the mobility is null), as expected for ideal particles. However, for q2D it is not difficult to show \cite{Bleibel2017, Pelaez2018} that,
\begin{equation}
  D_c = D_0 + (L_nk)^{-1} + O(k),
\end{equation}
with
\begin{equation}
  L_n := \frac{2}{3\pi a\bar{\rho}_{\text{2D}}},
\end{equation}
where $\bar{\rho}_{\text{2D}}$ is the surface density. Thus collective diffusion increases as the inverse of the wave number.
This analysis indicates that  to impose the strict\footnote{If $U=\half K_sz^2$ is used, there is no need of $\nabla_{\vec{s}}\cdot\tens{M}$, because we resolve the whole 2D flow.} confinement condition in q2D of Brownian particles one needs to include the thermal or spurious drift related to the divergence of the (effective) q2D mobility, $(\vec{\partial}_{\vec{s}}\cdot\tens{M})_{z=0}$ (according to eq. \eqref{eq:flowq2Ddivergence}). The resulting particle dynamics \textbf{at the confining plane} are described by eq. \eqref{eq:bdfull}.
This means that we can simply remove the $z$ component in our description and write
\begin{equation}
  \label{eq:bdq2d}
  d\vec{\ppos} = \tens{M}\vec{F}dt + \sqrt{2\kT \tens{M}}d\vec{\widetilde{W}} + \kT\left(\vec{\partial}_{\vec{\ppos}}\cdot\tens{M}\right)dt,
\end{equation}
where the particle positions and forces, the mobility and the noise are now defined only in the plane.
In order to implement eq. \eqref{eq:bdq2d} within the \gls{FCM} procedure (introduced in section \ref{sec:fcm}) we use the second equality of eq. \eqref{eq:driftq2d}, including also putative forces $\vec{F}$ (per particle) arising from colloidal interactions and/or external fields. In such a case the force density is,
\begin{equation}
  f(\vec{\fpos}) = \oper{S}\vec{F} + \vec{\partial}_{\vec{\ppos}}\oper{S}\left[\kT\right].
\end{equation}
We have generalized the Dirac delta by using the spreading operator, $\oper{S}$ (as we did when introducing spreading and interpolation in chapter \ref{sec:bdhi}). The second term corresponds to the osmotic force, $-\partial_{\vec{\fpos}}\pi = -\kT\vec{\partial}_{\vec{\fpos}}\oper{S} = \kT\vec{\partial}_{\vec{\ppos}}\oper{S}$. The kernel $\oper{S}$ spreads forces while its derivative\footnote{In fact, in the case of a Gaussian, we can spread a single quantity by using $\partial\phi_G(r) = 2\sigma_ar\phi_G(r)$ and spreading $\vec{F} + 2\kT\sigma_a\vec{r}$.} spreads internal energies\footnote{This latter idea was generalized in ~\cite{Balboa2013} to model ultrasound forces on compressible particles, with energy $\psi(\rho)\kT$.}.

Following the \gls{FCM}, we will set $\oper{S}$ as a Gaussian, and in order to implement a purely 2D computational set up, we need to integrate out the effect of the third direction, $z$, from the equations of motion. To that end, we recall that $\tens{M} = \oper{J}\tens{G}\oper{S}$, where $\tens{G}$ is the Green function for the fluid velocity field. The particle kernels are separable ($\oper{S} = \phi(x)\phi(y)\phi(z)$) which permits including them into the 2D Fourier transform of the Green function, to yield,
\begin{equation}
  \hat{\tens{G}}_{\qtd}(\vec{k} = (k_x, k_y)) = \frac{1}{2\pi}\int_{k_z=-\infty}^\infty\fou{\phi}(k_z)^2\fou{\tens{G}}_{\text{3D}}(\vec{k};k_z)dk_z.
\end{equation}
Using a Gaussian kernel for $\phi$ and the Oseen tensor for $\tens{G}_{\text{3D}}$,

\begin{equation}
  \label{eq:q2dGreenintegral}
  \begin{aligned}
  \fou{\tens{G}}_{\qtd}(\vec{k}) &= \frac{1}{2\pi\eta}\int_{k_z} \frac{dk_z}{(k')^2}\exp\left(-\frac{a^2k_z^2}{\pi}\right)\left( \mathbb{I} - \frac{\vec{k}'\otimes\vec{k}'}{(k')^2}\right) \\
  &= \eta^{-1}\left(g_k(ka)\vec{k}_{\perp}\otimes\vec{k}_{\perp} + f_k(ka)\vec{k}\otimes\vec{k}\right).
\end{aligned}
\end{equation}
Where $\vec{k}' = (k_x, k_y, k_z)$ is the three dimensional wavenumber and we recall that unless stated otherwise, in quasi 2D vectors are defined in $\mathbb{R}^2$. Additionally $\vec{k}_\perp := \vec{k}\times\hat{z} = (k_y, -k_x)$ is a vector perpendicular to $\vec{k}$.
The convolution with the $(x,y)$ kernel is performed explicitly via spreading in \gls{FCM}.
Note that this has restricted our algorithm to a Gaussian kernel, with its advantages and disadvantages. However, a different kernel can be used if it has an analytical Fourier form that allows the integral in Eq. \eqref{eq:q2dGreenintegral} to be solved analytically\footnote{The anaylical form of Eq. \eqref{eq:q2dGreenintegral} is not actually needed if somehow $f_k$ and $g_k$ are tabulated.}.
Solving Eq. \eqref{eq:q2dGreenintegral} yields
\begin{equation}
  \label{eq:q2dfg}
  \begin{aligned}
    g_{k}\left(K\right) & = \frac{1}{2K^3}\left[1-{\erf}\left(\frac{K}{\sqrt{\pi}}\right)\right]\exp\left(\frac{K^2}{\pi}\right)\\
    f_{k}\left(K\right) & = \left(\frac{1}{2} - \frac{K^{2}}{\pi}\right)g_k(K) - \frac{1}{2\pi K^3}
  \end{aligned}  
\end{equation}
We can also write the Green's function for true 2D by redefining $f_k$ and $g_k$ (just the Oseen tensor in 2D)
\begin{equation}
  \label{eq:t2dfg}
  \begin{aligned}
    g_{k}\left(K\right) & = 0\\
    f_{k}\left(K\right) & = \frac{a}{K^4}
  \end{aligned}  
\end{equation}
The Green's function \eqref{eq:q2dGreenintegral} is purely two-dimensional, allowing to reuse the \gls{FCM} machinery without the third direction altogether.

Using Eq. \eqref{eq:t2dfg} instead of Eq. \eqref{eq:q2dfg} allows for our algorithm to simulate a true 2D system. In fact, any 2D hydrodynamic kernel can be used as long as $f_k$ and $g_k$ are known\footnote{For instance, the Saffman kernel for membrane hydrodynamics \cite{Brown20112.}}.

The particle dynamics can thus be formally written as



%Up until now, we have not included the action of the thermal drift in the \gls{FCM}. We can use the general definition of the mobility tensor in Eq. \eqref{eq:bdhimob} as the double convolution of the Green's function with the kernel to write the thermal drift in the a similar form
%
%\begin{equation}
%  \label{eq:q2Ddriftconv}
%  \begin{aligned}
%    \partial_{\vec{\ppos}}\tens{M}_{ij}=& \int\delta_a(\vec{\ppos_i} -\vec{r})\nabla\cdot\tens{G}(\vec{r}-\vec{r}')\delta_a(\vec{\ppos_j} -\vec{r})d\vec{r}d\vec{r}' =\\
%     &\int\delta_a(\vec{\ppos_i} -\vec{r})\tens{G}(\vec{r}-\vec{r}')\nabla\delta_a(\vec{\ppos_j} -\vec{r})d\vec{r}d\vec{r}'    
%\end{aligned}
%\end{equation}
%
%Where the second equality can be easily proven via integration by parts. A formal proof of this description can be found in~\cite{Donev2014}. One simple way to see this is by computing the contribution of the thermal drift to the fluid velocity in Fourier space. Since we have periodic boundary conditions (remember that our description is now in the plane) the divergence commutes in Fourier space we can simply write
%\begin{equation}
%  i\vec{k}\cdot\fou{\tens{G}}\mathfrak{F}\left[\delta_a(\vec{\ppos}_j -\vec{r})\right] = \fou{\tens{G}}\cdot \left(i\vec{k}\mathfrak{F}\left[\delta_a(\vec{\ppos}_j -\vec{r})\right]\right)
%\end{equation}
%Which evidences that we can take the divergence in Eq. \eqref{eq:q2Ddriftconv} from the Green's function to the kernel.
%Once we have this, we can include the thermal drift in the \gls{FCM} framework as an external fluid forcing by redefining the force term in Eq. \eqref{eq:fcmvel} as
%
%\begin{equation}
%  \label{eq:q2Ddrifasforce}
%  \vec{f} = \oper{S}\vec{F} + \vec{\partial}\oper{S}(\kT)
%\end{equation}
%
%Where $\partial_\alpha\oper{S} = \partial\oper{S}/\partial r_\alpha$ is the derivative of the kernel, in this case a Gaussian. Note that we can either spread the quantity $\kT$ to the grid using the derivative of the kernel, or use the regular kernel and \emph{ik} differentiate the term in Fourier. However, since the second option would require to isolate the Fourier transform of the thermal drift it is in principle not worth it over the simple evaluation of the derivative of a Gaussian in real space.
%\footnote{In fact, in the case of a Gaussian, we can spread a single quantity by using $\partial\phi_G(r) = 2\sigma_ar\phi_G(r)$ and spreading $\vec{F} + 2\kT\sigma_a\vec{r}$.}
%
%Interestily enough, interpreting the thermal drift as an external force gives us some insight into the physical effects of the non zero divergence of the mobility. In particular, because the quasi 2D mobility will still be translationally invariant and isotropic in the plane, we can still assume the general form in Eq. \eqref{eq:bdhimobgeneral}. In general, at long distances we have the Oseen approximation for $f(r)$ and $g(r)$\footnote{Interpreting that $M_0=1$ for the mobility written in this way.}
%\begin{equation}
%  f(r) \approx g(r) \approx \frac{1}{8\pi\eta} \text{ for } r\gg a
%\end{equation}
%Which yields a zero divergence in true 3D and true 2D. However, in quasi 2D we have
%\begin{equation}
%  \nabla\cdot\tens{M}_{\qtd} \approx \frac{1}{8\pi\eta r}\frac{\vec{r}}{r^3} \text{ for } r\gg a
%\end{equation}
%Using Eq. \eqref{eq:q2Ddrifasforce} we can see that this is equivalent to having the particles interact via a repulsive electrostatic potential
%\begin{equation}
%  \label{eq:q2Deffrepulsion}
%  U_{\text{eff}}(r) = \frac{3a}{4r}\kT
%\end{equation}
%It is important to keep in mind, though, that this force is stochastic in nature and is in fluctuation-dissipation balance with the thermal fluctuations. The physical origin of this effective force comes from the rapid momentum transport perpendicular to the plane, which makes the plane in the flow behave with an effective non zero compressibility. This makes the quasi 2D system inherently different from a distribution of charges. In fact, since hydrodynamics will not change the equilibrium properties, these equations still describe, in the absence of other particle interactions, an ideal gas without structure. Nevertheless, this interpretation allows us to see the dramatic (purely hydrodynamic) effect of the confining force in the plane flow and highlights the necessity of including the thermal drift into the quasi 2D description.

%Let us rewrite here the particle velocity equation \eqref{eq:bdhibdrelation} for the quasi 2D case,
\begin{equation}
  \label{eq:q2dbdhirel}
  \frac{d\vec{\ppos}_i}{dt} = \oper{J}_{\vec{\ppos}_i}\left[\tens{G}_{\qtd}\left(\oper{S}\vec{F} + \vec{\partial}\oper{S}(\kT)\right) + \vec{w}(\vec{\fpos}, t)\right].
%  \begin{aligned}
  %&\int{\delta_a(\vec{\ppos}_i-\vec{r}')\left[\sum_j\tens{G}_{\qtd}(\vec{r}', \vec{r}'')\left[\delta_a(\vec{\ppos}_i-\vec{r}'')\vec{F}_j + \kT\vec{\partial}(\delta_a)(\vec{\ppos}_j-\vec{r}')\right]d\vec{r}'}d\vec{r}'' + \vec{w}(\vec{\ppos}_i, t) }
%\end{aligned}
\end{equation}
Here we have put the fluctuating part, $\vec{w}$, aside. The fluctuating stress tensor ($\tens{Z}$ in Eq. \eqref{eq:bdhibdrelation}) and its divergence are described in three dimensions. In the quasi 2D case we do not have an easy mechanism to project the stress tensor into the plane in order to keep a fully two dimensional description. Instead, we will use the fluctuation-dissipation balance to generate a stochastic fluid velocity directly.
%
%It is important to note that the gradient of the spreading operator in Eq. \eqref{eq:q2dbdhirel} is taken in the plane in which diffusion happens (i.e two dimensions). Similarly, we would like for spreading and interpolation to also take place in the two-dimensional confining plane.
%In order to do that, we preconvolve the three dimensional Green's function of the Stokes equation (i.e the Oseen tensor in Eq. \eqref{eq:bdhioseen})  in the third direction leveraging the fact that, by construction, there are no particle forces in the third direction and particles remain strictly in the plane. In Fourier space, this amounts to defining the quasi 2D Green's function as the integral of the Oseen tensor multiplied by the square of the spreading kernel (a Gaussian),
%\begin{equation}
%  \label{eq:q2dGreenintegral}
%  \begin{aligned}
%  \fou{\tens{G}}_{\qtd}(\vec{k} = (k_x, k_y)) &= \frac{1}{2\pi\eta}\int_{k_z} \frac{dk_z}{(k')^2}\exp\left(-\frac{a^2k_z^2}{\pi}\right)\left( \mathbb{I} - \frac{\vec{k}'\otimes\vec{k}'}{(k')^2}\right) \\
%  &= \eta^{-1}\left(g_k(ka)\vec{k}_{\perp}\otimes\vec{k}_{\perp} + f_k(ka)\vec{k}\otimes\vec{k}\right).
%\end{aligned}
%\end{equation}

As previously discussed (see chapter \ref{sec:bdhicon}), the mobility tensor corresponding to Eq. \eqref{eq:q2dGreenintegral} can be written as
\begin{equation}
  \tens{M}_{\qtd} = \oper{J}\tens{G}_{\qtd}\oper{S}.
\end{equation}
On the other hand, we know that fluctuation-dissipation (discussed in chapter \ref{sec:fdb}) mandates that the stochastic particle displacements,
\begin{equation}
\tilde{\vec{\pvel}}_i := \oper{J}_{\vec{\ppos}_i}\vec{w},
\end{equation}
must be related with the mobility as
\begin{equation}
  \langle\tilde{\vec{\pvel}}_i\otimes\tilde{\vec{\pvel}}_i\rangle = \frac{2\kT}{dt} \tens{M}_{\qtd}.
\end{equation}
Equivalently, we can write the fluctuation-dissipation relation for the fluid velocity in Fourier space,
\begin{equation}
  \label{eq:q2dfdb}
  \left\langle\fou{\vec{w}}\otimes \fou{\vec{w}}\right\rangle = \frac{2\kT}{dt} \fou{\tens{G}}_{\qtd}.
\end{equation}
It is straightforward to devise a functional form for the stochastic fluid velocity that satisfies Eq. \eqref{eq:q2dfdb}.
We can use a similar trick as we did for \gls{PSE} in Eq. \eqref{eq:psenoise} to separate the contribution of the noise in two,
\begin{equation}
  \begin{aligned}
    \fou{\vec{w}}(\vec{k}, t) := \sqrt{\frac{2\kT}{\eta}}\left(\sqrt{f_k(ka)}\vec{k}_\perp\fou{\tens{Z}}^1_k + \sqrt{g_k(ka)}\vec{k}\fou{\tens{Z}}^2_k\right).
  \end{aligned}
\end{equation}
Where $\fou{\tens{Z}}^{1,2}_k$ are independent Wiener processes. The same concerns about the delicate conjugacy properties of the noise we discussed in sec. \ref{sec:fcm} are applicable here.

We now have all the necessary ingredients to apply the \gls{FCM} framework and solve Eq. \eqref{eq:q2dbdhirel} for the particle velocities.
% This integral can be done analitically to give
%\begin{equation}
%  \fou{\tens{M}}_{\text{q2D}} = \frac{1}{\eta k^3}\left( \half\vec{k}_{\perp}\otimes\vec{k}_{\perp} + \frac{1}{4}\vec{k}\otimes\vec{k}\right)
%\end{equation}


\subsubsection*{Use in UAMMD}
The quasi 2D \gls{FCM} described in the previous section is available as an \emph{Integrator} in \uammd. The particular module, called \mintinline{\ucpp}{BDHI::BDHI2D} is templated for any 2D Green's function by providing $f_k$ and $g_k$ (from Eq. \eqref{eq:q2dGreenintegral}) through a special structure (refer the online documentation for more information). The aliases \mintinline{\ucpp}{BDHI::True2D} and \mintinline{\ucpp}{BDHI::Quasi2D} are provided as specializations of the base class for the hydrodynamic kernels described in the previous section (true 2D and quasi 2D respectively).
The translational invariance of the hydrodynamic radius is kept below a certain threshold via the tolerance parameter, which controls the support of the Gaussian kernels and the dimensions of the grid.
The rest of the input parameters are similar to the ones required for the rest of the hydrodynamic modules we have seen thus far.
%\todo{I could implement the same thing for FCM, generalizing the module for any $f_k$ and $g_k$ as long as they do not have thermal drift, which is not implemented in 3D. Maybe this could be done for FIB, where thermal drift is obtained via RFD.}
\begin{code2}[Example of the creation of a quasi 2D \emph{Integrator}.]{label=code:q2D}
#include<uammd.cuh>
#include<Integrator/Hydro/BDHI_quasi2D.cuh>
using namespace uammd;
//A function that creates and returns a quasi 2D integrator
auto createIntegratorQ2D(UAMMD sim){
  //Choose the hydrodynamic kernel
  using Hydro2D = BDHI::Quasi2D;
  //using Hydro2D = BDHI::True2D;
  Hydro2D::Parameters par;
  par.temperature = sim.par.temperature;
  par.viscosity = sim.par.viscosity;
  par.hydrodynamicRadius = sim.par.hydrodynamicRadius;
  par.dt = sim.par.dt;
  par.tolerance = sim.par.tolerance;
  par.box = sim.par.box;
  auto q2d = std::make_shared<Hydro2D>(sim.pd, par);
  return q2d;
}
\end{code2}

\newpage

\chapter{Hydrodynamics in doubly periodic geometries}\label{sec:dpstokes}
Thus far we have seen how to compute hydrodynamic displacements in domains that are either triply periodic (periodic in the three directions) or two dimensional and periodic in the plane. In the case of quasi 2D, where the particles are constrained to a plane, the boundary conditions for the fluid in the perpendicular directions are open, while in true 2D both the fluid and the particles exist in the plane. We refer to a domain as \gls{DP} when it is periodic in the plane directions ($x,y$) but aperiodic in the perpendicular one ($z$). In this sense the quasi 2D algorithm in chapter \ref{sec:q2D} can be regarded as a \gls{DP} algorithm. However, the aforementioned algorithm only allows to simulate environments in which the submerged particles diffuse in two dimensions (for instance, a fluid-fluid interface). If we are looking to model a system lying in between the triply periodic and quasi 2D regimes, for instance a lipid membrane (see Fig. \ref{fig:dpstokessketch}), currently our only option is to perform a triply periodic simulation and do some kind of finite-size analysis, measuring the relevant properties with an increasingly large simulation domain size in the perpendicular direction. Of course, given that the algorithmic complexity of all our pseudo-spectral algorithms scales linearly with the volume of the domain (or more appropriately, with the number of cells in the grid), the triply periodic approach is not optimal.
\begin{figure}[h]
  \centering
  \includesvg[width=\columnwidth]{gfx/dpstokes_sketch}
  \caption[ ]{Representation of a lipid membrane inside a doubly periodic domain. The \glspl{BC} can be customized at the domain limits ($z=-H,H$) to be either open or have no-slip walls.}
  \label{fig:dpstokessketch}
\end{figure}

We propose a novel pseudo-spectral \gls{FCM}-like algorithm, with a wildly different approach to the ones we have seen thus far, that allows to compute the hydrodynamic displacements of a suspension of colloids inside a \gls{DP} domain with either open or no-slip conditions in the third direction.
In particular, we will see how to solve the incompressible Stokes equation \eqref{eq:stokes} without fluctuations in a \gls{DP} domain of size $L_{x,y}$ (we will assume $L_x=L_y$, which can then be easily generalized). Thus, we want to solve
\begin{equation}
  \label{eq:dpstokes}
  \begin{aligned}
    &\nabla p_{\dpr} - \eta\nabla^2\vec{\fvel}_{\dpr} = \vec{f}\\
    &\nabla\cdot\vec{\fvel}_{\dpr} = 0,
\end{aligned}
\end{equation}
with $z\in(-\infty, \infty)$ and a certain set of \glspl{BC} (that will be introduced shortly) in $z$, assuming that $\vec{\fvel}_{\dpr}$ is bounded as $|z|\rightarrow\infty$. The plane $(x,y)$ is periodic as in quasi 2D.

Here $p$ is the pressure and the fluid forcing includes the particle forces and torques (similarly to the section on \gls{FCM}, see sec. \ref{sec:fcm})
\begin{equation}
  \vec{f}(\vec{\fpos}) = \oper{S}\vec{F} + \half\nabla\times\oper{S}_{\tau}\vec{\tau}.
\end{equation}

We assume that the fluid forcing is zero outside a domain $z\in [-H, H]$. In the case of an unbounded fluid this domain is arbitrary and physically meaningless, serving only to define a numerical domain for our grid-based solver. The same thing happens in the case of a bottom wall with the top limit at $z=H$. Furthermore, assuming $\vec{f}(|z|>H) = 0$ allows us to define a set of \glspl{BC} more easily.

Our solver is periodic in the plane $(x,y)$ (as in quasi 2D). In the $z$ direction, by incorporating different \glspl{BC}, we distinguish between three modes:
\begin{enumerate}
\item Unbounded in $z$, $z\in(-\infty, \infty)$. $\vec{\fvel}_{\dpr}$ is bounded as $|z|\rightarrow\infty$.
\item A no-slip  wall at the bottom of the domain, $z=-H$ (unbounded at $z>H$). $z\in [-H, \infty)$.
  The no-slip \gls{BC} for the wall is simply
  \begin{equation}
    \label{eq:dpstokesbwvelbc}
    \vec{\fvel}_{\dpr}(z=-H) = 0.   
  \end{equation} 
\item A slit channel, with no-slip walls located at $z=-H$ and $z=H$. $z\in [-H, H]$. Besides the \gls{BC} for the bottom wall, we add a similar one for the top one
  \begin{equation}
    \label{eq:dpstokestwvelbc}
    \vec{\fvel}_{\dpr}(z=H) = 0.
  \end{equation}
\end{enumerate}
The unbounded mode allows to generalize our quasi2D simulations to systems where particles are not restricted to a plane. Additionally the other modes can be used in a variety of simulations. For instance, the bottom wall mode can be used to model the hydrodynamics of a Quartz Crystal Microbalance (QCM), improving existing models~\cite{Melendez2020}.

The present \gls{DP} solver is vastly different from the \gls{FCM} family of methods we have seen thus far.

Our approach consists in solving the Stokes equation in an unbounded domain, adding the effects of the walls afterwards as a correction. Thus we first solve Eq. \eqref{eq:dpstokes} with free-space \glspl{BC} and $z\in (-\infty, \infty)$. Then we compute the final result in the presence of one or two walls as
\begin{equation}
  \label{eq:dpstokescorrsum}
  \begin{aligned}
  &\vec{\fvel} =   \vec{\fvel}_{\dpr} +   \vec{\fvel}_{\corr}\\
  &\vec{p} =   \vec{p}_{\dpr} +   \vec{p}_{\corr}.
\end{aligned}
\end{equation}

For the corrections we solve analytically Eq. \eqref{eq:dpstokes} in the absence of forces with one or two slip walls. In particular we have
\begin{equation}
  \label{eq:dpstokescorr}
\begin{aligned}
    &\nabla p_{\corr} - \eta\nabla^2\vec{\fvel}_{\corr} = 0\\
    &\nabla\cdot\vec{\fvel}_{\corr} = 0,
\end{aligned}
\end{equation}
solved in a domain with $z\in [-H,\infty)$ in the case of a bottom wall and $z\in [-H, H]$ in the case of a slit channel. Finally, we set slip \glspl{BC} at the walls so

\begin{equation}
  \label{eq:dpstokescorrbcs}
  \begin{aligned}
    &\vec{\fvel}_b:=-\vec{\fvel}_{\corr}(z=-H) = \vec{\fvel}_{\dpr}(z=-H)\\
    &\vec{\fvel}_t:=-\vec{\fvel}_{\corr}(z=H) = \vec{\fvel}_{\dpr}(z=H),
\end{aligned}
\end{equation}
where the second condition, at $z=H$, is imposed only in the case of a slit channel geometry.

In Appendix \ref{sec:bvp} we describe a fast and \gpu-friendly \gls{BVP} solver that solves one-dimensional \glspl{PDE} in the Chebyshev basis. Our strategy in the next sections will be to write equations for the velocity and pressure in a way that is compatible with this solver. 

We will first discuss the free-space solver ($\vec{\fvel}_{\corr} = 0$ and $p_\corr=0$) and then see how to solve the correction in each case. 
\section{Free-space solver}
Let us start by writing the equations for the pressure (Eq. \eqref{eq:stokespressure}) in Fourier space only in the plane $(x,y)$, which makes it easier to incorporate arbitrary \glspl{BC} in the $z$ direction.

\subsection*{Pressure solve}
By taking the divergence of Eq. \eqref{eq:dpstokes} we eliminate the velocity
\begin{equation}
  \label{eq:dpstokespressure}
  \nabla^2 p = \nabla\cdot\vec{f}.
\end{equation}
Transforming to Fourier space in the plane we get a one dimensional problem for each wave number
\begin{equation}
  \label{eq:dpstokesfreepressure}
  (\partial^2_{z}-k^2)\fou{p} =
  % \begin{bmatrix}
  \left[
    i\vec{k},
    \partial_z\right]
  % \end{bmatrix}
  \cdot\vec{f},
\end{equation}
where the wave vector, $\vec{k}:=(k_x, k_y)$, is defined on the plane and has modulus $k^2 = k_x^2 + k_y^2$. The forces, $\vec{f}=(f^x, f^y, f^z)$ can be non-zero only inside a certain domain with $z\in [-H,H]$. Note that the pressure (and the forces) are Fourier transformed only in the plane, so that $\fou{p} :=\fou{p}(\vec{k}, z)$. As a matter of fact, the \gls{FCT} is employed here to transform the signals in the $z$ direction to the Chebyshev basis, as this allows to use the \gls{BVP} solver and facilitates the evaluation of the derivatives in this direction (by using the relations for the Chebyshev coefficients introduced in Appendix \ref{sec:bvp}). Therefore we will work with the Chebyshev coefficients, $\fou{p}_n$ (or $\fou{\vec{f}}_n$), of the different quantities.

We can solve the pressure outside the domain using that
\begin{equation}
    (\partial^2_{z}-k^2)\fou{p} = 0, \quad \text{ for } z \notin [-H,H].
\end{equation}
Whose solution is
\begin{equation}
  \fou{p}(k, z) = C_1\exp(-kz) + C_2\exp(kz).
\end{equation}
By using the boundedness of the pressure at $|z|\rightarrow\infty$ we can write the solution outside the domain
\begin{equation}
  \begin{aligned}
    &\fou{p}(k,z\le -H) = C_2\exp(kz)\\
    &\fou{p}(k,z\ge H) = C_1\exp(-kz).
  \end{aligned}
\end{equation}
This implies
\begin{equation}
  \label{eq:dpstokespressurebcs}
  (\partial_z\pm k)\fou{p}(k, z=\pm H) = 0.
\end{equation}
For a given wave number, $k$, the system in Eq. \eqref{eq:dpstokesfreepressure} along the \glspl{BC} in Eq. \eqref{eq:dpstokespressurebcs} constitutes a \gls{BVP} that can be solved in Chebyshev space with the solver described in Appendix \ref{sec:bvp}.

Since this solver requires to define $z$ in the Chebyshev basis, we can apply $\partial_z$ in linear time by using recurrent relations on the Chebyshev coefficients (in this case of $\fou{f}^z$) instead of via \emph{ik} differentiation. As discussed in Appendix \ref{sec:bvp}, the Fourier-Chebyshev\footnote{Fourier in the plane $(x,y)$ and Chebyshev in $z$.} coefficients $\fou{p}(\vec{k}, z)$ are computed via a hybrid \gls{FFT}-\gls{FCT}. Which amounts to performing the 3D \gls{FFT} of $\vec{f}$ periodic extended in $z$.

\subsection*{Velocity solve}
Once the Chebyshev coefficients for the pressure for each wave number are known we can write the equations for the three components of the velocity in Fourier space as

\begin{equation}
  \label{eq:dpstokesvel}
  \eta\left(\partial^2_{z} -k^2\right)\fou{\vec{\fvel}} = 
  \begin{bmatrix}
    i\vec{k}\\
    \partial_z
  \end{bmatrix}
  \fou{p} -\fou{\vec{f}},
\end{equation}
where the derivative of the pressure, $\partial_z\fou{p}$, can be computed via the Chebyshev coefficients of the pressure.
For the \glspl{BC} we know that outside the domain, where $\fou{\vec{f}} = 0$, the velocity satisfies
\begin{equation}
  \label{eq:dpstokesvelout}
  \eta\left(\partial^2_{z} -k^2\right)\fou{\vec{\fvel}} = 
  \begin{bmatrix}
    i\vec{k}\\
    \partial_z
  \end{bmatrix}
  \fou{p}, \quad \text{ for } z \notin [-H,H].
\end{equation}
The \glspl{BC} for Eq. \eqref{eq:dpstokesvelout} must be computed independently for the plane, $\vec{\fvel}^\parallel$, and perpendicular, $\vec{\fvel}^\perp$, velocities.
\subsubsection*{Parallel velocity solve}
Using the boundness of the velocity at $\pm\infty$, the solution of Eq. \eqref{eq:dpstokesvelout} in the plane for $z \le -H \text{ and } z\ge H]$ is, respectively
\begin{equation}
  \label{eq:dpstokesparvel}
  \fou{\vec{\fvel}}^{\parallel} = \mp\frac{C_1\vec{k}\exp(\pm kz)\left(2zk\mp 1\right)}{4\eta ik^2} + C_2\exp(\pm zk).
\end{equation}
The derivative of Eq. \eqref{eq:dpstokesparvel} can be written as
\begin{equation}
  \label{eq:dpstokesparvelder}
  \partial_z\fou{\vec{\fvel}}^{\parallel} = \pm k\fou{\vec{\fvel}}^{\parallel} + \frac{C_1i\vec{k}\exp(\pm kz)}{2\eta k}.
\end{equation}
We can make use of the previously computed solution for the pressure outside the domain,
\begin{equation}
  \label{eq:dpstokespresout}
\fou{p}(\vec{k}, z \le -H \text{ or } z\ge H]) = C_1\exp\left(\pm zk\right),
\end{equation}
to finally write the \glspl{BC} for the parallel velocity as
\begin{equation}
  \label{eq:dpstokesparvelbc}
  \left(\partial_z\pm k\right)\fou{\vec{\fvel}}^\parallel(\vec{k}, \pm H) = \mp \frac{i\vec{k}}{2\eta k}\fou{p}(\vec{k}, \pm H).
\end{equation}
Note that the pressure at the domain limits, $\fou{p}(\vec{k}, \pm H)$, can be easily computed from the already available Chebyshev coefficients of the pressure.

\subsubsection*{Perpendicular velocity solve}
Following a similar strategy as with the parallel velocity, we can write the solution for the perpendicular velocity outside the domain as
\begin{equation}
    \label{eq:dpstokesperpvel}
  \fou{\vec{\fvel}}^{\perp} = \frac{C_1\vec{k}\exp(\pm kz)\left(2zk\mp 1\right)}{4\eta k} + C_2\exp(\pm zk)
\end{equation}
and its derivative
\begin{equation}
  \label{eq:dpstokesperpvelder}
  \partial_z\fou{\vec{\fvel}}^{\perp} = \pm k\fou{\vec{\fvel}}^{\perp} + \frac{C_1\exp(\pm kz)}{2\eta}.
\end{equation}
%Now, since we know that
%\begin{equation}
%  \partial_z\fou{p}(\vec{k}, z \le -H\text{ or} z\ge H) = \pm C_1\exp\left(\pm kz\right), 
%\end{equation}
Identifying the already known solution for the pressure (see Eq. \eqref{eq:dpstokespresout}) we can finally write the \glspl{BC} for the perpendicular velocity as
\begin{equation}
  \label{eq:dpstokesperpvelbc}
  \left(\partial_z\pm k\right)\fou{\vec{\fvel}}^\perp(\vec{k}, \pm H) = \frac{1}{2\eta}\fou{p}(\vec{k}, \pm H).
\end{equation}
%\todo{The k=0 modes}
Thus far we have shown how to compute, for all wave numbers, the Chebyshev coefficients for the pressure and the three directions of the velocity in the free-space case. We will now study the correction in the bottom wall and slit channel geometries.
\section{Corrections}
We will compute, for each wave number, the analytical solution of the velocity and pressure inside the domain due to the presence of the walls and sum it to the main solution.

Let us start with the bottom wall case.
\subsection*{Bottom wall}

We want to solve Eq. \eqref{eq:dpstokescorr} with \glspl{BC} in Eq. \eqref{eq:dpstokesbwvelbc}.

For simplicity, let us define and $z':=(z+H)/2$.
It is straightforward to prove that the following solutions for the velocity and the pressure satisfy Eq. \eqref{eq:dpstokescorr} with \glspl{BC} in Eq. \eqref{eq:dpstokesbwvelbc}:
\begin{equation}
  \fou{\vec{\fvel}}_{\corr}(\vec{k}, z) = \left(  \fou{\vec{\fvel}}_b -
    \begin{bmatrix}
    \vec{k}\\
    ik
  \end{bmatrix}
  \left(\left[\vec{k},ik\right]\cdot\fou{\vec{\fvel}}_b\right) \frac{z'}{k}\right)\exp\left(-kz'\right),
\end{equation}
\begin{equation}
  \fou{p}_{\corr}(\vec{k}, z) = 2\eta \left[-i\vec{k},k\right]\cdot\fou{\vec{\fvel}}_b\exp\left(-kz'\right).
\end{equation}

The velocity at the bottom, $\fou{\vec{\fvel}}_b$, can be trivially evaluated via its already available Chebyshev coefficients. Once the solution is computed for every wave number, the Chebyshev coefficients can be obtained by applying the \gls{FCT} to each of them. Finally, we compute the total solution using Eq. \eqref{eq:dpstokescorrsum}.

\subsection*{Slit channel}
The double wall case proves to be a little more convoluted. We want to solve Eq. \eqref{eq:dpstokescorr} with \glspl{BC} in Eq. \eqref{eq:dpstokesbwvelbc} and \eqref{eq:dpstokestwvelbc}.
The general solution for the correction in this case is
\begin{equation}
  \label{eq:dpstokesslitcor}
  \begin{aligned}
    \fou{\vec{\fvel}}_{\corr}(\vec{k}, z) =& \left(\frac{C_0}{2\eta k} z'
      \begin{bmatrix}
        -i\vec{k}\\
        k
      \end{bmatrix}
      +
      \begin{bmatrix}
        C_1\\
        C_2\\
        C_3
      \end{bmatrix}
    \right)
    \exp(-kz')+\\    
    &\left(\frac{D_0}{2\eta k} z'
      \begin{bmatrix}
        i\vec{k}\\
        k
      \end{bmatrix}
      +
      \begin{bmatrix}
        D_1\\
        D_2\\
        D_3
      \end{bmatrix}
    \right)\exp(kz')
  \end{aligned}
\end{equation}
\begin{equation}
  \fou{p}_{\corr}(\vec{k}, z) = C_0\exp(-k z') + D_0\exp(k z')
\end{equation}
The six equations above are not enough to compute the values of the eight unknown coefficients, $\vec{C}=C_{[0,1,2,3]}$ and $\vec{D} = D_{[0,1,2,3]}$. For the remaining two equations we can use the incompressibility condition in Eq. \eqref{eq:dpstokes},
\begin{equation}
  \label{eq:dpstokesincompres}
  [i\vec{k}, \partial_z] \cdot\fou{\vec{\fvel}}_{\corr}(\vec{k}, z) = 0,
\end{equation}
evaluated at both the domain limits, $z=-H,H$.
Evaluating Eq. \eqref{eq:dpstokesincompres} and replacing $\partial_z\fou{\fvel}_{\corr}^z(\vec{k}, z=\pm H)$ from Eq. \eqref{eq:dpstokesslitcor} yields the last two required equations,
\begin{equation}
  \begin{aligned}
    \frac{C_0}{2\eta} + \frac{D_0}{2\eta} - C_3k + D3_k &= -ik_x\fou{\vec{\fvel}}_b^x - ik_y\fou{\vec{\fvel}}_b^y\\
    \left[(1-2kH)\frac{C_0}{2\eta}-kC_3\right]\exp(-2kH) +&\\ \left[(1+2kH)\frac{D_0}{2\eta} + kD_3\right]\exp(2kH) &= -ik_x\fou{\vec{\fvel}}_t^x - ik_y\fou{\vec{\fvel}}_t^y.
\end{aligned}
\end{equation}
We now have a linear system of 8 equations and 8 unknowns ($\vec{C}$ and $\vec{D}$). Once this system is solved\footnote{The system can be written in matrix form and partially precomputed (via matrix inversion) so that solving the slit correction at runtime amounts to a simple 8x8 matrix-vector multiplication. The related functions in \uammd can be found in the file \emph{StokesSlab/Correction.cuh}.} we can evaluate the correction and compute its Chebyshev coefficients. Finally we sum the correction to the free domain solution, obtaining the final result as per Eq. \eqref{eq:dpstokescorrsum}.

\section{The zeroth wave number}
Instead of computing the zeroth mode for each of the subproblems separately (the free space solver and the correction solve) we treat them as a unique solver for simplicity. The free space solver (no walls) does not require a correction and handling the zeroth mode amounts to simply setting $\vec{\fvel}(\vec{k}=0,z) = 0$. Let us go through the bottom wall and slit channel geometries.

\subsection*{Bottom wall}
For the zeroth mode, the equation for total pressure is reduced to
\begin{equation}
  \label{eq:dppressurebtm}
  \partial_z\fou{p}(\vec{k}=0, z) = \fou{f}_z(\vec{k} = 0, z),
\end{equation}
the solution of which is
\begin{equation}
  \fou{p}(\vec{k}=0, z) = \int_{-H}^z\fou{\vec{f}}(\vec{k} = 0, z')dz' + C.
\end{equation}
We can see that the constant $C=0$ by choosing $\fou{p}(\vec{k} =0, z=-H) = 0$ and write a \gls{BVP} for the pressure that can be solved as usual,
\begin{equation}
  \partial_z^2\fou{p}(\vec{k} = 0, z) = \partial_zf_z(\vec{k}=0, z),
\end{equation}
with \glspl{BC} given by
\begin{equation}
  \fou{p}(\vec{k} = 0, z=-H) = 0, \quad \fou{p}(\vec{k} = 0, z=H) = \int_{-H}^H\fou{\vec{f}}(\vec{k} = 0, z')dz'.
\end{equation}
On the other hand, the in-plane velocity satisfies
\begin{equation}
  \partial_z^2\fou{\vec{\fvel}}^{\parallel} = -\eta^{-1}
  \begin{bmatrix}
    1\\
    1
  \end{bmatrix}
  \fou{f}_z.
\end{equation}
For the \glspl{BC} we can use the fact that the bottom wall imposes a no-slip condition to elucidate $\fou{\fvel}^\parallel(\vec{k} = 0, z=-H) = 0$. Given that the forces are bound to the inside of the domain, we can assume that $\fou{\fvel}^\parallel(\vec{k} = 0, z\ge H)$ is constant. Using the continuity of its derivative at the domain limit, we can obtain the second \gls{BC} as $\partial_z\fou{\vec{\fvel}}^\parallel(\vec{k} = 0, z=H) = 0$.

Finally, the equation for the perpendicular velocity when $\vec{k} = 0$ is simplified to
\begin{equation}
  \partial_z\fou{\vec{\fvel}}^\perp(\vec{k} = 0, z) = 0.
\end{equation}
Making use of the no-slip wall, we can simply choose $\fou{\vec{\fvel}}^\perp(\vec{k} = 0, z) = 0$.

\subsection*{Slit channel}
Since the difference between the boundary conditions of the slit channel and bottom wall cases only affect the velocity at the boundaries, the computation of the pressure is equivalent in both cases.
Furthermore, since for the bottom wall we chose $\fou{\vec{\fvel}}_z(\vec{k}=0, z) =0$ just by using the no-slip condition at the bottom wall, this component is also equivalent\footnote{Any other value would result in a net flow across the no-slip walls, which is of course impossible.}.

The \gls{BVP} for the parallel velocities can be laid out following similar arguments as with the bottom wall case. In particular, the zeroth mode satisfies
\begin{equation}
  \partial_z^2\fou{\vec{\fvel}}^\parallel(\vec{k} = 0, z) = -\eta^{-1}
  \begin{bmatrix}
    1\\
    1
  \end{bmatrix}
  \fou{f}_z.
\end{equation}
For the \glspl{BC}, due to the no-slip condition, we have $\fou{\vec{\fvel}}^\parallel(\vec{k} = 0, z=\pm H) = 0$.

\section{On spreading and interpolation}

Using the Chebyshev basis to discretize the $z$ direction makes the grid of our solver non-regular. In particular, we will have a regular binning in the plane directions and bins defined at the extrema of the Chebyshev polynomials in $z$.
%Thus we need to evaluate the forces in Eq. \eqref{eq:dpstokes} at the roots of the Chebyshev polynomials in the $z$ direction.

Similarly to all the Force Coupling Method-like methods described thus far, the communication between the particles information and the grid (in this case to construct $\vec{f}$ in Eq. \eqref{eq:dpstokes} and interpolate the velocities back to the particle positions) is carried out via the Immersed Boundary module in UAMMD (see chapter \ref{ch:icm}). Note however, that there is a fundamental difference with respect to the rest of the use cases for the \gls{IBM} up to this point. In particular, now the grid is not regular in the $z$ direction, being a Chebyshev grid instead (see Fig. \ref{fig:chebgrid}). 
\begin{figure}[h]
  \centering
  \includesvg[width=0.5\columnwidth]{gfx/chebgrid}
  \caption[ ]{Representation of a hybrid regular-Chebyshev grid. In the Doubly Periodic Stokes algorithm, the plane-parallel components ($x$ and $y$) are described on a regular grid (with nodes at $x = -L/2 + i/N L$ and similarly for $y$), whereas the perpendicular direction ($z$) is discretized at the Gauss-Chebyshev-Lobatto points (the extrema of the Chebyshev polynomials), $z=H\cos(\pi i/N_z)$ (where $i$ runs from $0$ to $N_z$).}
  \label{fig:chebgrid}
\end{figure}

The Chebyshev spacing in the $z$ direction has several consequences with respect to our usual regular grid. First, the non-regularity of the grid makes the number of support cells of the kernel vary with the height of the particle. Secondly, the quadrature weights (i.e. the sizes of each cell) are different for each height. In particular, we have to use the so-called Clenshaw-Curtis weights~\cite{Clenshaw1960}, so that a cell centered at the Chebyshev root $z = H\cos(\pi i/N_z)$ will have a quadrature weight, $w(z)$, of\footnote{The \uammd source code containing the special considerations for Chebyshev grids can be found at \emph{src/utils/ChebyshevUtils.cuh}.}
\begin{equation}
  \label{eq:clencurt}
  w(z=H\cos(\pi i/N_z)) = \frac{2}{N_z}\sum_{k=0}^i\cos\left(\frac{2ik\pi}{N_z}\right)\left\{
  \begin{aligned}
    &1/2 & i=0,N_z\\
    & 1 & \textrm{otherwise}
  \end{aligned}\right.
\end{equation}
In the plane directions, the quadrature weights are the cell sizes, $h$, and are the same everywhere.
The \uammd \gls{IBM} interface can be customized (via templates) to accept arbitrary quadrature weights and non regular grids as well as different supports for each particle according to its position.

Furthermore, given that the domain is not periodic in $z$, we need special considerations when spreading and interpolating near the boundaries. In the free-space case we can simply define the limits of the domain far enough from the particles so that $\vec{f}(z\notin [-H,H]) = 0$. However, in the presence of a wall we must ensure that the force goes smoothly to zero at the wall. We enforce this by subtracting the envelope of a given particle centered at its image about the wall. We can thus redefine the kernel close to the wall\footnote{Closer than the kernel's support, which effectively means everywhere.} as
\begin{equation}
  \label{eq:dpstokeskernel}
  \widetilde{\delta_a}(\vec{\fpos}-\vec{\ppos}_i) = \delta_a(\vec{\fpos}-\vec{\ppos}_i) - \delta_a(\vec{\fpos}-\vec{\ppos}^{\text{img}}_i),
\end{equation}
where $\vec{\ppos}_i^{\text{img}}$ is the particle's point of reflection about the wall, defined as
\begin{equation}
  \label{eq:dpstokesimg}
  \begin{aligned}
  &\vec{\ppos}_i^{\text{img}_b} = \vec{\ppos}_i - 2(H+r_z)\hat{\vec{e}}_z, \quad \text{For the bottom wall},\\
  &\vec{\ppos}_i^{\text{img}_t} = \vec{\ppos}_i + 2(H-r_z)\hat{\vec{e}}_z, \quad \text{For the top wall}.
\end{aligned}
\end{equation}
Here $\hat{\vec{e}}_z$ is a unitary vector in the $z$ direction. The no-slip condition is then imposed by the fact that a particle at the height of a wall does not affect (nor is it affected by) the fluid. A visual representation of the effect of the images is available in Fig. \ref{fig:dpstokesspread}.
\begin{figure}[h]
  \centering
  \includesvg[width=\columnwidth]{gfx/dpstokesspread}
  \caption[ ]{Representation of the image spreading in the \gls{DP} Stokes algorithm. Forces acting on particles (blue circles) are translated to the fluid as a force density. Due to images, part of the particle's force is zero near walls. Zero fluid forcing is depicted as white. A particle located at exactly the height of a wall has no effect on the fluid.}
  \label{fig:dpstokesspread}
\end{figure}



\section*{Use in UAMMD}
At the time of writing, the Doubly Periodic Stokes module is still in development (although in its final stages) and thus its interface is bound to change in the near future. Furthermore, its applicability is hindered by the absence of fluctuations, which make this module not suitable for Brownian Dynamics simulations in its current state (note, however, that the addition of fluctuations is a work in progress). For the time being this module is not available as an integrator and can only be used to compute the hydrodynamic displacements of a series of particles given the forces and/or torques acting on them. The heuristics for the different parameters required by the module are still being actively tested and developed, so many implementation details leak into the parameter list. In particular, the BM kernel parameters (mainly $\beta$ and the support, $w$) and the grid size information must be provided.

\begin{code2}[Using the DPStokes module for doubly periodic hydrodynamics.]{label=code:dpstokes}
#include <Integrator/BDHI/DoublyPeriodic/DPStokesSlab.cuh>
using namespace uammd::DPStokesSlab_ns;  
auto createDPStokesModule(Parameters par){
  DPStokes::Parameters par;
  par.nx         = par.nx;
  par.ny         = par.;
  par.nz	 = par.nz;
  par.viscosity	 = par.viscosity;
  par.Lx	 = par.Lx;
  par.Ly	 = par.Ly;
  par.H		 = par.H;
  par.w = par.w; //support for the forces
  par.beta = par.beta; //beta for the forces
  par.w_d = par.w_d; //suport for the torques
  par.beta_d = par.beta_d; //beta for the torques
  par.mode = WallMode::none; //Can also be bottom or slit
  auto dpstokes = std::make_shared<DPStokes>(par);
  return dpstokes;
}

auto computeHydrodynamicDisplacements(UAMMD sim, std::shared_ptr<DPStokes> dpstokes){
  auto pos = sim.pd->getPos(access::gpu, access::read);
  auto force = sim.pd->getForce(access::gpu, access::read);
  auto torques = sim.pd->getTorque(access::gpu, access::read);
  int numberParticles = pos.size();
  //The forces or torques can be replaced by a nullptr, which will spare the related computations.
  auto displacements = dpstokes->Mdot(pos.begin(), force.begin(), torques.begin(), numberParticles);
  //The result of Mdot contains the linear and dipolar displacements:
  //auto MF = displacements.first; //linear displacements
  //auto MT = displacements.second; //angular displacements
  return displacements;
}

\end{code2}

\chapter{Triply Periodic Electrostatics} \label{ch:tppoisson}

We are going to describe a fast spectral solver for the Poisson equation with periodic boundary conditions and Gaussian sources of charge with arbitrary widths centered at the particles' locations. Our approach is similar to the one presented at ~\cite{Lindbo2011}. However, the authors of ~\cite{Lindbo2011} make use of the so-called Fast Gaussian Gridding (FGG) to accelerate spreading/interpolation while UAMMD's implementation uses the algorithms laid out in chapter \ref{sec:ibm}. While FGG reduces the number of evaluations of the kernel, it is restricted to the Gaussian one and, since our implementation is generic for any kernel, we deem FGG unworthy for our purposes. The Ewald splitting framework, already introduced in chapter \ref{sec:pse} for the Stokes equation, follows closely from the mathematical machinery laid out in ~\cite{Tornberg2015}.
One notable thing about the following algorithm is that it can be seen as merely a reinterpretation of terms in the non-fluctuating version of the \gls{FCM}\footnote{In fact, UAMMD's implementation of triply periodic electrostatics shares most of the code with the \gls{FCM} implementation. Mainly the spreading/interpolation and the \gls{FFT}.}.
In general, we want to solve the Poisson equation in a periodic domain in the presence of a charge density $f(\vec{\fpos}=(x,y,z))$,
\begin{equation}
  \label{eq:ttpoisson}
 \varepsilon\Delta\phi=-f.
\end{equation}
Here $\varepsilon$ represents the permittivity of the medium and $f$ accounts for $N$ Gaussian charges of strength $Q_i$ located at $\vec{\ppos}_i$,
\begin{equation}
  \label{eq:tppoisson_cdens}
  f(\vec{\fpos})= \oper{S}(\vec{\fpos})\vec{Q} = \sum_iQ_i\delta_a(||\vec{\fpos}-\vec{\ppos}_i||).
\end{equation}
Let us denote with the vector containing all charges as $\vec{Q} = \{Q_1,\dots,Q_N\}$.
We will use the spreading operator, $\oper{S}$, (introduced in chapter \ref{sec:bdhi}) to transform the particles charges to a smooth charge density field. We use the Gaussian kernel,
\begin{equation}
  \label{eq:tpppoisson_gaussiansource}
  \delta_a(\vec{r})=\frac{1}{\left(2\pi a^2\right)^{3/2}}\exp{\left(\frac{-r^2}{2a^2}\right)},
\end{equation}
Identifying $a$ as the width the charges (notice that the case $a\rightarrow 0$ corresponds to point charges).
Once Eq. \eqref{eq:ttpoisson} is solved we have the value of the potential in every point in space and can be evaluated at the charges locations via the interpolation operator (introduced in chapter \ref{sec:bdhi})
\begin{equation}  
  \phi_{\vec{\ppos}_i} = \oper{J}_{\vec{\ppos}_i}\phi = \int Q_i\delta_a(\vec{\ppos}_i - \vec{\fpos})\phi(\vec{\fpos})d\vec{\fpos}.
\end{equation}
Here $\oper{J}$ represents the interpolation operator, that averages a quantity defined in space to a charge's location.
The electrostatic energy can be computed as
\begin{equation}
  \label{tppoisson_avgpot}
  U =  \frac{1}{2}\sum_i{\phi_{\vec{\ppos}_i}}.
\end{equation}
% Where $\bar{\phi}(\vec{z}_i)=\bar{\phi}_i$ is the convolution of the potential with a Gaussian centered at the charge's location and can also be interpreted as the average potential at that point.

In a similar way we compute the electrostatic force, $\vec{F}_i = -Q_i\nabla_i{\phi}$, acting on each charge from the electric field
\begin{equation}
  \label{eq:tppoisson_fieldnablaphi}
  \vec{E} = -\nabla{\phi}.
\end{equation}
Interpolating again we get
\begin{equation}
  \label{tppoisson_avgfield}
\vec{E}_i = \oper{J}_{\vec{\ppos}_i}\vec{E}(\vec{\fpos}).
\end{equation}
Thus, the electrostatic force acting on particle $i$ is
\begin{equation}
  \label{eq:tppoisson_force}
\vec{F}_i = Q_i\oper{J}_{\vec{\ppos}_i}\vec{E}.
\end{equation}

% Where the sum in Eq. \ref{eq:tpppoisson_gaussiansource} goes through every point $\vec{z}_k$ in the domain.
Given that Eq. \eqref{eq:tpppoisson_gaussiansource} has in principle an infinite support evaluating Eq. \eqref{eq:tppoisson_cdens} at every point in space, as well as computing the averages of the electric potential and field in Eqs. \eqref{tppoisson_avgpot} and \eqref{tppoisson_avgfield} can be highly inefficient. In practice we overcome this limitation by truncating Eq. \eqref{eq:tpppoisson_gaussiansource} at a certain distance according to a desired tolerance.
\subsection*{Basic Algorithm}
Eq. \eqref{eq:ttpoisson} can be easily solved in Fourier space by convolution with the Poisson's Greens function,
\begin{equation}
  \label{tppoisson_phihat}
 \hat\phi(\vec{k}) = \frac{\hat f(\vec{k})}{\varepsilon k^2}.
\end{equation}
We can reuse the methodological machinery devised for the \gls{FCM} (see chapter \ref{sec:fcm}) identifying $\fou{\oper{G}} := \frac{1}{k^2}$ as the Green's function and interpreting the viscosity as the permittivity. Naturally, in the current case fluctuations are not present.

The electric field can be derived from the potential in fourier space via \emph{ik} differentiation,
\begin{equation}
    \label{tppoisson_ehat}
  \hat{\vec{E}} = i\vec{k}\hat{\phi}.
\end{equation}

Similarly to the section on \gls{FCM}, Eq. \eqref{tppoisson_phihat} can be discretized using 3D \gls{FFT} in a grid with spacing fine enough to resolve the Gaussian charges in Eq. \eqref{eq:tppoisson_cdens}.

The whole algorithm, going from particle charges to forces, can be summarized as follows
\begin{enumerate}
\item Spread charges to the grid, $f=\oper{S}\vec{Q}$.
\item Fourier transform $\hat{f} = \mathfrak{F}f$.
\item Multiply by the Poisson's Greens function to obtain the potential, $\fou{\phi} = \frac{\hat{f}}{\varepsilon k^2}$.
\item Compute field via \emph{ik} differentiation, $\fou{\vec{E}} = i\vec{k}\fou\phi$.
\item Transform potential and field back to real space $\phi = \mathfrak{F}^{-1}\fou\phi$; $\vec{E} = \mathfrak{F}^{-1}\fou{\vec{E}}$.
\item Interpolate energy and/or force to charge locations, $\phi_i = \oper{J}\phi$; $\vec{E}_i = \oper{J}_{\vec{\ppos}_i}\vec{E}$.
\end{enumerate}
%Using operator notation
%\begin{equation}
%  \label{eq:tppoison_alg}
%  \vec{F}_i = Q_i\oper{J}_{\vec{\ppos}}\textrm{FFT}^{-1} \left[\frac{i\vec{K}}{\varepsilon K^2} \cdot \textrm{FFT}\left( \oper{S} Q\right)\right]
%\end{equation}
%Where $\textrm{FFT}$ represents the Fourier transform, $\vec{K}$ is the tensor with of all wave vectors (being $K$ their modulus).
%\begin{equation}
%  \label{eq:tppoison_alg_u}
%  U_i = q_iJ_iF^{-1} \frac{1}{\epsilon K^2}F Sq
%\end{equation}
\section{Ewald splitting}\label{sec:tppoisson_ewald}
The main problem with the approach in the previous section is that the grid needs to be fine enough to correctly describe the Gaussian sources. Thus a small width results in a high number of grid cells. This hinders the ability to simulate large domains (in terms of $a$) or narrow (or even point) sources. In order to overcome this limitation we use an Ewald splitting technique~\cite{Tornberg2015} (reminiscent of the use case introduced with \gls{PSE} in chapter \ref{sec:pse}).

We can write the potential as
\begin{equation}
 \phi=(\phi - \gamma^{1/2}\star\psi) + \gamma^{1/2}\star\psi = \phi^{\near} + \phi^{\far},
\end{equation}
where $\star$ represents convolution and the intermediate solution $\psi$ satisfies
 \begin{equation}
 \varepsilon\Delta\psi=-f\star\gamma^{1/2}.
\end{equation}   
The splitting function $\gamma$ is defined as
 \begin{equation}
 \gamma^{1/2} = \frac{8\xi^3}{(2\pi)^{3/2}}\exp\left(-2r^2\xi^2\right).
\end{equation}
Here the splitting parameter, $\xi$, is an arbitrary factor that is chosen to optimize performance. 
Given that the Laplacian commutes with the convolution we can divide the problem in two separate parts, denoted as near and far field  
 \begin{equation}
 \varepsilon\Delta\phi^{\far}=-f\star\gamma,
\end{equation}   
\begin{equation}
 \label{tppoisson_ewald_near}
 \varepsilon\Delta\phi^{\near}=-f\star(1-\gamma).
\end{equation}   
The convolution of two Gaussians is also a Gaussian, so in the case of the far field the RHS results in wider Gaussian sources that can be interpreted as smeared versions of the original ones. The far field RHS thus decays exponentially in Fourier space and is solved as in the non Ewald split case by effectively modifying the width of the Gaussian sources from $a$ to
\begin{equation}
  g_t = \sqrt{\frac{1}{4\xi^2} + a^2}.
\end{equation}
Notice that this overcomes the difficulties for the case of point sources, since we can arbitrarily increase $\xi$ to work with an arbitrarily large Gaussian source.

In contrast the near field effective charges are sharply peaked and narrower than the originals, rapidly decaying to zero. We can compute the Green's function for Eq. \eqref{tppoisson_ewald_near} analytically by convolving the original Poisson Green's function with the effective charges in Eq. \eqref{tppoisson_ewald_near} in Fourier space, yielding
\begin{equation}
  \hat{\oper{G}}^{\near}(k) = \frac{(1-\hat{\gamma})\hat{f}}{\varepsilon k^2}.
\end{equation}
The inverse transform of this kernel gives us the potential at any point of the grid as
\begin{equation}
  \phi^{\near}(\vec{\fpos}) = \sum_i{Q_i\oper{G}^{\near}(||\vec{\fpos}-\vec{\ppos}_i||)}.
\end{equation}
However, we are only interested in the potential averaged at the charges locations. Instead of storing and computing the potential in a grid and then interpolating as in the far field we can compute this analytically. Given that $\oper{J} \phi^{\near} = f\star \phi^{\near}$ we can define a pre-interpolated interaction kernel as
\begin{equation}
  \hat{\oper{G}}_{\oper{J}}^{\near}(k) = \hat{f}\hat{\oper{G}}^{\near} = \frac{(1-\hat{\gamma})\hat{f}^2}{\varepsilon k^2}.
\end{equation}
This expression is radially symmetric, which allows to compute the inverse Fourier transform as
\begin{equation}
  \label{tppoisson_gnear}
  \begin{aligned}
    \oper{G}_{\oper{J}}^{\near}(r) &= \frac{1}{2\pi^2r}\int{\hat{\oper{G}}^{\near}_{\oper{J}}k \sin(kr)dk} \\
            &= \frac{1}{4\pi\epsilon r}\left(\erf\left(\frac{r}{2a}\right) - \erf\left(\frac{r}{2g_t}\right)\right),
  \end{aligned}
\end{equation}
which decays exponentially and can be truncated at a certain cut off radius $r_c$. Furthermore this expression requires special consideration at short distances where numerical issues could arise. In these cases the Taylor expansion of Eq. \eqref{tppoisson_gnear} can be used instead.
We can use this expression to compute the potential or energy at the charges locations as
\begin{equation}
  U_i = Q_i\oper{J}_{\vec{\ppos}_i}\phi^{\near}(\vec{\fpos}) = Q_i\sum_j{Q_j\oper{G}_{\oper{J}}^{\near}(||\vec{\ppos}_i - \vec{\ppos}_j||)}.
\end{equation}
Similarly we compute the electric field or force acting on each charge,
\begin{equation}
  \vec{F}_i = Q_i \oper{J}_{\vec{\ppos}_i} \vec{E}^{\near} = Q_i\sum_j{Q_j\partial_r G_{\oper{J}}^{\near}({r_{ij}})\frac{\vec{r}_{ij}}{r_{ij}}},
\end{equation}
where $\vec{r}_{ij} = \vec{\ppos}_i - \vec{\ppos}_j$ and $r_{ij} = ||\vec{r}_{ij}||$. The derivative of Eq. \eqref{tppoisson_gnear} can be computed analytically \cite{Maxian2021}.
\subsection*{Accuracy}
There are several parameters than can be tweaked to control the overall accuracy of the algorithm:
The grid cell size $h:=L/n$ (being $L$ the box size and $n$ the number of grid cells\footnote{A cubic box is considered, but the arguments can be extended easily to any domain dimensions.}) controls how fine the Gaussian sources are described (providing a cut off wave number for the fourier description of the Poisson's Green's function). Empirically I have found that $h = \left[1.3 - \text{min}\left(-0.1\log_{10}(\epsilon), 0.9\right)\right]g_t$ ensures enough accuracy to ensure a relative error below the desired tolerance, $\epsilon$.
Note that $n$ should be chosen to be an \gls{FFT}-friendly number (see Appendix \ref{ch:appendixa}).

On the other hand, we also truncate the Gaussian kernel, $\phi_G$, up to a certain distance, $r_c$, such that $\phi_G(r_c)< \epsilon\phi_G(0)$.

A cut off distance is also required for the near field, where we choose $r_c^{nf}$ such that $\oper{G}_{\oper{J}}^{\near}(r_c) < \epsilon\oper{G}_{\oper{J}}^{\near}(0)$.


\subsection*{Use in UAMMD}

This algorithm is exposed in \uammd via the \emph{SpectralEwaldPoisson} \emph{Interactor} module.

\begin{code2}[Usage example of the triply periodic Poisson module.]{label=code:poisson}
#include<uammd.cuh>
#include<Interactor/SpectralEwaldPoisson.cuh>
using namespace uammd;
//Creates and returns a triply periodic Poisson solver Interactor
auto createTPPoissonInteractor(UAMMD sim){
  Poisson::Parameters par;
  par.box = sim.par.box;
  //Permittivity
  par.epsilon = sim.par.epsilon;
  //Gaussian width of the sources
  par.gw = sim.par.gw; 
  //Overall tolerance of the algorithm
  par.tolerance = sim.par.tolerance;
  //If a splitting parameter is passed
  // the code will run in Ewald split mode
  //Otherwise, the non Ewald version will be used
  //par.split = 1.0;
  return std::make_shared<Poisson>(sim.pd, par);
}
\end{code2}
The tolerance parameter is the maximum relative error allowed in the potential for two charges. The potential for $L\rightarrow\infty$ is extrapolated and compared with the analytical solution. Also in Ewald split mode the relative error between two different splits is less than the tolerance.

\chapter{Doubly Periodic Electrostatics}\label{ch:dppoisson}
\newcommand{\botl}{{0}}
\newcommand{\topl}{{H}}

%\begin{itemize}
%\item Introduce the need for slit channel electrostatics
%\end{itemize}
We present a novel algorithm for computing the electrostatic energies and forces for a collection of charges in a doubly-periodic environment (a slab). Our algorithm can account for arbitrary dielectric jumps across the boundaries of the slab and an arbitrary distribution of surface charge at the domain walls (in the open direction).

\begin{figure}[h]
  \centering
  \includesvg[width=\columnwidth]{gfx/dppoisson_sketch}
  \caption[ ]{Schematic representation of the doubly periodic domain described by Eqs. \ref{eq:dppoisson} and \ref{eq:dppoissonbcs1}-\ref{eq:dppoissonbcs4}. Each domain wall is represented with a different color. Blue clouds represent Gaussian charge sources.}
  \label{fig:dppoisson_sketch}
\end{figure}

%
%Doubly periodic means periodic in XY and unbounded in Z, with the possibility of placing surface charges \sigma_(b/t) at the domain limits and having different permittivities below, inside and above the domain.  
%A triply periodic solver is also available in UAMMD. See [3] for more info.
%### Algorithm summary  
A complete description of the algorithm can be found in ~\cite{Maxian2021}.
We model charges as Gaussian sources (as opposed to point charges). Even so our algorithm provides spectral accuracy via Ewald splitting, which naturally decouples the width of the sources and the grid size. This allows us to use an arbitrarily narrow Gaussian to simulate point charges if needed. By using Gaussian charges we avoid divergent self-interactions while more closely modelling electrolyte solutions, in which solvated ions are not point charges but rather have a certain characteristic charge size that we can mimic via the Gaussian width.
We want to solve the Poisson equation with Gaussian charge sources in a doubly periodic domain of width $H$ and size $L_{xy}$ in the plane. In particular, we seek to solve
\begin{equation}\label{eq:dppoisson}
 \varepsilon_0\Delta\phi(\vec{x} = (x,y,z))=-f(\vec{x})=-\oper{S}(\vec{x})\vec{Q},
\end{equation}
where 
\begin{equation}\label{eq:dppoissongauss}
 \oper{S}(\vec{x})\vec{Q} = \sum_iQ_i\left(2\pi a^2\right)^{-3/2}\exp\left(-\frac{||\vec{\ppos}_i-\vec{x}||^2}{2a^2}\right),
\end{equation}
where $a$ is the width of the Gaussian charges with strength $Q_i$\footnote{In \uammd's implementation, the permittivity inside the domain is set to unity and the charges are reescaled as $\vec{Q}' = \vec{Q}/\varepsilon_0$.}.
We impose that the sources do not overlap the boundaries in the $z$ direction, $f(z>\topl \text{ or } z<\botl) = 0$, so that the charge density integrates to one inside the slab. Given that the Gaussian is not compactly supported we truncate it at $n_\sigma a \ge 4 a$ to overcome this, ensuring that the integral is at least $99.9\%$ of the charge $Q$.

Finally, we solve \eqref{eq:dppoisson} with the following set of \bcs for the potential
\begin{equation}\phi(x,y,z\rightarrow \botl^+)=\phi(x,y,z\rightarrow \botl^-)\label{eq:dppoissonbcs1}\end{equation}  
\begin{equation}\phi(x,y,z\rightarrow \topl^-)=\phi(x,y,z\rightarrow \topl^+)\label{eq:dppoissonbcs2}\end{equation}
And for the electric field 
\begin{equation}\varepsilon_0 \frac{\partial \phi}{\partial z}(x,y,z\rightarrow \botl^+)-\varepsilon_b \frac{\partial \phi}{\partial z}(x,y,z\rightarrow \botl^-)=-\sigma_b(x,y)\label{eq:dppoissonbcs3}\end{equation}  
\begin{equation}\varepsilon_0 \frac{\partial \phi}{\partial z}(x,y,z\rightarrow \topl^-)-\varepsilon_t \frac{\partial \phi}{\partial z}(x,y,z\rightarrow \topl^+)=\sigma_t(x,y)\label{eq:dppoissonbcs4}\end{equation} 
We introduce, via these \bcs, the possibility of having arbitrary surface charges at the walls, $\sigma_b$ and $\sigma_t$ for the bottom and top respectively. Additionally, we can set different permittivities inside the slab ($\varepsilon_0$) above ($\varepsilon_t$) and below ($\varepsilon_b$) it. See figure \ref{fig:dppoisson_sketch} for a representation of the described set up.
Finally, we assume that the domain is overall electroneutral,
\begin{equation}
  \label{eq:dppoisson_electroneutral}
  \sum_{k=1}^N{Q_k} + \int_0^{L_{xy}}{\int_0^{L_{xy}}{(\sigma_b(x,y) + \sigma_t(x,y))dx dy}} = 0.
\end{equation}
Similarly to the triply periodic algorithm described in chapter \ref{ch:tppoisson}, we are interested in computing the system energy and the particle forces.
Once the potential is known the total electrostatic energy (inside the simulation domain) can be computed as
\begin{equation}
  \label{eq:dppu}
  U = \frac{1}{2}\sum_i{Q_i\oper{J}_{\vec{q}_i}\phi} + \frac{1}{2}\sum_{wall=b,t}{\int_{x,y}{\sigma_{wall}\phi(x,y,wall)dx dy}}
\end{equation}
Where $\oper{J}$ represents the interpolation operator as in chapter \ref{ch:tppoisson}.
In the same way as with the triply periodic algorithm, we can compute the electric field using Eq. \eqref{eq:tppoisson_fieldnablaphi} and interpolate it to the particles positions with Eq. \eqref{tppoisson_avgfield}. Finally, we compute the force acting on each particle with Eq. \eqref{eq:tppoisson_force}.

Since we do not have periodic boundaries in the $z$ direction, the application of the nabla operator on the potential to compute the electric field is not as straightforward as in the triply periodic case. In particular, as we will study in the following sections, our solver works in the Fourier-Chebyshev space (similar to the doubly periodic Stokes solver in chapter \ref{sec:dpstokes}).

\section{Solver description}
We use a grid-based solver to get an algorithm with linear complexity with the number of particles. We evaluate the \rhs of \eqref{eq:dppoisson} in a grid by spreading the charges (as in Chapter \ref{ch:tppoisson}), then solve the potential on that grid and finally interpolate the required quantities back to the particle positions (i.e the energy in Eq. \eqref{eq:dppu} or the force with Eq. \eqref{eq:tppoisson_force}).
In order to accurately describe the charge density $f(\vec{x})$ in the grid we must choose a sufficiently small grid size, $h$. Since the correct description of the charge density field produced by the Gaussian sources requires $h \sim a$ this method will be inefficient for point-like charges ($a\rightarrow 0$). To overcome this limitation (similarly to the triply periodic case described in chapter \ref{ch:tppoisson}) we make use of Ewald splitting.

Let us start with the non-Ewald split case by considering that the distance between charges is comparable to their width, $a$.

In a move reminiscent of the \gls{DP} Stokes algorithm in Chapter \ref{sec:dpstokes} we start by separating the problem into two sub-problems. First we solve \eqref{eq:dppoisson} in free space (no dielectric jumps or surface charges) and then introduce a harmonic correction to account for the jump \bcs (section \ref{sec:dpcorr}). In particular, we separate the potential as
\begin{equation}
  \label{eq:dppoissonpotsep}
  \phi = \phi_{\dpr} + \phi_{\corr}
\end{equation}


We first find the free-space potential, $\phi_{\dpr}$, by solving the Poisson equation \eqref{eq:dppoisson} with free-space \bcs and uniform permittivity $\varepsilon_0$ (see Eq. \eqref{eq:dppoissonfreespacepot}). Then we compute the correction, $\phi_{\corr}$, by solving a Laplace equation that satisfies the \bcs in Eqs. \eqref{eq:dppoissonbcs1}-\eqref{eq:dppoissonbcs4}.
Notice that on the one hand the free-space potential is oblivious to the presence of the surface charged walls and, on the other, the harmonic correction does not include the Gaussian sources in its formulation. This means that each separate problem is not necessarily electroneutral, we address electroneutrality further on in section \ref{sec:dpelectroneutral}.

\subsection{Free space solver, $\phi_{\dpr}$}\label{sec:dpsolver}
Let us start by describing the free-space solver. We want to solve
\begin{equation}
  \label{eq:dppoissonfreespacepot}
  \varepsilon_0 \Delta \phi_{\dpr}(\vec{\fpos}) = -f(\vec{\fpos}) \quad \text{for}\quad \botl<z<\topl
\end{equation}
bounded at infinity so that $\partial_z\phi_{\dpr}(z \rightarrow \pm \infty) \rightarrow 0$ and with periodic boundary conditions in the plane.
We can use the fact that the source charges are contained inside the domain $z\in[\botl, \topl]$ to find the necessary \bcs to constitute a \gls{BVP}, which can then be solved by the method in Appendix \ref{sec:bvp} (first introduced in chapter \ref{sec:dpstokes} for the Stokes equation). In particular, we can write
\begin{equation}
  \varepsilon_0 \Delta \phi_{\dpr} = 0\quad\text{for}\quad z\le\botl \quad\text{or}\quad z\ge \topl.
\end{equation}
this equation can be solved analytically by taking its Fourier transform in the plane, so that for each wavenumber we have
\begin{equation}
  (\partial_z^2 - k^2)\fou{\phi}_{\dpr}(\vec{k}, z) = 0.
\end{equation}
Which has an analytical solution
\begin{equation}
  \fou{\phi}_{\dpr}(\vec{k}, z \le \botl) \propto \exp(kz)\quad\text{and}\quad\fou{\phi}_{\dpr}(\vec{k}, z \ge \topl) \propto \exp(-kz),
\end{equation}
hinting at the following \bcs for Eq. \eqref{eq:dppoissonfreespacepot},
\begin{equation}
  \label{eq:dppoissonfreespacebcs}
  (\partial_z + k)\fou{\phi}_{\dpr}(\vec{k}, \topl) = 0\quad\text{and}\quad(\partial_z - k)\fou{\phi}_{\dpr}(\vec{k}, \botl) = 0, 
\end{equation}
owing to the continuity of $\phi_{\dpr}$ and $\partial_z\phi_{\dpr}$ at the boundaries.
We can now write Eq. \eqref{eq:dppoissonfreespacepot} in Fourier space in the plane and couple it with the \bcs in Eq. \eqref{eq:dppoissonfreespacebcs} to constitute a two point \gls{BVP} that can be efficiently solved in the \gpu with the solver described in Appendix \ref{sec:bvp}. We can thus rewrite Eq. \eqref{eq:dppoissonfreespacepot} as
\begin{equation}
  \label{eq:dppoissonfreespacepotbvp}
  \varepsilon_0(\partial_z^2 - k^2)\fou{\phi}_{\dpr}(\vec{k}, z) = -\fou{f}(\vec{k}, z) \quad \text{for}\quad \botl\le z\le \topl.
\end{equation}
The $k=0$ mode requires special treatment, since the \gls{BVP} in Eqs. \eqref{eq:dppoissonfreespacepotbvp} with \bcs in Eq. \eqref{eq:dppoissonfreespacebcs} is not necessarily electroneutral (since it does not consider the surface charges), we will consider it later along with the correction (given that the problem for $\phi$ is indeed electroneutral).
\subsection{Harmonic correction, $\phi_{\corr}$}\label{sec:dpcorr}
We now correct the solution $\phi_{\dpr}$, for which we did not take into account the \bcs in Eqs. \eqref{eq:dppoissonbcs1}-\eqref{eq:dppoissonbcs4}, mainly neglecting the surface charges $\sigma_{b/t}$. In contrast, the correction potential does not require taking into account the Gaussian sources, which were included in $\phi_{\dpr}$. Thus, the correction will have a Laplace equation
\begin{equation}
  \label{eq:dppoissonlaplacecorr}
  \Delta \phi_{\corr} = 0
\end{equation}
for all space, $z\in(-\infty, \infty)$. For the \bcs we use the four jump conditions at the domain limits (two for the potential and two for the field). In general, we will have a mismatch at the boundaries in both the potential and the field given by
\begin{equation}
  \label{eq:dppoissonmismatchcorr}
  \begin{aligned}
    &\phi_{\corr}(x,y,z\rightarrow \botl^+) - \phi_{\corr}(x,y,z\rightarrow \botl^-) = -m_{\phi}^{b}(x,y),\\
    &\phi_{\corr}(x,y,z\rightarrow \topl^-) - \phi_{\corr}(x,y,z\rightarrow \topl^+) = -m_{\phi}^{t}(x,y),\\
    &\varepsilon_0\partial_z\phi_{\corr}(x,y,z\rightarrow \botl^+) - \varepsilon_{b}\partial_z\phi_{\corr}(x,y,z\rightarrow \botl^-) = -m_E^{b}(x,y)\\
    &\varepsilon_0\partial_z\phi_{\corr}(x,y,z\rightarrow \topl^-) - \varepsilon_{t}\partial_z\phi_{\corr}(x,y,z\rightarrow \topl^+) = -m_E^{t}(x,y).
  \end{aligned}
\end{equation}
Using $\phi = \phi_{\dpr} + \phi_{\corr}$ we can write them in terms of the already computed $\phi_{\dpr}$, 
\begin{equation}
  \begin{aligned}
    &m_{\phi}^{b} = \phi_{\dpr}(z\rightarrow \botl^+) - \phi_{\dpr}(z\rightarrow \botl^-) = 0,\\
    &m_{\phi}^{t} = \phi_{\dpr}(z\rightarrow \topl^-) - \phi_{\dpr}(z\rightarrow \topl^+) = 0,\\
    &m_E^{b} = (\varepsilon_0 - \varepsilon_{b})\partial_z\phi_{\dpr}(z=\botl) + \sigma_{b},\\
    &m_E^{t} = (\varepsilon_0 - \varepsilon_{t})\partial_z\phi_{\dpr}(z=\topl) - \sigma_{t}.
  \end{aligned}
\end{equation}
Where we have used the continuity of $\phi_{\dpr}$ and its derivative at the domain limits to simplify the expressions for the field mismatch. Additionally, the potential mismatch is zero in the current case, nonetheless we keep it in the description, since it will become non-zero in the Ewald split case (see section \ref{sec:dpewald}).

We can write the general solution for the correction potential inside the domain in Fourier space (as usual, transformed only in the plane) as 
\begin{equation}
  \label{eq:dppoissoncorrpoteq}
  \fou{\phi}_{\corr}(\vec{k}, z) = A(\vec{k})\exp(k z) + B(\vec{k})\exp(-kz), \quad \text{for} \quad \botl\le z\le \topl
\end{equation}

Using the \bcs in Eq. \eqref{eq:dppoissonmismatchcorr} it can be shown that the solution is

\begin{equation}
  \label{eq:dppoissoncorrpot}
  \begin{aligned}
    \fou{\phi}_{\corr}(\vec{k}, z) = \alpha(k)\bigg[ &(\varepsilon_t +1)\exp(-k z)(\fou{m}_E^b - \varepsilon_bk \fou{m}_\phi^b)\\
    &-(\varepsilon_b+1)\exp\left(k\left(z-H\right)\right)(\fou{m}_E^t + \varepsilon_tk\fou{m}_\phi^t)\\
    &+(\varepsilon_b-1)\exp\left(-k(H+z)\right)(\varepsilon_tk\fou{m}_\phi^t + \fou{m}_E^t)\\
    &-(\varepsilon_t -1)\exp\left(k(z-2H)\right)(\fou{m}_E^b - \varepsilon_bk\fou{m}_\phi^b)
    \bigg].
\end{aligned}
\end{equation}
Where we have defined for convenience the helper function, $\alpha(k)$, as
\begin{equation}
  \alpha(k) := \left(k(\varepsilon_b +1)(\varepsilon_t+1) - (\varepsilon_b\varepsilon_t + \varepsilon_b + \varepsilon_t -1)\exp(-2Hk) \right)^{-1}
\end{equation}
The expression for the correction potential in Eq. \eqref{eq:dppoissoncorrpot} has been carefully laid out to minimize overflow errors in a numerical implementation (due to the evaluation of the exponential terms). In particular, \uammd's implementation evaluates this expression in double precision regardless of the overall precision mode, since the standard math library allows for a maximum exponent of $~709$. Exponential overflow is not an issue in the non-Ewald split case, but as we will see in the following sections, the Ewald-split case requires evaluating Eq. \eqref{eq:dppoissoncorrpot} below $z=-H_e$, which results in the term $\exp(kH_e)$ being evaluated.

The only missing piece in our description is the $k=0$ mode, which must be computed for directly for $\phi$, since this is the only way to ensure the solution is well defined (which requires electroneutrality, i.e the integral of the overall charge being zero).
 
\subsection{The $k=0$ mode}\label{sec:dpelectroneutral}
For the zeroth mode we can write the equation for the uncorrected potential as
\begin{equation}
  \label{eq:dppoissonk0}
  \partial_z ^2\fou{\phi}_{\dpr}(\vec{k}=\vec{0}, z) = -  \varepsilon_0^{-1}\fou{f}(\vec{0}, z).
\end{equation}
%With \bcs in Eqs. \eqref{eq:dppoissonbcs1}-\eqref{eq:dppoissonbcs4}.
%Using the boundness of $\fou{\phi}$ at infinity we can safely assume that the potential $\fou\{phi}(\vec{0}, z)$ remains constant outside the boundaries and given that we can choose $\fou{\phi}(\vec{0},z)$ at one place, we set
%\begin{equation}
%  \fou{\phi}(\vec{0}, z\ge H) = A_0\quad\text{and}\quad\fou{\phi}(\vec{0}, z\le -H) = -A_0
%\end{equation}
%The constant $A_0$ can be evaluated by multiplying Eq. \eqref{eq:dppoissonk0} by $z$ and integrating by parts,
%\begin{equation}
%  \int_{-H}^H
%\end{equation}
Which can be solved by integrating the zeroth mode of the charge density twice, making use of the Chebyshev coefficients of $\fou{f}$. Alternatively, our \gls{BVP} solver can handle Eq. \eqref{eq:dppoissonfreespacepotbvp} with the (homogeneous) \bcs in Eq. \eqref{eq:dppoissonfreespacebcs} in the case $k=0$\footnote{Which is how \uammd computes the linear mode.}.
The potential in Eq. \eqref{eq:dppoissonk0} is only valid up to a linear correction, given by the solution of Eq. \eqref{eq:dppoissoncorrpoteq} for $k=0$,
\begin{equation}
  \fou{\phi}_{\corr}(\vec{0}, \botl\le z\le \topl) = A_0z + B_0.
\end{equation}
Let us also define the correction potential outside the boundaries as
\begin{equation}
  \fou{\phi}_{\corr}(\vec{0}, z> \topl) = A^t_0z + B^t_0 \quad\text{and}\quad \fou{\phi}_{\corr}(\vec{0}, z<\botl) = A^b_0z + B^b_0.
\end{equation}
The final potential, $\phi(\vec{0},z) = \phi_{\dpr}(\vec{0},z) + \phi_{\corr}(\vec{0},z)$ must satisfy the \bcs in Eqs. \eqref{eq:dppoissonbcs1}-\eqref{eq:dppoissonbcs4}, replacing the equations for the partial potentials and using the mismatches yields a system of equations for the unknown coefficients as
\begin{equation}
  \label{eq:dppoissonB0}
  \fou{\phi}_{\dpr}(\vec{0}, \botl) + B_0 = B_0^b = 0,
\end{equation}
\begin{equation}
  A_0H + B_0 + \fou{\phi}_{\dpr}(\vec{0}, \topl) = A_0^tH + B_0^t,
\end{equation}
\begin{equation}
  m_E^b +\varepsilon_0A_0-\varepsilon_bA_0^b = 0,
\end{equation}
\begin{equation}
  m_E^t +\varepsilon_0A_0-\varepsilon_tA_0^t = 0.
\end{equation}
We only need the values of $A_0$ and $B_0$ in order to compute $\fou{\phi}(\vec{0},z)$ inside the slab. We can obtain $B_0$ from Eq. \eqref{eq:dppoissonB0}. For $A_0$ we can use the decay of the field outside the slab, which requires that the correction potentials outside the boundaries cancel the one produced by $\fou{\phi}_{\dpr}$, so
\begin{equation}
  A_0^b = -\partial_z\fou{\phi}_{\dpr}(\vec{0}, z\le\botl),\quad\text{and}\quad A_0^t = -\partial_z\fou{\phi}_{\dpr}(\vec{0}, z\ge\topl).
\end{equation}
Using the above expressions we can arrive at two solutions, which are identical for electroneutral slabs, for $A_0$,
\begin{equation}
  \begin{aligned}
    A_0 &= -\varepsilon_0^{-1}(m_E^b +\varepsilon_b\partial_z\fou{\phi}_{\dpr}(\vec{0}, \botl)) =\\
    &= -\varepsilon_0^{-1}(m_E^t +\varepsilon_t\partial_z\fou{\phi}_{\dpr}(\vec{0}, \topl)).
\end{aligned}
\end{equation}
In \uammd's implementation we average both expressions to minimize the discretization errors\footnote{In \uammd, the code for the zeroth mode correction can be found in the mismatch compute source file, \emph{PoissonSlab/MismatchCompute.cuh}.}

%On the other hand, the equation for the correction potential (see Eq. \eqref{eq:dppoissonlaplacecorr}) with $k=0$ is
%\begin{equation}
%  \phi_{\corr}(\vec{0}, z) = A_0z+B_0
%\end{equation}
%
%
%The final solution for the linear mode can be written as
%\begin{equation}
%  \phi(\vec{0}, z) = \phi_{\dpr}(\vec{0}, z) + Cz
%\end{equation}
%

\subsection{Wrapping up}
Once the correction is evaluated for all wavenumbers (except the zeroth mode, which as we have seen is handled independently), we apply the \gls{FCT} to each one, obtaining the Chebyshev coefficients that can then be summed to the uncorrected solution, $\fou{\phi}_{\dpr}$ to finally get the total potential, $\phi$, via Eq. \eqref{eq:dppoissonpotsep}.

In order to compute the electric field we can use \emph{ik} differentiation in the plane directions and use the Chebyshev coefficients in z for each wavenumber to compute its derivative (see Appendix \ref{sec:bvp}).

Then we apply the inverse Fourier-Chebyshev transform to get the potential and/or field, finally interpolating to the charges positions (see Eq. \eqref{tppoisson_avgfield} and \eqref{tppoisson_avgpot}).


\section{Ewald splitting}\label{sec:dpewald}
In order to reuse the Ewald splitting machinery laid out for the triply periodic case (see chapter \ref{ch:tppoisson}) we must shift our approach and describe the problem using image charges instead of boundary conditions.
\subsection*{The method of images}
In the presence of a single wall with a jump in permittivity (for instance, the bottom wall at $z=0$, below of which the permittivity is $\varepsilon_b$), the method of images  \cite{Griffiths2014} states that the potential (and field) created by a charge, $q$, at the location, $(x,y,z)$, is equivalent to that of a medium with uniform permittivity, $\varepsilon_0$, containing the original charge along with an image charge reflected across the wall, $(x,y,-z)$, with charge
\begin{equation}
  q^* = -q\frac{\varepsilon_b - \varepsilon_0}{\varepsilon_b + \varepsilon_0}.
\end{equation}
Additionally, the potential and field at the position of the image charge, $(x,y,-z)$, is equivalent to the medium having permittivity $\varepsilon_0$ in the presence of a single charge, located at $(x,y,z)$ with charge
\begin{equation}
  q^{*}_{\text{img}} = q\frac{2\varepsilon_0}{\varepsilon_b + \varepsilon_0}.
\end{equation}
With two walls the situation becomes much more complex, since the image charges further reflect through the opposite wall (see Fig. \ref{fig:dppoisson_images}) and so on and so forth, resulting in infinite images.
\begin{figure}
  \centering
  \includesvg[width=\columnwidth]{gfx/dppoisson_images}
  \caption[ ]{Enforcing the dielectric jumps using images. The presence of the two dielectric boundaries (at $z=\botl$ and $z=\topl$) causes each charge to have infinite reflections. For instance, the charge $q$ reflects through the bottom with an effective charge $q^*$, which in turn reflects again through the top wall with $q^{**}$. We manage to Ewald split the problem into a near and far field contributions in such a way that both parts only need to take into account the first reflections at most.}
  \label{fig:dppoisson_images}
\end{figure}
The image construction enforces the \bcs in Eqs. \eqref{eq:dppoissonbcs1}-\eqref{eq:dppoissonbcs4} without surface charges (which we will introduce later) even when the particles are Gaussian clouds instead of point-like charges.
In the following discussion, we will see how to deal with images when we split the problem into a near and far field contributions (using the Ewald-splitting tools in chapter \ref{ch:tppoisson}).

\subsection{Near field}
As discussed in sec \ref{sec:tppoisson_ewald}, the range of the near field Green's function is related to the splitting parameter, $\xi$, and we can truncate it at a certain distance, $r_{c}$, in order to meet a certain tolerance requirement. We can take advantage of this and truncate $G_{\oper{J}}^{\near}$ to limit the near field computation to just the first set of images above or below the walls. We can achieve this by imposing $r_c<H$, we further restrict $r_c < L_{xy}/2$ in order to apply the \gls{MIC} in the plane.

Besides having to include the first images, this part of the algorithm is then identical to the triply periodic case in chapter \ref{sec:tppoisson_ewald}.\footnote{In \uammd's implementation, the near field images are handled via a special \emph{Transverser} that checks, for a given neighbour, its position and that of its two images (top and bottom wall). The related code can be found at the file \emph{Interactor/DoublyPeriodic/PoissonSlab/NearField.cuh}.}.
\subsection{Far field}

The treatment for the far field is quite convoluted due to the requirement of solving the potential outside and inside the domain while taking into account the image charges in a spectrally accurate manner. The algorithm for the far field is carefully laid out in ~\cite{Maxian2021}.

\section{How to use in UAMMD}
The creation of the Doubly Periodic Poisson Interactor is similar to that of the triply periodic case. With the exception that now the box size is communicated separately in the parallel and perpendicular directions and the permittivity can be different inside and outside the domain. Besides the parameters in the source code example \ref{code:dppoisson}, additional ones are available to fine-tune several internal precision parameters (such as support, upsampling or overall tolerance). By default, the module will provide an overall tolerance of around 4 digits, which is the study case in the original work describing the doubly periodic algorithm~\cite{Maxian2021}. Additionally, a special functor can be provided specifying the surface charges. The description of the surface charge parameter is left for UAMMD's online documentation (see Appendix \ref{ch:online}).
In all instances, the surface charge will enforce overall electroneutrality inside the domain. For instance, if a single positive charge of strength $Q$ is located inside the domain, each wall will be assigned a constant charge of $-Q/2$.

\begin{code2}[Usage example of the doubly periodic Poisson module.]{label=code:dppoisson}
#include<Interactor/DoublyPeriodic/DPPoissonSlab.cuh>

auto createDPPoissonInteractor(UAMMD sim){  
  DPPoissonSlab::Parameters par;
  par.Lxy = sim.par.Lxy;
  par.H = sim.par.H; //Domain height
  DPPoissonSlab::Permitivity perm;
  perm.inside = sim.par.permInside;
  perm.top = sim.par.permTop;
  perm.bottom = sim.par.permBottom;
  par.permitivity = perm;
  par.gw = sim.par.gw; //Width of the Gaussian sources
  par.split = gw*0.1; //Splitting parameter
  auto poisson = make_shared<DPPoissonSlab>(sim.pd, par);
  return poisson;
}
\end{code2}

\newpage
\cleardoublepage
%\ctparttext{Collection of tools and algorithms useful in soft matter simulations.}
%\part{GPU enabled post processing}\label{pt:tools}
%
%\begin{itemize}
%\item The unreasonable convenience of an UNIX-like environment. The pipe as a connector between operations.
%\item The UNIX phylosophy of do one thing and one thing only.
%\end{itemize}
%
%\chapter{Radial Distribution Function}
%\chapter{Mean Square Displacement}
%\chapter{Fast Correlations}
%\chapter{Fast Histogram}
%\chapter{FFT}
%
%
%\chapter{Superpunto}
%\begin{figure}[H]
%  \label{fig:spunto}
%  \centering
%  \includegraphics[width=\textwidth]{gfx/shotlogo}
%  \caption[ ]{A screenshot of a \gls{LJ} fluid quenched to a low temperature made with superpunto. The simulation contains $10$ million particles, which are being rendered to the screen at about 30 frames per second in a GTX 980 \gpu.}
%\end{figure}
%
%\begin{itemize}
%\item Superpunto, a particle visualizator.
%\item Explain the need for a fast particle visualizer with a super simple format and pipe capabilities. Real time visualization using the pipe.
%  
%\end{itemize}
%
%\section{Format}
%inputfile should have the following structure:
%\begin{minted}{\ucpp}
%#Lx=X;Ly=Y;Lz=Z; Comments are used to separate frames, you can force the size of the simulation box starting the comment as in this example. All three L must be provided
%X1 Y1 Z1 r1 c1 Vx Vy Vz#You can comment here aswell! If your file has more than
%X2 ...         #8 columns, the rest will be ignored!
%.
%.
%.
%\# frame=2
%X1 Y1 Z1 r1 c1 Vx Vy Vz
%.
%.
%.
%\# frame = 3
%\end{minted}
%
%\section{Graphical Techniques}
%
%\begin{itemize}
%\item Instanced drawing with OpenGL 4.5
%\item The G-buffer technique
%\item Lighting model.
%\item Screen space ambient oclussion (SSAO).
%\end{itemize}
%
%\section{Controls}
%
%\begin{itemize}
%\item  Movement:
%  \begin{itemize}
%  \item Move with WASD, E and Q to tilt and Shift/Ctrl to go up/Down
%  \item Use +/- to increase/decrease the speed
%  \item Look around holding ALT and moving the mouse
%  \item Rotate the world in XYZ using 123/456    
%  \end{itemize}
%\item Frame control:
%  \begin{itemize}
%  \item Press Space to go to the next frame, R to the previous
%  \item Press T to go to the last frame, B takes you to the first one
%  \item Press M to play the frames at 60 FPS, M again to pause
%  \item Press C to take a screenshot in png
%  \item Press L to play and record to a mp4 until L is pressed again
%  \end{itemize}
%\end{itemize}
%\subsection*{Options}
%\begin{itemize}
%\item --record :  Makes a movie of all the frames in file and generates a .mp4
%\item --frames-between-screenshots X : Number of frames skipped between screenshots when recording (default = 2)
%\item --background R G B : Background color in RGB, default R=G=B=0.0
%\item --palette X : Change the color palette
%\item --RGB : Read colors as hex values in BGR (as integers) (0xFF=red=255). Overrides palette
%\item --nobox : Do not render the bounding box.
%\item --noaxis : Do not render axis labels.
%\item --resolution X Y : Set the resolution (aka window size). Default is 800x800          
%\end{itemize}
%
%
%\newpage
%\cleardoublepage
\ctparttext{Scientific publications using UAMMD.}
\part{New physics and applications}\label{pt:applications}

In this part we showcase the works, published in scientific journals, that resulted from the development of this thesis. We will see new physics and works in which \uammd played a fundamental role either as a simulation engine or accelerator.
This section does not intend to be self-contained (the readers are referred to the published material), but is rather a way to illustrate the capabilities of UAMMD in disparate physical scenarios.

Besides the works presented in this part of the manuscript, it is worth mentioning ~\cite{Maxian2021Fibers}, a recent publication in which the author of this manuscript appears as a collaborator, in which UAMMD acts as an external accelerator library. In particular, an already established, in-house code written in python by the author for simulations of dynamically cross-linked actin networks offloads the construction of a neighbour list to UAMMD (using the cell list). Thanks to the inclusion of UAMMD, the previous bottle-neck of their implementation became effectively instantaneous and allowed them to reach larger simulation times and systems.

\chapter{Measuring intracellular viscosity}

In ~\cite{Pelaez2019} we use \gls{PSE} (chapter \ref{sec:pse}) to model the environment of a cell. In particular, we study how the presence of microtubulae affects the viscosity measured by a nanorocker marker.
The experimental collaborators in ~\cite{Pelaez2019} were testing the effects of two drugs, colchicine and Taxol, on the internal structure of HeLa cells (which are $20$-$40\mu$m in diameter). These drugs are so-called tubulin interactors, meaning that they either promote the polymerization (Taxol) or depolymerization (colchicine) of microtubulae (i.e the cell's cytoskeleton).
In particular, local mesoscopic intracellular viscosity was being probed via a non-spherical upconverting nanorocker particle (a $\beta-NaYF_4$ hexagonal-shaped nanoparticle with a $400$nm thickness, see fig \ref{fig:cellvis}). The mean squared angular displacement (MSAD) of the nanorocker is sampled using a polarized spectroscopy technique that allows to track the orientation of the nanoparticle in real time~\cite{RodriguezSevilla2016}.
The orientational fluctuations (a.i. the MSAD) of the nanorocker can then be related directly to the local viscosity using the well-known relation for the time evolution of the MSAD for a disk-like particle,
\begin{equation}
  \label{eq:msad}
  \text{MSAD}(t) = \frac{\kT}{3\eta V f/f_0}t.
\end{equation}
Where $V$ is the volume of the particle and $f/f_0$ is the so-called Perrin's friction coefficient, which relates the friction of the non-spherical object with that of a sphere of equivalent volume. 
\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{gfx/hexagon}
  \caption[ ]{Representation of the simulation environment used to model the experimental setup in ~\cite{Pelaez2019}. The nanorocker (grey) and the microtubulae (red) are modeled as spring-connected blobs.}
  \label{fig:cellvis}
\end{figure}
Experiments with Taxol and colchicine revealed a strong and unintuitive relation between the concentration of microtubulae and the mesoscopic intracellular viscosity. In particular, experiments showed a ten-fold reduction in the viscosity after Taxol administration, corresponding to the presence of a higher concentration of microtubulae in the cellular environment. Consistently, the posterior administration of colchicine, which depolymerizes the microtubulae into a suspension of tubuline heterodimers, induces an increase in viscosity.

We infer the origin of the viscosity reduction lies in the specific sampling size of the nanorocker (with an average size of $400$nm). For a typical concentration of tubulin ($25$-$100\mu$M) in the cytoplasm the volume fraction occupied by the microtubulae is between $5$~$10^{-3}$ and $5$~$10^{-2}$. At these concentrations we find a polydisperse ensemble of rods that form a fractal gel with pores typically smaller than the average length of the rods. A tracer particle diffusing in this scenario is known to have a severely reduced translational and rotational diffusion~\cite{Solomon2010,Alcazar2018}. However, after Taxol administration, the microtubule network is formed by much larger rods, resulting in the nanorocker diffusing through much larger structures. Colchicine produces the opposite effect, reducing the typical rod size and, consequently, the average length of the cytoplasm components that drag the sampling particle. Our hypothesis relies on the expectation that, when the typical distance between microtubules in the ordered phase (after Taxol administration) becomes larger than the nanorocker size the steric hidrance of the microtubulae is significantly reduced. In order to validate this, we setup a series of simulation using the \gls{PSE} method (see chapter \ref{sec:pse}) with various concentration of microtubulae such that the average free space between rods is larger than the rocker's size (see figure \ref{fig:msad}).
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{gfx/MSDA_comparison_exp}
  \caption[ ]{Computed and experimentally measured mean squared angular displacement (MSAD) over time. Comparison is made between simulations of free (brown line) and laser-illuminated (confined, orange line) nanorocker, surrounded by 10 microtubules (MT), and the experimental results (Exp.) after 5 h incubation time with Taxol (purple data). Thered dashed line represents the MSAD calculated from Equation (1) using the measured viscosity for an incubation time of 5 h. The good agreement between the red and purple curves validates the approximation of the nanorocker to a disk. The inset shows the mean squared displacement (MSD) for free nanorockers in the presence of 0, 5, and 10 microtubules. The agreement between the experimental and simulation results indicates that the movement of the particle is not affected by the presence of microtubules since their separation is much bigger than the particle.}
  \label{fig:msad}
\end{figure}
We perform simulations with and without taking into account the translational confining effect of the probing laser on the nanorocker (by trapping the nanorocker with a soft potential well to a domain about 10\% larger than its size) and find no discernable effect on its rotational diffusion. In the confined case, any modification of the particle rotation is only due to the hydrodynamic effects of the microtubulae. This evidences that the translational confinement of the probing beam does not affect its orientational fluctuating motion.
Our simulations yield MSAD curves that match the theory and simulations in the absence of microtubulae to within statistical uncertainty. This evidences that the presence of microtubulae, at these concentrations, has practically no effect on the viscosity sampled by the nanorocker. From this, we conclude that, once all microtubulae are formed due to the effect of Taxol, there is less amount of intracellular material of comparable size to the nanorocker, which effectively reduces the viscosity sensed by it.

%Chemicals capable of producing structural and chemical changes on cells are used to treat diseases (e.g., cancer). Further development and optimization of chemotherapies require thorough knowledge of the effect of the chemical on the cellular structure and dynamics. This involves studying, in a noninvasive way, the properties of individual cells after drug administration. Intracellular viscosity is affected by chemical treatments and it can be reliably used to monitor chemotherapies at the cellular level. Here, cancer cell monitoring during chemotherapeutic treatments is demonstrated using intracellular allocated upconverting nanorockers. A simple analysis of the polarized visible emission of a single particle provides a real-time readout of its rocking dynamics that are directly correlated to the cytoplasmic viscosity. Numerical simulations and immunodetection are used to correlate the measured intracellular viscosity alterations to the changes produced in the cytoskeleton of cancer cells by anticancer drugs (colchicine and Taxol). This study evidences the possibility of monitoring cellular properties under an external chemical stimulus for the study and development of new treatments. Moreover, it provides the biomedical community with new tools to study intracellular dynamics and cell functioning.
%

\chapter{Star Polymer dynamics in shear flow}\label{ch:starpolymers}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{star}
  \caption[ ]{A star polymer solution under a shear flow.}
  \label{fig:starpic}
\end{figure}

In ~\cite{Pelaez2020} we use \gls{BDHI} via Cholesky (see chapter \ref{sec:chol}) to study the rheological properties of a low density solution of star polymers in shear flow.
We investigate tank-treading and breathing dynamics of individual star molecules under shear flow and their relation with the macromolecule architecture. Tank-treading consists of the rotation of the arms of the star around a molecule’s center, whereas breathing consists in expansions and contractions of the whole molecule at a certain characteristic frequency. We derive scaling arguments for the trends of the frequency and decorrelation rates of both rotation and breathing modes versus the shear rate $\dot{\gamma}$, which are supported by extensive Brownian hydrodynamics simulations. We find that breathing occurs if $\dot{\gamma}$ is made faster than the equilibrium decorrelation rate $\Gamma_0$ of two perpendicular eigenvectors of the gyration tensor. $\Gamma_0$ increases with the star functionality, $F$, as $\Gamma_0 \sim F^{0.5}$, which contrasts with the rotational and arm-length relaxation rates ($F^{-0.6}$ and $F^ {-0.15}$ respectively). For $\dot{\gamma} > \Gamma_0$, the star becomes ellipsoidal and elongates in the flow direction and this determines the onset of the non-Newtonian regime. Remarkably, $\dot{\gamma}/\Gamma_0$ provides universal trends for the shape, dynamics, and rheology of the star polymer.
%
%We present a theoretical study of the different rotation and relaxation modes that a star polymer in solution experiences when under shear flow. Star polymers have been extensively studied as, while being one natural step away from linear polymers, they present a variety of interesting properties not present on the former such as their unique rheology, applications in load delivery and distinctive topology. These kind of polymeric structures under shear flow present quite rich dynamics, with a series of very distinct rotation and relaxation modes that are often overlooked and/or misinterpreted in the literature. We focus on the origin of tank-treading (arm-rotation) and breathing (shape fluctuations) modes of star molecules, which have remained elusive in the literature. 
% We use arguments based on force balance and monomer kinetics to predict the frequency and decorrelation rates of the different modes as functions of the star polymer structure. Furthermore we study the several equilibrium relaxation times and provide universal scaling laws for them. We also find that the equilibrium relaxation time for the star’s anisotropic shape fluctuations determine the onset of breathing and provides universal scaling laws for the star orientation angle in flow. Our claims are backed by extensive Brownian hydrodynamics simulations, allowing us to also study how the strong hydrodynamic correlations between arms affect the different rotation dynamics.
%

\chapter{Hydrodynamics in confined geometries}

Let us start by discussing two works that study hydrodynamics in confined geometries. First in section \ref{sec:softconf} when particles are confined to the plane via a soft harmonic potential and then in section \ref{sec:strictconf} when the confinement is strict (in the quasi 2D regime introduced in chapter \ref{sec:q2D}).

\section{From soft to strict confinement}\label{sec:softconf}
In ~\cite{Pelaez2017} we study the hydrodynamics-enhanced collective diffusion of a group of colloids under soft two dimensional confinement as the confinement becomes stiff. Colloids are embedded in an unbounded fluid but their movement in the Z direction is constrained via a harmonic potential (see Fig. \ref{fig:q2dsch}) as we introduced in chapter \ref{sec:q2D}. As the spring constant of the confining (parabolic) potential is decreased, in this work we study the crossover between purely 3D hydrodynamics and quasi2D both numerically and theoretically.

It is well-known that, in quasi2D, collective diffusion is enhanced due to the effective compressibility of the in-plane hydrodynamics. This is evidenced in Eq. \eqref{eq:q2Deffrepulsion} (and related discussion), where we saw how the divergence of the in-plane mobility can be interpreted as the colloids interacting via a Coulomb-like repulsive potential in an incompressible fluid. In ~\cite{Pelaez2017} we find that the enhancement of the collective diffusion is quite robust and remains significant when moving from strict to soft confinement.

The enhancement, being collective in nature, decreases with the concentration of colloids. As an example, for a projected surface fraction, $\phi = \pi a^2N/L^2$, of about $0.4$, a rather soft confining potential of width $\delta = 3a$ will induce more than a ten-fold increase in collective diffusion for a density perturbation of typical size given by a wavelength $ka \sim 6\cdot 10^{-2}$ (corresponding to a wavelength around $100a$). For nanoparticles (of about $10$ nm) these lengths are not small (order $10$ microns). We can find several experimental set-ups where the confinement is even stiffer than our example, $\delta\sim a$ (and thus the increase in diffusion is more pronounced). For instance, in the presence of walls (note that the mobility will be regularized by the presence of a wall) depletion forces can constrain the out-of-plane diffusion of colloids near the wall to $\delta\sim a$. If the surface and the colloids are charged the confinement will be related to the Debye length (nanometers). On the other hand, the theory presented in ~\cite{Pelaez2017} can be directly applicable to confinement by ultrasound~\cite{Balboa2013}. The pressure waves generated by high frequency (MHz) ultrasounds (an effective quadratic potential) can confine micron-sized colloids to a width $\delta \sim a$.
In Fig. \ref{fig:q2DEnhancement} we show the collective diffusion coefficient at short times, given by ~\cite{Dhont1996},
\begin{equation}
  \label{eq:q2dcoldif}
  D_c(k) = D_0\frac{H(k)}{S(k)},
\end{equation}
for systems with several densities.
In Eq. \eqref{eq:q2dcoldif} $S(k)$ is the structure factor (which is $1$ for ideal particles, i.e no structure). $H(k):= H_s + H_c(k)$ is the so-called hydrodynamic mobility function, which further divides into a self ($H_s$) and collective ($H_c$) contributions.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{gfx/q2DEnhancement}
  \caption[ ]{Collective diffusion, $D_c(k)$, of a group of ideal colloids under strict confinement ($\delta\rightarrow 0$) for several densities. Dashed line correspond to $D_c/D_0 = 1 + \frac{\lambda}{2\pi L_h}$. Here $L_h := \frac{2a}{3\phi}$ is the hydrodynamic length and $\lambda := \frac{2\pi}{k}$ is the wavelenght of a density perturbation. Note that for $\lambda \rightarrow \infty$, $D_c\rightarrow D_0$, corresponding to the single particle diffusion coefficient, i.e. in this limit there are no collective effects (in the case of ideal particles).}
  \label{fig:q2DEnhancement}
\end{figure}

Under strict two-dimensional confinement (infinitely stiff trap) the collective colloidal diffusion is enhanced and diverges at zero wave number (like $k^{-1}$), due to the hydrodynamic propagation of the confining force across the layer. Physically this results in increasingly large density perturbations ``repelling'' with increasing strength.


At intermediate and short wavelengths, we study to what extent the hydrodynamic enhancement of diffusion is masked by the conservative forces between colloids, which make the structure factor different from $1$. 

Notably, at very large wavelengths, the collective diffusion becomes even faster than the solvent momentum diffusive transport and a transition from Stokesian dynamics to inertial dynamics takes place, which we study using the \gls{ICM} described in chapter \ref{ch:icm}.

Figure~\ref{fig:q2Dfrom3dto2d} shows the gradual transition from quasi-2D (enhanced) collective diffusion and 3D diffusion, as the stiffness of the trap is decreased. The trap stiffness length is given by $\delta = \left(\frac{\kT}{K_{\text{trap}}}\right)^{\half}$, where $U_{\text{trap}} = \half K_{\text{trap}} z^2$. As expected $H(k)\rightarrow0$ as the diffusion becomes 3D. In other words $D_c(k) = D_0$ for ideal particles in 3D. We observe that the analytic solution for the collective diffusion of colloids under a Gaussian trap of width $\delta$ still shows enhanced diffusion for large wavelengths $k\delta < 1$, and a gradual transition to normal diffusion for $k\delta > 1$ (see Fig. \ref{fig:q2Dfrom3dto2d}).

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{gfx/q2Dfrom3dto2d}
  \caption[ ]{The enhancement of the collective diffusion (given by the hydrodynamic function $H_c(k)$ in Eq. \ref{eq:q2dcoldif}) of a group of ideal colloids ($S(k)=1$) as the confinement goes from non-existent ($\delta\rightarrow \infty$) to stiff ($\delta\rightarrow 0$) trap. Dashed lines correspond to Eq. (18) in ~\cite{Pelaez2017}, from where this figure has been taken.}
  \label{fig:q2Dfrom3dto2d}
\end{figure}

% Using our inertial coupling method code (resolving fluid inertia), we study this transition by performing simulations at small Schmidt number. Simulations confirm theoretical predictions for the $k \rightarrow 0$ limit [Phys. Rev. E 90, 062314 (2014)] showing negative density-density time correlations. However, at finite $k$ simulations show deviations from the theory.

\section{Limit of strict confinement}\label{sec:strictconf}
In ~\cite{Pelaez2018} we focus on the limit $\delta\rightarrow 0$ by taking the mathematical limit of the confining force becoming infinite. We already introduced this limit in chapter \ref{sec:q2D} by introducing a constraint into the mobility $M=M(z)$, leading to a thermal drift $\partial_zM\neq 0$. We saw how the confining force propagates to the fluid resulting in an effectively compressible in-plane mobility (which is the origin of the anomalous collective diffusion). In fact, as represented in Fig. \ref{fig:q2DEnhancement} the collective diffusion coefficient diverges like the inverse of the wavenumber.

In ~\cite{Pelaez2018} we extend the previously existing hydrodynamic theory to account for a species/color labeling of the particles (which is needed to model experiments based on fluorescent techniques). We then study the magnitude and dynamics of the density and color density fluctuations theoretically (using linearized fluctuating hydrodynamics) and simulations (with our novel \gls{BDHI} quasi 2D \gpu algorithm, see chapter \ref{sec:q2D}).
The action of the effective repulsion fundamentally changes the dynamics in quasi 2D as evidenced in figs. \ref{fig:q2Doverdens} and \ref{fig:q2Dstripes}, where we place striped over-densities in the middle of the domain and study their time evolution.

We measure concentration as the number density defined as
\begin{equation}
  c^{(1)}(\vec{\fpos}) = \sum_i\delta(\vec{\fpos}-\vec{\ppos}_i)
\end{equation}
\begin{figure}[H]
  \centering
  \hspace*{-2.0cm}
  \subfloat{\includegraphics[width=0.7\linewidth]{gfx/q2Dstripehisttotal}}
  \subfloat{\includegraphics[width=0.7\linewidth]{gfx/q2Dstripehist}}
  \caption[ ]{Time evolution of a stripe density perturbation initially localized in the middle third of the domain in quasi 2D. Shown here are the density profiles averaged both in ensemble and in the x direction. (Left) The total density, $c^{(1)}(y, t)$, at time $t=0$ (dashed-solid black line) and at a later point in time (red squares), theory for the quasi 2D line (red) comes from our mean field (Eq. 30 in ~\cite{Pelaez2018}). The solution to the diffusion equation without hydrodynamic interactions (at the same time) is shown as a solid black line. (Right) We tag particles starting in the middle stripe as green ($c^{(1)}_G$, green lines and squares) and the rest as red ($c^{(1)}_R$, red lines and circles) and plot their density profiles, $c^{(1)}_{R/G}$. Red and green symbols correspond to $c^{(1)}_R$ and $c^{(1)}_G$ at the same point in time as the left pane. Dotted lines correspond to $t=0$. Our mean field theory is also shown here with solid lines. Dashed lines correspond to the diffusion equation without hydrodynamic interactions. All particles are passive tracers.}
  \label{fig:q2Doverdens}
\end{figure}
Fig. \ref{fig:q2Doverdens} shows the ensemble average of the time evolution of the density profiles, evidencing the effect of the enhanced collective diffusion (marked as q2D in the figure).
On the other hand, we study non-equilibrium giant fluctuations by looking at the time evolution of a single realization of the diffusing stripes. In Fig. \ref{fig:q2Dstripes} we look at the time evolution of an over-density, while \ref{fig:q2Dcolorstripe} shows simulations with ``color'' density gradients. In the latter simulations the system has uniform total density across the whole domain, but we mark (color) particles starting in the middle third of the domain and study the evolution of their concentration.
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{gfx/q2Dstripes}
  \caption[ ]{Time evolution (snapshots at times increasing from left to right) of a density perturbation initially localized in the middle third of the domain (blue represents zero concentration of particles). We show snapshots for quasi 2D (top row) and true 2D (bottom row) at the same diffusive times ($t^{t2D/q2D}= tD_0^{t2D/q2D}/a^2$). The images show the number density by counting the number of particles in each cell of a $128^2$ grid; the color bar is fixed across all panes from 0(blue) to 0.4 (red). All particles are passive tracers.}
  \label{fig:q2Dstripes}
\end{figure}
Giant fluctuations (evident in the bottom row of Fig. \ref{fig:q2Dstripes} for true 2D) have disappeared in quasi 2D in the presence of a density gradient. However, in Fig. \ref{fig:q2Dstripes} giant fluctuations are being masked by the vast dispersion enhanced by the collective diffusion in the presence of density gradients.

The visual appearance of the true 2D panes is similar in figs. \ref{fig:q2Dstripes} and \ref{fig:q2Dcolorstripe} as expected, given that in both cases particles are passive, non-interacting, tracers. Furthermore, we observe giant fluctuations in both density and color gradient experiments for true 2D. On the other hand, giant fluctuations can only be seen for quasi 2D in the color gradient case.

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{gfx/q2Dcolorstripe}
  \caption[ ]{Time evolution (snapshots at times increasing from left to right) of a color (species) density perturbation initially localized in the middle third of the domain. All simulations have a uniform total density, with a packing fraction of $\phi = 1$. We show snapshots in the absence of hydrodynamics (BD, top row), quasi 2D (middle row) and true 2D (bottom row) at the same diffusive times ($t^{BD/t2D/q2D}= tD_0^{BD/t2D/q2D}/a^2$). The images show the number density by counting the number of particles in each cell of a $64^2$ grid; the color bar is fixed across all panes from 0(blue) to 0.4 (red). All particles are passive tracers.}
  \label{fig:q2Dcolorstripe}
\end{figure}

Another puzzling fact discovered in this work is the effect of q2D dynamics on the single particle diffusion (ideal tracers).
In contrast to the diverging collective diffusion we find that the long time self diffusion coefficient is reduced (see Fig. \ref{fig:q2Dselfreduction}). Although the effect is small (less than 15\% reduction of the self diffusion coefficient, see Fig. \ref{fig:q2Dselfreduction}) its physics are an example of coupling between density fluctuations and single particle diffusion, in systems where the divergence of mobility is not zero. The phenomena has also been observed in charged particles, where the electrostatic potential has the same role as the thermal drift $\partial_zM$ here~\cite{Pelaez2018}.

The mathematical formalism for these type of phenomena is not trivial, and reference ~\cite{Pelaez2018} provides a first sketch of it.
%due to the hydrodynamic coupling between a single particle and collective density fluctuations (even for non-interacting particles). We dilucidate the cause of this is \todo{fill}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{gfx/q2Dselfreduction}
  \caption[ ]{Self diffusion coefficient (measured via the mean square displacement (MSD)) of a single particle in quasi 2D for different packing densities, $\phi$.}
  \label{fig:q2Dselfreduction}
\end{figure}





%\chapter{Simulations of dynamically cross-linked actin networks}
%Simulations in this work are accelerated by \uammd's neighbour list capabilities.
%\todo{FILL}

\chapter{Optofluidic control of the dispersion of nanoscale dumbbells}

In ~\cite{Melendez2019} \uammd is used in conjunction with an already established in-house solver for interaction between particles and optical fields to study the hydrodynamically-enhanced dispersion of a collection of nanoscale dumbbells in a vortex lattice formed by an incident laser~\cite{Buscalioni2018}. In particular, the system is simulated with open boundaries via the Cholesky decomposition approach in chapter \ref{sec:chol}. In some simulations hydrodynamics are turned off to isolate their effect by using plain \gls{BD} (introduced in chapter \ref{sec:bd}).

Previous research has shown that gold nanoparticles immersed in water in an optical vortex lattice formed by the perpendicular intersection of two standing light waves with a $\pi/2$ rad phase difference will experience enhanced dispersion that scales with the intensity of the incident laser. We show that flexible nanoscale dumbbells (created by attaching two such gold particles by means of a molecule chain such as a DNA or oligomer) in the same field display different types of motion depending on the chain length and field intensity. We have not disregarded the secondary optical forces due to light scattering. The dumbbells may disperse, rotate, or remain trapped. For some values of the parameters, the (enhanced) dispersion possesses a displacement distribution with exponential tails and exhibiting time-dependent diffusion, including anomalous (though Brownian) diffusion.

The interested reader is referred to ~\cite{Melendez2019} for details. For the sake of this dissertation, the main outcome of this work (and research line) is to highlight the ability of UAMMD to include other type of interactions between fields and particles, which can be expressed in terms of Green functions. In particular, secondary optical forces (arising from light scattered by the nanoparticles) are evaluated from the optical Green function (see ~\cite{Buscalioni2018,Melendez2019}). The techniques developed here for the hydrodynamic propagators can also be used for the optical problem, and this subject is part of ongoing research, the resulting ``opto-hydrodynamic'' computational framework promises to be of great usefulness to analyze recent experiments with plasmonic nanoparticles ~\cite{Ye2020}.






\chapter{Ongoing work, future directions and conclusions}\label{ch:conclusions}
We are living in the birth of the GPU era, in which new computing technologies evolve every day and the software development kits (SDK) that allow to use them evolve alongside them. Lagging behind both hardware and SDK are the applications that make use of them, such as UAMMD. Finally, one step behind all of them lie the novel algorithms, some of which have been developed during this thesis. Climbing up the proverbial ladder requires constant work and research into these new technologies, as if we were walking a mechanical escalator in reverse. In order to successfully survive the constant climb it is worth to, in my opinion, lose the fear of returning back to basics, to reinvent the wheel. We chose CUDA for UAMMD because it was the best choice at the moment, but we did so knowing that this might change over time. In UAMMD reinventing the wheel is easy by design. The base modules (\emph{Interactor}, \emph{Integrator}, \emph{ParticleData},...) are simple and short, allowing for extension or modification of even the most basic assumptions. 

\subsubsection*{New programming interfaces}
Recently SYCL, a cross-platform C++ programming model for heterogeneous computing, has been gaining a lot of traction. The SYCL standard was introduced by the Khronos group (the same group behind OpenCL and OpenGL) and several implementations already exist (like Intel's DPC++ and hipSYCL just to name a few). SYCL aims to provide a common specification allowing to write code that will run in any massively parallel accelerator, such as a GPU or CPU, by having backends to accomodate the different hardware vendors (CUDA for NVIDIA, ROCm for AMD, oneAPI for Intel...). In the future, it is within the realm of possibilities to port UAMMD to SYCL, which will enable it to run on any hardware and, moreover, would make it more future-proof.

\subsubsection*{multi-GPU}
Currently UAMMD is a single GPU library. Once a GPU is selected at initialization every other GPU in the system is ignored. As a matter of fact, in the description of the different algorithms we have not acknowledged the existence of multi-GPU. While some algorithms, such as the neighbour lists or the immersed boundary method, present almost linear scalings when using several processors, others, mainly the ones involving the Fourier transform, require an all-to-all communication in a multi-GPU environment. Many novel algorithms in UAMMD are based on the \gls{FFT} and, given that a single GPU can already fit quite a large system, the extra software complexity and maintenance burden associated with a multi-GPU implementation was deemed not worthy of the effort. Furthermore, our simulations are often governed by thermal fluctuations and require us to launch the same simulation many times to gather statistics. In this case the multi-GPU parallelization is trivial. Nonetheless, the codebase was designed with the possibility of going multi-GPU in mind. In the future it would be possible for a developer to incrementally make the different basic UAMMD modules multi-GPU-aware, but it is not something being considered at the moment.
\subsubsection*{Outreach}
UAMMD offers a plethora of tools that researchers can leverage to accelerate their simulations, however, making use of them requires certain knowledge and resources that might not be in the skillset of a typical research group, much less an experimental group. One of our current lines of work consists in bridging the gap between us, producers of scientific software, and their potential users, such as experimental research groups, in particular by offering easy-to-use graphical user interfaces (GUI) tailored for the (quantitative) reproduction of specific experiments or experimental techniques. At the moment, we are pursuing this idea by iterating, with the help of a research group at CSIC, towards a GUI for a ``virtual QCM'' which will aid in the characterization of samples for this sensor.

\subsubsection*{What others are doing with UAMMD}
At the moment, extensions to UAMMD are being actively developed by several collaborators. I still retain my role as the lead developer and routinely push fixes, improvements and new modules to the main repository, but other researchers have been forking, extending and hacking UAMMD for some time now. It is worth mentioning here some of the ongoing projects that are being carried out by collaborators using UAMMD.

\begin{itemize}
\item Pablo Ibañez is studying mechanical properties of virus capsids under the stress of an atomic force microscope's tip.
\item Marc Meléndez and collaborators are studying the dynamical properties of a Quartz Crystal Microbalance (QCM) biosensor for sample characterization, which involves embedding an oscillating wall in a fluid with inertial effects. The subtle coupling between a sample (placed on top of the wall) and the oscillating motion of the wall is studied to provide insights about the sample.
\item Pablo Palacios is studying the response of bioconjugated magnetic nanoparticles to an alternating magnetic field for biomarker detection. This endeavor requires the development of a new \gls{FCM}-like algorithm for magnetism. Furthermore, the aforementioned nanoparticles have a direction, requiring to take torques and angular displacements into account when adding hydrodynamics.
\item Collaborators at the New York University are using UAMMD's hydrodynamic and electrostatics modules for several works, such as the study of the rheological properties of a fiber suspension (actin networks), charge diffusion in ionic channels, and more. 
  % \item Interaction between particles and fields via free energy functionals.
\item Salvatore Assenza is incorporating coarse-grained models for simulations of ADN strand suspensions.
\end{itemize}

Finally, several works exploring UAMMD's facet as an external, GPU-enabled, accelerator library are ongoing, mostly making use of the hydrodynamic solvers, electrostatic solvers and neighbour list capabilities of UAMMD. In particular, I routinely provide external collaborators with python interfaces for the different UAMMD modules required by their research.

\subsubsection*{Novel algorithms}
New algorithms have been developed that enable simulations of physical regimes that where previously unachievable. These algorithms are accompanied by highly performant GPU implementations in UAMMD, boosting their applicability. The bulk of this thesis contribution to this end deals with hydrodynamics and electrostatics in confined geometries. Furthermore, an efficient, GPU-enabled, algorithm for particle-grid communication has been developed and shown to outperform existing alternatives in the common use cases in complex fluid simulations.


\subsubsection*{New physics}
%\begin{itemize}
%\item
Although this thesis is more centered around developing new techniques and software than finding and/or explaining new physical phenomena per se, several works presenting new physics have been shown. In particular, works studying diffusion in confined geometries, rheological properties of star polymer suspensions, intra-cellular viscosity and opto-hydrodynamics have been discussed. Some of these lines of work are still active and more publications are expected.



\chapter{Spanish conclusions}
Estamos viviendo los orígenes de la era de la GPU, en la que las nuevas tecnologías evolucionan cada día, así como las herramientas de software que permiten hacer uso de ellas. Un paso por detrás se encuentran las aplicaciones que hacen uso de la GPU, tales como UAMMD. Finalmente, aún más atrás están los algoritmos novedosos, algunos de los cuales han sido desarrollados en esta tesis. Trepar por la escalera proverbial requiere trabajo e investigación constantes en estas nuevas tecnologías, como si quisiéramos subir por una escalera mecánica en sentido contrario. Para sobrevivir con éxito esta constante subida es necesario, en mi opinión, perder el miedo a volver a explorar las asunciones más básicas y a reinventar la rueda. Elegimos CUDA para UAMMD porque era la opción más adecuada en el momento, pero lo hicimos sabiendo que esto podría cambiar en el futuro. En UAMMD reinventar la rueda es fácil por diseño. Los módulos base (\emph{Interactor}, \emph{Integrator}, \emph{ParticleData},...) son simples y cortos, permitiendo la extensión y/o modificación de incluso sus principios más fundamentales.

\subsubsection*{Nuevos algoritmos}
Durante esta tesis se han desarollado nuevos algoritmos que permiten la simulación de regímenes físicos que eran previamente inalcanzables. Estos algoritmos están acompañados de implementaciones de alto rendimiento en GPU dentro de UAMMD, impulsando su aplicabilidad. El grueso de los algoritmos desarrollados en esta tesis lidian con la hidrodinámica y electrostática en geometrías confinadas. Además, se ha desarrollado e implementado en UAMMD un algoritmo GPU para la comunicación entre partículas y una malla que muestra ser más performante que las alternativas existentes en los casos de uso habituales en las simulaciones de fluidos complejos.

\subsubsection*{Nueva Física}
%\begin{itemize}
%\item
Aunque esta tesis no está centrada en encontrar/explicar nuevos fenómenos físicos per se, se han mostrado varios trabajos presentando nueva física. En particular se han mencionado trabajos que estudian difusión en geometrías confinadas, propiedades rehológicas de polímeros estrella en suspensión, viscosidad intra celular y opto-hidrodinámica. Algunas de estas lineas de trabajo están aún activas y se esperan más publicaciones.



%\item For instance, the Saffman mobility could be used to model the diffusion of proteins in a lipid membrane~\cite{Saffman}.
%\end{itemize}

%\todo{ What should I say here?}

%\begin{itemize}
%\item Long time self diffussion
%\item Linear response theory for MSD
%\item Ensemble averages for color
%\item giant fluctuations comparison BD, q2D, t2D
%\item Bridging the gap to 3D, the short time collective diffusion coefficient and the hydrodynamic function
%\item Mention steric repulsion, only affects large wavenumbers (steric repulsion size), but the colective effect remains.
%\end{itemize}
%Bulk diffusion of particles in liquids is well-known to be controlled by hydrodynamics, and diffusion on interfaces is no exception. While the diffusion of colloids and polymers on a fluid-fluid interface has been studied theoretically since the 1970s ~\cite{FluidFluidInterface_Polymers,Diffusion2D_TwoViscosities}, \emph{collective diffusion} in a monolayer of colloidal particles confined to a fluid-fluid interface has only recently been explored in some detail both theoretically ~\cite{Bleibel2014,Bleibel2016,Bleibel2015} and experimentally[11].
%
%The hydrodynamic coupling between the confining force acting on each individual colloid and their collective motion leads to surprising effects.
%For instance, if the constraint is holonomic (colloids are restricted to move on a purely 2D domain), the confining force can be seen to arise from the non-zero divergence of the mobility tensor, which is related to the reduction of entropy imposed across the confining plane. In the case of a point wise force in unbounded fluid such flow perturbation (Oseen) decays like the inverse of the distance and, as a consequence, the diffusion coefficient of larger and larger colloidal density fluctuations increases without bounds. In particular, this results in the short-time collective diffusion diverging as the inverse of the wave number.
%This unexpected finding has prompted re-examination of previous experimental results ~\cite{Diffusion2D_Experiments_Weitz,SD_TwoWalls,Diffusion2D_Experiments_Rice}, and it is plausible that the effect may have been overlooked in a number of other prior experimental and theoretical works as well.
%
%The physical origin of the anomalous collective diffusion has already been elucidated in prior work by others~\cite{Bleibel2014,Bleibel2016,Bleibel2015,Dominguez2014}. Basically, momentum conservation implies that the confining force acting on each particle has to propagate to the surrounding, incompressible, solvent. In the plane, the resulting hydrodynamic drag created by one particle acts like a flow source that induces an effective repulsive force on the other particles.
%
%This results in strong displacement correlations which are the origin of the anomalous collective diffusion.
%A new interpretation of this phenomena is obtained if one focuses on the in-plane dynamics.
%In the plane, the field of diffusive displacements appears to be compressible due to the momentum source associated with each confined particle.
%In the limit of strict confinement, the flow acts as a compressible two-dimensional cut through an incompressible three-dimensional flow field.
%Because of this apparent compressibility, the Brownian motion of the particles creates an osmotic pressure proportional to $\kT$, which propagates via the fluid inducing long-ranged repulsive interactions.
%This long-ranged repulsion decays slowly with the particle-particle distance, and dramatically accelerates collective diffusion at large scales.
%Furthermore,
%% contrary to what was previously thought ~\cite{Pelaez2017,Bleibel2017},
%these long-ranged correlations also appreciably modify the single-particle diffusion, as we will see in future sections.
%
%Such types of hydrodynamic enhancements do not only take place when the fluid is unbounded in the three directions. The presence of distant boundaries modifies the propagation (e.g., $1/r^2$ inside a slit channel[12,13]), leading to yet-unexplained and stronger variations of this phenomena which might also depend on the wall's slip boundary conditions.
%We will study how this anomalous collective diffusion becomes normal as the confinement is made softer.
%
%During the following chapters, we will study the collective diffusion and transport properties of a suspension of particles confined to a plane in two regimes:
%\begin{itemize}
%\item When the confinement is not strict and particles can leave the plane. For this, we can make use of the \gls{FIB} or \gls{FCM} described in previous sections and impose an external confining force on each particle. Although our implementations for these algorithms are triply periodic, we will neglect the contributions of the periodic copies of the system in the third direction by making the domain be much larger than the typical width of the suspension. We can also study the transient to the inertial regime by using \gls{ICM}.
%\item In the limit of strict confinement. Since our region of interest is the purely 2D plane in which particles diffuse, we will develop a new algorithm that takes into account the unbounded fluid in the $z$ direction implicitly.
%\end{itemize}
%Additionally, we will describe a new doubly periodic algorithm capable of solving hydrodynamics in the first scenario by directly imposing a different set of boundary conditions at the domain limits in the third direction. In particular, we can use this new framework to impose open boundaries, with the possibility of adding one or two walls at the domain limits (particles near a wall or a slit channel).
%
%
%%\chapter{Diffusion under soft confinement}\label{ch:softq2D}
%We will see how this length scale governs the collective intermediate scattering function of colloids under harmonic confinement. We will examine how the enhancement of the collective diffussion dissapears going from the limit of strict confinement with $K_s\rightarrow \infty$ to the purely open 3D hydrodynamics with $K_s\rightarrow 0$.
%Excluded volume and other potential interactions between colloids add another length scale to the collective diffusion, which partially masks the anomalous hydrodynamic enhancement. This masking depends on the static structure factor, $S(k)$, and here we illustrate the case of repulsive, \gls{WCA}, and attractive, \gls{LJ}, potentials.
%
%Later, we will take the mathematical limit of $K_s\rightarrow \infty$ to connect 
%
%%% Q2d paper intro:
%
%%Previous work on collective diffusion of colloids on fluid-fluid interfaces~\cite{Bleibel2014,Bleibel2016,Bleibel2015} has focused on the \emph{ensemble average}. Our focus here is on the \emph{fluctuations} around the ensemble average, i.e., on the equations for the evolution of a particular \emph{instance} (trajectory) of diffusive processes on interfaces. We use the fluctuating DDFT-HI developed in ~\cite{Donev2014}, which is closely-related to \emph{fluctuating hydrodynamics} (FHD)~\cite{Donev2014_2}. The (formal) nonlinear equations of FHD are challenging to interpret ~\cite{SPDE_Diffusion_DDFT,DDFT_Hydro}, but, at the same time, they are more physically transparent than the ensemble-averaged equations, and do not require closures for noninteracting particles~\cite{DDFT_Hydro}. Furthermore, as we will see later, the fluctuating DDFT-HI equations are a very useful tool for constructing linear-time Brownian Dynamics (BD) algorithms.
%%%%%%%%
%
%
%\begin{itemize}
%\item Short time Collective diffusion
%\item Recovering 3D Hydrodynamics
%\item Color density fluctuations
%  Understanding the magnitude and the dynamics of density fluctuations is crucial for several reasons. First, in actual experiments one observes individual instances, not the ensemble average. While in many systems typical instances are quite similar to the average, i.e., the fluctuations are small compared to the mean, this is not always the case. It is well-known that nonequilibrium fluctuations in diffusive processes are, rather generally, much larger in magnitude than equilibrium fluctuations, and are also long ranged ~\cite{FluctHydroNonEq_Book}.
%Experiments in microgravity have measured these ``giant fluctuations'' in three dimensions, and shown that nonequilibrium diffusive fluctuations are correlated over macroscopic distances ~\cite{FractalDiffusion_Microgravity,GRADFLEXTransient}.
%
%For diffusion in two-dimensional systems such as thin smectic films in vacuum ~\cite{ThinFilms_True2D}, linearized FHD predicts that the magnitude of the nonequilibrium fluctuations becomes comparable to the mean ~\cite{GiantFluctuations_ThinFilms}. The appearance of such ``colossal fluctuations'' implies that the ensemble average is no longer informative, since each instance looks very different from the mean.
%It is therefore essential to understand whether individual instances of diffusive mixing processes on fluid-fluid interfaces evolve similarly to the ensemble average.
%Second, fluctuations can be measured in experiments and reveal information about the underlying microscopic mechanism of diffusion. For diffusion in bulk three-dimensional liquids or in truly two-dimensional liquids (e.g., a hard-disk fluid), the ensemble average strictly follows the familiar Fick's law ~\cite{DiffusionJSTAT}.
%This means that the ensemble-averaged concentration in a system where the diffusing particles are strongly correlated by hydrodynamics (e.g., two-dimensional thin smectic films ~\cite{ThinFilms_HIs}), will look indistinguishable from the ensemble average in a system where the diffusing particles are uncorrelated (e.g., diffusion in a solid).
%
%But if one examines the magnitude of the nonequilibrium fluctuations, the hydrodynamic correlations are revealed through an unexpected power-law dependence of the structure factor on the wavenumber ~\cite{GiantFluctuations_ThinFilms}.
%
%Third, collective fluctuations can couple bi-directionally to the motion of each particle and therefore \emph{renormalize} transport coefficients. For example, the fluctuating hydrodynamic theory developed in ~\cite{TracerDiffusion_Demery} shows that collective density fluctuations reduce the effective diffusion coefficient of a tagged particle in a dense system of uncorrelated Brownian soft spheres.
%In this paper we will show that a similar effect exists even for an ideal gas of non-interacting but hydrodynamically-correlated particles diffusing on a fluid-fluid interface.
%
%We will study the ensemble average and fluctuations of the density of spherical colloidal particles confined to diffuse on a fluid-fluid interface. We closely mimic the physical setting used in prior studies ~\cite{ConfinedDiffusion_2D,DDFT_Diffusion_2D,Diffusion2D_IdealGas}, and make a number of strong simplifying assumptions:
%\begin{enumerate}
%\item We assume that the interface is perfectly flat and that two fluids have the same viscosity; the case of unequal viscosity simply amounts to taking the arithmetic average of the two viscosities ~\cite{Diffusion2D_TwoViscosities,FluidFluidInterface_Polymers}.
%\item In order to isolate the role of hydrodynamics from the role of other direct interactions such as capillary forces or steric/electrostatic repulsion, we focus on an \emph{ideal gas} of non-interacting spherical colloids ~\cite{Diffusion2D_IdealGas}.
%  While such an idealized system could not be studied experimentally, it is a natural candidate for testing existing and developing new theories.
%  Furthermore, light scattering observational data for colloids at a fluid interface support predictions made for an ideal gas of particles ~\cite{Diffusion2D_IdealGas}.
%\item We assume that the colloids are strictly confined to the interface by a strong confining force in the $z$-direction, i.e., they cannot leave the $x-y$ plane.
%  In reality, the confinement would be partial, for example, a laser sheet may provide a harmonic confining potential in the $z$ direction. However, prior work ~\cite{PartiallyConfined_Quasi2D,PartiallyConfinedDiffusion_2D} has shown that partial confinement only changes the results for larger wavenumbers (i.e., for wavelengths smaller than or comparable to the range of movement in the $z$ direction), and does not affect the anomalous diffusion in the plane.
%  At the same time, we will show here that the case of strict confinement can be simulated much more efficiently using a two-dimensional Brownian dynamics algorithm.
%\item We use a minimal far-field description of the hydrodynamics, as used in prior work by others ~\cite{ConfinedDiffusion_2D,DDFT_Diffusion_2D,Diffusion2D_IdealGas}.
%Such a Rotne-Prager approximation is quantitatively accurate only for dilute suspensions, and we may question its usefulness in studying a \emph{collective }effect that is dominant at larger packing densities.
%Nevertheless, as already mentioned, the anomalous collective diffusion arises because of a long-ranged (far-field) repulsion $\sim k_{B}T$.
%We therefore believe that short-ranged (near-field) corrections to the hydrodynamics will not qualitatively change the phenomena studied here.
%\end{enumerate}
%While our focus is diffusion on a fluid-fluid interface, which we will refer to as Quasi2D (abbreviated q2D) diffusion, we will contrast Quasi2D diffusion to diffusion in truly two-dimensional systems such as thin films in vacuum ~\cite{ThinFilms_True2D}, which we will refer to as True2D (abbreviated t2D). Even though the diffusion is constrained to a two-dimensional plane in both cases, the hydrodynamics is essentially three-dimensional in Quasi2D but it is two-dimensional in True2D.
%Our computational and analytical tools can easily be extended to other types of hydrodynamics. For example, it is believed that lateral diffusion in lipid membranes ~\cite{MembraneDiffusion_Review} or thin smectic films in air ~\cite{ThinFilms_HIs} can be described using a hydrodynamic model first proposed by Saffman. The (2+1)-dimensional ((2+1)D for short) Saffman model has already been combined with linearized fluctuating hydrodynamics in ~\cite{GiantFluctuations_ThinFilms}, but an experimental confirmation of the predictions of the theory is still lacking.
%
%\end{itemize}
%
%\begin{itemize}
%\item Enhanced diffusion under soft confinement~\cite{Pelaez2017}
%\item Limit to infinetely stiff confinement~\cite{Pelaez2018}
%\end{itemize}
%

\newpage
\cleardoublepage

\part{Appendix}

\appendix

\chapter{Basic notions of CUDA/C++ programming}\label{sec:cpp}

\uammd uses the C++14 standard\footnote{The only restriction in adopting more modern standards, like C++20, is the adoption rate of CUDA. Up to the date of this writing, the latest CUDA toolkit supports up to C++17.}, which presents subtle but sometimes important differences with the previous iteration of the language (C++11). Note however, that this standard is wildly different to ``classic'' C++98, which could be considered another language altogether. Furthermore, \uammd uses the CUDA extensions to the language, which introduce some new keywords and subtle rules.

In this chapter, we will give a few hints to ease the interpretation of the code examples throughout this manuscript for the uninitiated. Note that this is not intended to be a thorough description or tutorial of the language. For that the reader is redirected to the plethora of resources available online regarding CUDA and modern C++. This chapter summarizes most of the tools from the C++ language (the bare minimum) that a reader needs to be aware of in order to follow the logic in the examples scattered throughout this manuscript.

CUDA/C++ is a compiled language, meaning that before executing a source code, it must be transformed into an executable binary by an external program called the \emph{compiler}. We will also refrain from discussing the compilation of CUDA codes\footnote{\uammd's online resources shed light on this process, which might change over time for reasons outside of the author's control.}.

Let's start with some basic concepts in C++ and then go to the \gpu. The entry point for execution in all C++ codes starts with a function called ``main''. This function must be defined by the user with the name ``main'' and a returning type int (see example code \ref{code:minimal}).

\begin{code2}[A C++ program that does nothing.]
{label=code:minimal}
//This is a comment, which is ignored by the compiler
//The main function must be present in all C++ programs
int main(){
  //main must return an integer, encoded as 0 if the
  // program terminated sucessfully, and 1 otherwise.
  return 0;
}
\end{code2}

\subsection*{The auto keyword}
The keyword \emph{auto} can be seen in almost every code example in this manuscript. In contrast with other loosely typed languages, C++ forces the developer to always state the actual type of a variable when defining it. Similarly, the return type of a function must be specified.
In some situations, for instance when dealing with metaprogramming (which we will discuss shortly), knowing the name of the relevant type can become cumbersome and verbose. Luckily, the latest standards allow to replace the type name by the keyword \emph{auto}. When the compiler sees a variable with the type \emph{auto}, it will automatically deduce the correct type for it. Example code \ref{code:foo} introduces a couple of use cases for the \emph{auto} keyword.

\subsection*{Functions}
Main is an example of a function (albeit a special one), the syntax for defining custom functions is similar, see code \ref{code:foo}. The special return type \mintinline{\ucpp}{void} can be used to signify that a function returns nothing.

\begin{code2}[Defining a function in C++.] {label=code:foo}
//We can define a function with the syntax below
//The returning type, in this case int, can be replaced 
// by the keyword auto.
int foo(int a){
  //This function takes an integer, referred to as "a"
  // and returns its value multiplied by 2
  return a*2;
}
//The main function will simply call the foo function with an
// arbitrary number and store the result in a variable
int main(){
  int value = foo(12);
  //value holds the integer 24
  //The type of the variable, int, can be replaced by auto.
  //For instance,
  auto value2 = foo(12);
  //The type of value2 is automatically deduced to be "int"
  return 0;
}
\end{code2}

\subsection*{Including other codes}
By default, a C++ source code will have access only to the basic rules of the language. We can add libraries via the \emph{include} directive, which will make available to us every function, class and variable defined in another file. For instance, in example code \ref{code:hello} we can see how to include the standard library header file that provides printing functionality.

\begin{code2}[The classic Hello World program in C++. Once executed, it will print ``Hello world'' to the terminal.] {label=code:hello}
  #include<iostream> //Includes std::cout and std::endl
  //Uncommenting the line below permits to omit writing std::
  //using namespace std;
  int main(){    
    std::cout<<"Hello world"<<std::endl;    
    return 0;  
  }
\end{code2}

\subsection*{Namespaces and the using keyword}
In order to avoid naming collisions (two libraries or files defining entities with the same name) C++ provides the concept of \emph{namespaces}. We have already seen the standard library namespace, \emph{std}, in action in example \ref{code:hello}. In order to reference an entity inside a given namespace, the ``::'' operator is used. In the aforementioned example, we access the object \emph{cout} of the \emph{std} namespace with \mintinline{\ucpp}{std::cout}.

This can become a nuisance when a given namespace is constantly used. C++ provides the \mintinline{\ucpp}{using namespace} construct to mitigate this. For instance, throughout this manuscript, the line \mintinline{\ucpp}{using namespace uammd;} is used to omit writting \mintinline{\ucpp}{uammd::} every time an \uammd entity is referenced.

The \mintinline{\ucpp}{using} keyword has a second usage, it allows to ``rename'' a type. In C++ type names can become quite convoluted and long. The \mintinline{\ucpp}{using} keyword can be used in these cases. For instance, in example code \ref{code:pf} we write the line \mintinline{\ucpp}{using PF = PairForces<Potential, NeighbourList>;}, allowing to use simply ``PF'' when we want to refer to the longer type name.
\subsection*{Objects}
Although C++ can work as a functional language, it is heavily object oriented. The \mintinline{\ucpp}{struct} and \mintinline{\ucpp}{class} keywords are used to define objects in C++.

An object can hold both variables and functions, which are referred to as \emph{members}. A member function in a class has access to all other members of the object. In general, object members (functions or variables) can have either private or public visibility. From the outside, only public members can be accessed, whereas private members can only be accessed by other members of the same class.

In the author's personal programming style, structs are usually employed to serve as an aggregate of variables, whereas classes are used as a more complex object (with member functions, etc). Note however, that the struct and class keywords are almost interchangeable\footnote{The only difference between a struct and a class is the default visibility of its members. By default, members in a struct have public visibility, whereas a class defaults to private visibility.}.

\begin{code2}[Examples of object creation.] {label=code:class}
  #include<iostream>
  //The syntax for creating a struct with type name Parameters, holds two double variables.
  struct Parameters{
    double var1;
    double var2;    
  };
  //A class with public visibility is equivalent to a struct.
  class MyClass{
    //The public keyword marks every defined member below as accessible from outside
    public:
    void print(){
      std::cout<<"Hello"<<std::endl;
    }
  };
  int main(){
    //Create an instance of the struct Parameters
    Parameters par;
    //Set the value of one of its variables
    par.var1 = 1.0;
    //Create an instance of the object MyClass
    MyClass c;
    //This "c" is created here as an instance of MyClass
    //Call its member "print".
    c.print();
    return 0;
  }
\end{code2}
Given that objects can be created and passed around as regular variables, we can use them to encapsulate and transport logic between different parts of the code.

In \uammd, every \emph{Interactor} and \emph{Integrator} is provided as a C++ \mintinline{\ucpp}{class}.

\subsection*{Scope}
A name (be it of a function, class, variable, etc...) is only visible inside a certain portion (or portions) of the source code. This portion is called the \emph{scope} of the name. There are several scopes in a C++ source code, however we will limit our introduction to discuss the lifespan of the variables we create.

In particular, when a new instance of an object (or in general any variable) is created, as we saw in example \ref{code:class}, it will live until it goes out of \emph{scope}. At that point, the object/variable will be destroyed.
Destruction involves the invalidation of the variable and the release of the memory allocated for it. Additionally, in the case of objects the destructor function, if defined for that object, will be called. When a name goes out of scope it is made available for use again (i.e. we cannot have two variables with the same name, unless is out of scope).

The portion of code between two curly brackets is called a \emph{block}. As a rule of thumb, a variable will live until the code block in which it has been defined is closed. In \uammd this is used, for instance, to release the handles to the particle properties provided by \emph{ParticleData}. When the destructor of one of these handles is called, it signals \emph{ParticleData} for it to take it into account (for instance, by synchronizing the CPU and \gpu versions of the data).

Let us showcase the lifespan of a variable by creating some instances inside different types of code blocks.
\begin{code2}[Examples of a variable going out of scope]{label=code:scope}
  //Let us define a class and give it a destructor
  class MyClass{

    public:
    //The destructor is a special member function that has ~[class name] as signature.
    ~MyClass(){
      std::cerr<<"This is printed when the instance goes out of scope"<<std::endl;
    }
    //Additionally, a constructor can be defined.
    //The constructor can even have input arguments.
    MyClass(){
      std::cerr<<"This is printed when an instance is created"<<std::endl;
    }
    //If either the constructor or destructor are not present, they are automatically defined by the compiler as just empty functions
  };

  //A function that just creates an instance of MyClass
  void foo(){
    MyClass c; //"c" is created and its constructor is called
  }//"c" goes out of scope and its destructor is called
  
  int main(){
    {//This is a code block
      MyClass c; //"c" is created and its constructor is called
      //Note that even though we used the name "c" in the function foo, we can use it again here, since the variable "c" inside foo went out of scope
    }// "c" goes out of scope here and its destructor is called
    float someVariable;//Let's create some variable here
    //A loop also works as a code block
    for(int i=0; i<10; i++){
      MyClass c; //"c" is created
      //Again, the name "c" is available because it was not currently in scope
    }//"c" goes out of scope

    //MyClass someVariable; //This is an illegal name, because there is already a variable in scope with this name
    MyClass someVariable2;
    return 0;
  }//someVariable2 is released here and its destructor is called
\end{code2}

\subsection*{Object inheritance}
In C++, objects can inherit the functionality of other classes, and even override some of it. For instance, every polygon has an area, we can create a class called \emph{Polygon} that provides a member function called \emph{area} (returning the area). However, a polygon is just a concept that has no area per se. We can then create another class called \emph{Square} that inherits from \emph{Polygon} and overrides the \emph{area} function, providing its squared side.

Given that the area of a \emph{Polygon} object is meaningless, we can mark it's \emph{area} function as \mintinline{\ucpp}{virtual} and add the suffix \mintinline{\ucpp}{= 0;} to its signature. This transforms the \emph{Polygon} class into a \emph{virtual base class}, which means that it cannot be instanced directly, rather it must be inherited and the virtual members must be overriden. See example code \ref{code:inherit}.
\begin{code2}[Inheriting a virtual class.] {label=code:inherit}
  //This is a pure virtual class. A "Polygon" cannot be instantiated, only classes that inherit from it are instantiable.
  class Polygon{
    public:
    virtual double area() = 0;
  };
  //This class is a kind of Polygon (inherits from it)
  class Square: public Polygon{
    double side = 2.0; //An arbitrary value for the side
    public:
    //We can override the area member of Polygon
    virtual double area() override{
      return side*side;
    }
  };
  //This class is a kind of Polygon (inherits from it)
  class Circle: public Polygon{
    double radius = 2.0; //An arbitrary value for the radius
    public:
    //We can override the area member of Polygon
    virtual double area() override{
      return 3.1415*radius*radius;
    }
  };

  int main(){
    Square s;
    auto a = s.area();
    Circle c;
    auto a2 = c.area();
    //Polygon pol; //Illegal, a pure virtual class cannot be instanciated.
    return 0;  
  }
\end{code2}

In \uammd, \emph{Interactor} and \emph{Integrator} are virtual classes such as \emph{Polygon}. One of the benefits of using inheritance is that it allows to provide a general logical interface. In general C++ an inherited class can masquerade as an instance of the parent class (but not the other way around). So a \emph{Square} can be used as a \emph{Polygon} but not viceversa. This only happens, however, for pointers (since the virtual base class cannot be instanced directly).

Let us discuss now the type of pointer used throughout this manuscript, the \mintinline{\ucpp}{std::shared_ptr}.

\subsection*{Shared pointers}
Without going into details (the reader can learn more online), a C++ shared pointer can hold ownership of an object through a pointer. In the \uammd examples in this manuscript we use shared pointers mainly to pass around \emph{Interactors} and \emph{Integrators}. For instance, in example \ref{code:pf} we create and return an instance of \emph{PairForces}, a class inherited from \emph{Interactor}. On the other hand, the \emph{Integrator} member function \mintinline{\ucpp}{addInteractor();} will take in its argument a pointer to an \emph{Interactor}, meaning that the \emph{PairForces} instance created in example \ref{code:pf} can be passed to an \emph{Integrator} (such as the one created in example \ref{code:verletnve}) directly.


\begin{code2}[Using shared pointer to hold many types of polygons.] {label=code:sharedptr}
  #include<memory> //std::shared_ptr
  #include<vector> //std::vector
  //Using the Polygon declarations of the previous example
  //This function takes a pointer to a polygon and returns its area
  auto getAreaOf(std::shared_ptr<Polygon> pol){
    return pol->area();
  }
  int main(){
    //Create pointers to a square and a circle
    auto s = std::make_shared<Square>();
    auto c = std::make_shared<Circle>();
    //The getAreaOf function works for pointers of any class that inherits from Polygon
    double as = getAreaOf(s);
    double ac = getAreaOf(c);
    //We can also aggregate pointers to Polygon children in a vector
    std::vector<std::shared_ptr<Polygon> > vec({s, c});
    return 0;  
  }
\end{code2}

\subsection*{Metaprogramming}
In addition to inheritance, C++ offers another way of writing generic code called templates. A template is a function or object that is generic for any type (sometimes the type is restricted under some assumptions). For instance, let us rewrite the function \mintinline{\ucpp}{getAreaOf} in example \ref{code:sharedptr} using metaprogramming instead of inheritance.

\begin{code2}[Using shared pointer to hold many types of polygons.] {label=code:sharedptr}
  //This function takes any type and returns its area. Of course, it will not compile if the provided type has no member function called area.
  //When we call this function, T will be replaced by the type of the provided argument
  template<class T>
  auto getAreaOf(T pol){
    return pol.area();
  }
  int main(){
    Square s;
    Circle c;
    //The getAreaOf function now works any type that provides a function called area
    double as = getAreaOf(s);
    double ac = getAreaOf(c);    
    double var = 1.0;
    //The code would be invalid if a double is passed, since the function expects the member area to exists.
    //double avar = getAreaOf(var);
    return 0;  
  }
\end{code2}

In \uammd, templates are used extensively to generalize modules. For instance, the \emph{PairForces} module (see example code \ref{code:pf}) is templated for both the potential and the neighbour list. In another example, the \gls{IBM} module is templated for several aspects of the algorithm, like the spreading kernel, the particle and grid quantities or the quadrature weights.

\subsection*{Iterators}
In C++, \emph{Iterators} are a type of object that \emph{points} to an element in a range of elements (such as a container or pseudo-container) and has the ability to \emph{iterate} through the elements of that range. The canonical example of an iterator is a pointer to the first element of an array. We can use a pointer to iterate through the contents of the array. However, like many things in programming, the internal workings of a certain iterator are irrelevant as long as the iterator works ``as is''. For instance, a \emph{constant} input iterator works by always returning the same value, regardless of what ``element'' in the ``container'' it is pointing to, as if it was iterating over an infinite sized array filled with the same number. In practice, a constant input iterator would be implemented as simply providing the same number all the time, without requiring any underlying memory to be allocated or present.

Several categories of iterators exists depending on the functionality they provide, figure \ref{fig:iterator} provides an overview.
\begin{figure}
  \centering
  \includesvg[width=0.25\columnwidth]{gfx/iterator}
  \caption[ ]{The different kinds of C++ iterators. Iterators can be either for input (reading), output (writing) or both (read/write). The most generic type of iterator is called \emph{Random Access} and allows for arbitrary access to its elements. Below, a \emph{Bidirectional} constrains the access to be always next to the previously accessed element. In other words, a \emph{Bidirectional} iterator can only be issued to advance or regress one element at a time. Finally, the most restricted iterator kind, the \emph{Forward} iterator, can only be accesed element by element in order. }
  \label{fig:iterator}
\end{figure}
Iterators play a central role in connecting algorithms with containers in C++ and allow for deep customization. For instance, say that a given algorithm requires as input an ordered range of numbers and, after some operations, fills with some results the contents of two output arrays. Furthermore, say we are only interested in one of the two output arrays. Instead of allocating three vectors and filling the first with the range of numbers we could use some fancy iterators to save a lot of work; For the input range of numbers, we could pass an input \emph{counting iterator} (see example code \ref{code:iterators}). For the non-required output, we could provide the algorithm with an output \emph{discard iterator}\footnote{The thrust library provides a series of fancy iterators like the ones mentioned in this paragraph (counting, constant, discard...).} that simply ignores any value that is assigned to any of its elements (thus preventing unnecessary memory copies).

In another instance an algorithm may require to iterate through the elements of a container in reverse (starting at the last element and iterating up to the first), a \emph{reverse} iterator could be used instead of reordering the container prior to the execution of the algorithm.

Iterators appear many times in \uammd. For instance, any \emph{random access} iterator can be provided for the input per-particle and per-grid quantities in the \gls{IBM} module (see chapter \ref{sec:ibm}). The only restriction here would be that depending on whether we are spreading or interpolating, the iterators will be required to be either input or output ones. A simple example case of this would be, for instance, spreading the charges of a collection of equally charged particles. A \emph{constant iterator} could be provided in this case.

\begin{code2}[Using iterators in C++. In this example a counting iterator for the range 0-5 is constructed from three different sources. For the first two cases a container (vector) is created and filled with the actual integers from 0 to 5 (requiring memory allocation). We then get an iterator to the range of the container via the method \emph{begin} of the vector (which returns an iterator pointing to the first element, see the variable ``vit''). An equivalent way in this case is to simply get a pointer to the first element of the container (variable ``pit''). Finally, a counting iterator is constructed using the one available in the thrust library. This special iterator is not associated to an underlying container, instead it is an object that provides an overloaded bracket operator that when given an integer, ``i'', simply returns that same integer, ``i''. Note that in this example, ``vit'' and ``pit'' are input/output iterators, while ``cit'' is just an input iterator. The method ``cbegin'' in vector can be used to make ``vit'' an input iterator instead. Additionally, making the type of ``pit'' be ``const int*'' will also turn it into an input iterator. On the other hand, ``cit'' cannot be an output iterator, since there is nothing to write to behind it.] {label=code:iterators}
#include<iostream>
#include<vector>
#include<thrust/iterator/counting_iterator.h>
//Three ways of making a counting iterator
int main(){
  std::vector<int> v{0,1,2,3,4,5}; //A vector containing the integers from 0 to 5
  auto vit = v.begin(); //Vector iterator
  int* pit = &v[0];     //Pointer to the first element of the vector
  thrust::counting_iterator<int> cit(0); //A counting iterator that starts at 0
  //All three iterators return the same numbers in this range
  //Note that only "cit" is accessible beyond i=5, since the vector only has 6 elements
  for(int i = 0; i<v.size(); i++){
    std::cout<<vit[i]<<" "<<pit[i]<<" "<<cit[i]<<std::endl;
  }
  return 0;
}
\end{code2}
\subsection*{Other keywords}
The \mintinline{\ucpp}{const} keyword simply informs the reader (and compiler) that the value of the accompanying variable cannot be modified after its declaration. When appearing in the signature of an object's member function, it conveys that the function will not modify the state of the object in any way.

\section{Basics rules of CUDA programming}
CUDA is an extension of C++. All the language rules we have seen thus far also apply to CUDA. CUDA introduces some new keywords and an \gls{API}.

We use the \mintinline{\ucpp}{cudaMalloc*} family of functions to request global memory, although in practice we have higher-level containers available, such as \mintinline{\ucpp}{thrust::device_vector<T>}. Nowadays, it is not considered good practice to call \mintinline{\ucpp}{cudaMalloc} directly, similarly to using \mintinline{\ucpp}{malloc/new} vs. \mintinline{\ucpp}{std::vector<T>} in the CPU.
We can populate the allocated global memory copying data from the CPU via the \mintinline{\ucpp}{cudaMemcpy*} family of functions. As with allocation, a series of higher-level functions are available as alternatives. In particular we have \mintinline{\ucpp}{thrust::copy}, which handles CPU-\gpu copies. Additionally, kernels can write to or read freely from global memory arrays, with the atomic considerations proper to a parallel environment.
We use the keyword \mintinline{\ucpp}{__global__} to mark a C++ function as a CUDA kernel, which must return a \mintinline{\ucpp}{void} (i.e not return a value), a kernel must be called with a special syntax and specifying the thread grid geometry (see example code \ref{code:cudakernel}).
Additionally, kernels might call other functions, which must be adorned with the \mintinline{\ucpp}{__device__} attribute\footnote{The \mintinline{\ucpp}{__host__} attribute also exists. A function adorned with both device and host attributes can be called from both the device and the host.}.

\begin{code2}[Creating vectors allocated in the \gpu, populating them and performing the saxpy operation using a kernel.]{label=code:cudakernel}
  #include<thrust/device_vector.h>
  #include<iostream>
  
  //Example of a device function, returns a*x+y
  __device__ float saxpy_element(float a, float x, float y){
    returns a*x + y;
  }
  
  //Performs the operation y = a*x + y, where y and x are vectors of size n and a is a number  
  //Each thread will handle one element
  //This kernel must be launched with at least n threads
  __global__ void saxpy(float a, const float *x, float *y, int n){
    //Built-in variables are available in kernels to compute things like a unique id for each thread in the grid
    int id = blockIdx.x*blockDim.x + threadIdx.x;
    //The variable "id" lives in the thread-local, aka register, memory space.
    //In case the kernel is launched with more threads than elements.
    if(id>=n) return;
    //The contents of the input arrays, x and y, live in the global memory space
    y[id] = saxpy_element(a, x[id], y[id]);
  }
  
  int main(){
    const int n = 1<<20;
    //Allocate two vectors in GPU global memory
    thrust::device_vector<float> y(n), x(n);
    //Fill the vectors with some arbitrary number
    //These operations will be performed in the GPU
    thrust::fill(y.begin(), y.end(), 1.123f);
    thrust::fill(x.begin(), x.end(), 2.123f);      
    float a = 1.0f;
    //Configure and launch the kernel
    int blockSize = 128;  //Number of threads per block
    int nBlocks = n/blockSize + 1; //Number of blocks
    //Get pointers to the global memory
    float* y_ptr = thrust::raw_pointer_cast(y.data());
    float* x_ptr = thrust::raw_pointer_cast(x.data());
    //Writing or reading from these pointers from the CPU will result in a crash, since the CPU cannot access the device memory (and viceversa).
    //y_ptr[0] = 1.0; //<- Not allowed
    //Launch the kernel with nBlocks blocks of blockSize threads each
    saxpy<<<nBlocks, blockSize>>>(a, x_ptr, y_ptr, n);
    //Let us download the results to the CPU
    std::vector<float> y_host(n);
    //The thrust library knows this is a device to host copy
    thrust::copy(y.begin(), y.end(), y_host.begin());
    //Now we can print the contents of the CPU copy
    std::cout<<y_host[0]<<<std::endl;
    return 0;
  }
\end{code2}

In a CUDA code it is important to always know the provenance of a pointer in order to know where to access it from\footnote{There is a special kind of memory, called \emph{managed memory}, that can be accessed from both devices freely. However, it comes with its own downsides and is beyond the scope of this introduction. \uammd sometimes defaults to managed memory when compiled in debug mode.}.
This is the original motivation for the \emph{ParticleData} \uammd object.

%\item Different memories in the GPU; global memory, register memory, shared memory, constant memory.
%  The \gpu has a series of memory spaces that we can leverage.
%  \begin{itemize}
%  \item Global memory: The main memory, it is the equivalent of the heap memory in the CPU. Memory allocated in this memory space is accessible at all times by every thread in a kernel. We use the \mintinline{\ucpp}{cudaMalloc*} family of functions to request global memory, although in practice we have higher level containers available, such as \mintinline{\ucpp}{thrust::device_vector<T>}. Nowadays, it is not a good practice to call \mintinline{\ucpp}{cudaMalloc} directly, similarly as with \mintinline{\ucpp}{malloc/new} vs. \mintinline{\ucpp}{std::vector<T>} in the CPU.
%    We can populate the allocated global memory copying data from the CPU via the \mintinline{\ucpp}{cudaMemcpy*} family of functions. As with allocation, a series of higher level functions are available as alternatives. In particular we have \mintinline{\ucpp}{thrust::copy}, which handles CPU-\gpu copies. Additionally, kernels can write or read freely to global memory arrays, with the atomic considerations proper to a parallel environment.
%    Similar to the CPU's heap memory, global memory is slow to access (although a series of caches exists to mitigate this).
%  \item Register memory: Local storage for each thread in a block, very limited but has the fastest access time of all memory spaces.
%  \item Shared memory: Kernels have available a fast-access memory space that is shared by all threads inside a block.
%    



\chapter{Dealing with the FFT in the GPU} \label{ch:appendixa}

Many of the numerical techniques in this manuscript make use of the \gls{FFT}, mostly in order to easily compute convolutions. The \gpu is a very powerful hardware for \gls{FFT}, but using the available \glspl{API} can become a real nuisance. In this chapter we will see how \uammd makes use of the CUDA library \emph{cuFFT}~\cite{cufft} and how to work with data in Fourier space. While the information hereafter focuses on the \emph{cuFFT} library, most lessons are applicable to other popular \gls{FFT} libraries (such as \emph{FFTW}).
The documentation of the \emph{cuFFT} library is sometimes obscure (and even lacking). There are some particular details that have made me waste countless hours. Hopefully with the help of this chapter you can save some.

Let's assume we want to work on some three-dimensional\footnote{The dimensionality of the transformation is tipically referred to as \emph{rank}, so a 3D transform has rank 3.} data defined on the nodes (cells) of a mesh (grid) with size $\vec{n} = (n_x, n_y, n_z)$. For instance, the forces spread to the grid in \gls{FCM}.
A one dimensional transform has size $n: = \vec{n} =(n_x,1,1)$.

We will store the data in a linear array of size $n_xn_yn_z$, and access the value of cell $\vec{c} = (c_x,c_y,c_z)$ at the element $i = c_x + (c_y + c_zn_y)n_x$. Where $c_{x/y/z} \in [0, n_{x/y/z}-1]$.

\section*{Choosing an efficient grid size}
\gls{FFT} libraries will typically process transformations of any sizes, however, many algorithms are dramatically more efficient when the size meets some conditions. Often the most efficient transform sizes are the powers of two ($n = 2^i$), but in general a multiple of the first prime numbers will work as well

\begin{equation}
  \label{eq:fftfriendly}
n = 2^i3^j5^k7^l11^m
\end{equation}

Where $i,j,k,l,m$ are positive integers. A number that can be expressed like this is referred to as an \gls{FFT}-friendly number.
Typically we will look for the next friendly number given a certain target size. One way to find this number is to simply generate all numbers, limiting the exponents to some low arbitrary numbers, that meet Eq. \eqref{eq:fftfriendly}, sort them and find the nearest one to a target\footnote{The code in \uammd that performs this operation is at \emph{utils/Grid.cuh}.}.

Although this is a good rule of thumb, we can expect the performance to vary between implementations, hardware and version. The best strategy is to test on a case by case basis. This effort may be worth it for long simulations, since this means that a bigger transformation can sometimes be faster. Given that in many spectral algorithms a bigger grid size means more accuracy, we could get a more accurate run with a shorter runtime. As a side note, in my personal experience, virtually every \gls{FFT} implementation will perform much better when $i\le 1$ in Eq. \eqref{eq:fftfriendly}.


\section*{Complex to real (C2R) and Real to complex (R2C) transforms}

One particular optimization commonly employed is to take advantage of the fact that, in a C2R or R2C transform, half the wave numbers are redundant. In these cases the signal in Fourier space, $\hat{s}(\vec{k})$, must meet that
\begin{equation}
  \hat{s}(\vec{k}) = \hat{s}(\vec{n} - \vec{k})^*
\end{equation}
In particular \emph{cuFFT} will only store the first $\textrm{floor}(n/2)+1$ wavenumbers in the $x$ direction. By default, an R2C transform will take an input of size $\vec{n}$ real numbers and return a complex array of size $\hat{\vec{n}} := (\textrm{floor}(n_x/2)+1, n_y, n_z)$ complex numbers\footnote{Note that since a complex number is stored as of two real numbers, the storage of both arrays is similar}. Similarly, a C2R transform will take a $\hat{\vec{n}}$ sized complex array and return a size $\vec{n}$ array with real numbers.

In principle, \emph{cuFFT} allows to store the input and output of a C2R or R2C transform in the same array in what it's called an \emph{in-place} transform. However, in my experience this results in unexpected behavior, so I advise against it\footnote{The documentation hints that in these cases the input size in the $x$ direction for an in-place C2R transform should be $2(\textrm{floor}(n_x/2)+1)$ to accomodate for all necessary complex values in the output. However, this does not seem to be the same rule for a C2R transform and has caused some problems to me in the past}.

\section*{Data layout}
Since most spectral algorithms in \uammd require C2R and R2C transforms it is worth giving here some more details about the correspondence of wave numbers and elements in the data array.

Here is a convenience function that returns the wave number in each direction that corresponds to a certain index in the data array.
\begin{code2}[Getting the wave number corresponding to a given linearized index.]{label=code:i2wn}
//Returns the wave number in each direction given a linear index, i, and a grid size, nk.
int3 indexToWaveNumber(int i, int3 nk){
  int ikx = i%(nk.x/2+1);
  int iky = (i/(nk.x/2+1))%nk.y;
  int ikz = i/((nk.x/2+1)*nk.y);
  ikx -= nk.x*(ikx >= (nk.x/2+1));
  iky -= nk.y*(iky >= (nk.y/2+1));
  ikz -= nk.z*(ikz >= (nk.z/2+1));
  return make_int3(ikx, iky, ikz);
}
\end{code2}

Most \gls{FFT} libraries provide some kind of advanced interface allowing to customize the data layout to some extent. In \emph{cuFFT} this is called the \emph{Advanced Data Layout}. In \uammd this is used extensively to compute three or four transformations in a single batch using interleaved data. For example, we can transform in a single call the three directions of the forces spread to the grid in \gls{FCM} (see sec. \ref{sec:fcm}), which are stored in a vector with \emph{real3}\footnote{The real3 type stores three scalars contiguously.} type elements.
For example, if we want to transform the three coordinates for the forces in an interleaved array, we can instruct the library to interpret this data as three signals, each starting after the other and strided by three elements.
\section*{Nyquist points}
The complex data resulting of a R2C transform (or the input data of a C2R transform) meet the condition $data(\vec{i}) = data(\vec{i} - \vec{n})^*$. A consequence of this is that, in a transformation with an even number of points in some direction, there are some points that are the conjugates of themselves. We call these uncoupled modes Nyquist points.

In \uammd, we take this into account mainly when including random fluctuations in Fourier space (see for instance the \gls{FCM} in chapter \ref{sec:fcm}).

There are $8$ nyquist points at most (including the $\vec{k} = (0,0,0)$ mode), corresponding to the vertices of the inferior left quadrant of the grid.
Here is a convenience function that determines if a certain wave number is a Nyquist point:
\begin{code2}[Finding out if a wave number corresponds to a Nyquist point given the wave number and the grid size.]{label=code:nyquist}  
bool isNyquistWaveNumber(int3 ik, int3 n){
  //Is the current wave number a nyquist point?
  const bool isXnyquist = (ik.x == n.x - ik.x);
  const bool isYnyquist = (ik.y == n.y - ik.y);
  const bool isZnyquist = (ik.z == n.z - ik.z);
  const bool nyquist =  
    (isXnyquist and ik.y==0    and ik.z==0)    or //1
    (isXnyquist and isYnyquist and ik.z==0)    or //2
    (ik.x==0    and isYnyquist and ik.z==0)    or //3
    (isXnyquist and ik.y==0    and isZnyquist) or //4
    (ik.x==0    and ik.y==0    and isZnyquist) or //5
    (ik.x==0    and isYnyquist and isZnyquist) or //6
    (isXnyquist and isYnyquist and isZnyquist);   //7
  return nyquist;
}
\end{code2}


\chapter{Boundary Value Problem (BVP) Solver} \label{sec:bvp}
In sections \ref{sec:dpstokes} and \ref{ch:dppoisson} we laid out the basic \glspl{PDE} as \glspl{BVP} with the following generic form
\begin{equation}
  \label{eq:bvpmain}
  \begin{aligned}
    &(\partial^2_{z}-k^2)y(z)=f(z)\\
    &(\partial_{z}\pm k)y(\pm 1)=
    \begin{bmatrix}
      \alpha\\
      \beta
    \end{bmatrix}
\end{aligned}
\end{equation}
Where $\partial_z y:= \frac{\partial y}{\partial z} := y'(z)$ and $\partial^2_{z} y:= \frac{\partial^2 y}{\partial z^2} := y''(z)$
%\begin{equation}
%  \label{eq:bvpmain}
%  \begin{aligned}
%    &y''(z)-k^2y(z)=f(z)\\
%    &y'(1)+ ky(1)=\alpha ,\qquad y'(-1)- ky(-1)=\beta.
%\end{aligned}
% \end{equation}

The domain is here defined in the $[-1, 1]$ range for simplicity. We can later generalize to any size via a simple change in units. For instance by defining $z = z'/H$ (with $z'\in [-H, H]$) and $k = k'H$.

In this section we describe a generic, \gpu-friendly, spectral solver for the set of equations in \eqref{eq:bvpmain} based on the spectral integration method described in~\cite{Greengard1991}.

We will work with the discrete Chebyshev transform of Eq. \eqref{eq:bvpmain}. In general, we can expand a given function, $g(z)$ with $z\in \mathcal{R}$ into the first $N$ terms of its Chebyshev series so that
\begin{equation}
\label{eq:bvpchebexp}
g(z) = \sum_{n=0}^{N-1} \fou{g}_n T_n(z)
\end{equation}
Where we use the subscript $n$ to distinguish between the Chebyshev and Fourier coefficients (since throughout this manuscript we have used the subscript $k$ to denote Fourier coefficients).

In general, the Chebyshev polynomials of the first kind, $T_n$, are defined via a recurrent relation with
\begin{equation}
  \label{eq:bvpchebpoly}
  \begin{aligned}
    T_0(x) &= 1\\
    T_1(x) &= x\\
    T_{n+1}(x) &= 2xT_n(x) - T_{n-1}(x)
  \end{aligned}
\end{equation}

On the other hand, the Chebyshev polynomials can be interpreted as a cosine series under a change of variables. Given that by definition $T_n(cos(\theta))= cos(n\theta)$, evaluating the function at the extrema points $x_n = cos(n\pi/(N-1))$ transforms the expansion in Eq. \eqref{eq:bvpchebexp} into a cosine series. We can then leverage the \gls{FFT} to perform a discrete cosine transform by transforming the periodic extension of a signal. This allows to obtain the Chebyshev coefficients of a signal in $O(Nlog(N))$ operations. We refer to this technique as the \gls{FCT} (see chapter 8 of ~\cite{Trefethen2000}).
%In Appendix \ref{sec:fct} we give some details about how to do this in a computer implementation.

We can then leverage several interesting properties of the Chebyshev series to easily (and exactly) derivate and integrate the different quantities.

%If we evaluate $f(z)$ in the roots of the Chebyshev polynomials, $z_i=cos(i\pi/N)$, we can leverage the \gls{FFT} to provide the coefficients of the Chebyshev transform (in what is usually referred to as a Fast Chebyshev Transform (FCT)) by using it to compute a discrete cosine transform. The proof, while straight forward, is cumbersome and beyond the scope of this work, it can be found in Chapter 8 of~\cite{Trefethen2000}. In particular, the fast Chebyshev transform is carried out by taking the \gls{FFT} of the periodic extension of $f(z)$\footnote{cuFFT can be used for this, which is the approach in \uammd.} 

In particular, we can use the well known indefinite integrals of the Chebyshev polynomials to integrate $\fou{y}''_n$ twice and get $\fou{y}_n$. First by integrating $\fou{y}''_n$:
\begin{equation}
  \begin{aligned}
\fou{y}'_1&= \frac{1}{2}\left(2\fou{y}''_0-\fou{y}''_2\right)\\
\fou{y}'_n &= \frac{1}{2n}\left(\fou{y}''_{n-1}-\fou{y}''_{n+1}\right), \quad n \geq 1.
\end{aligned}
\end{equation}
%Eq. \eqref{eq:ds} is the relationship used to apply the Chebyshev differentiation matrix in linear time to compute $\displaystyle \frac{\partial \hat{f}^z}{\partial z}$ in Eq.\ \eqref{eq:bvp_p}. Specifically, we set $d_{N-1}=0$ and can then compute $a_n$ for all $n=N-1, \dots 0$ (in that order).  
And then integrating again
\begin{equation}
  \label{eq:bvpyn}
  \begin{aligned}
&\fou{y}_1 = \frac{1}{2}\left(2\fou{y}'_0-\fou{y}'_2\right)=\fou{y}'_0 -\frac{1}{8}\left(\fou{y}''_1-\fou{y}''_3\right)\\
&\fou{y}_2 = \frac{1}{4}\left(\fou{y}'_1-\fou{y}'_3\right)=\frac{1}{4}\left[\frac{1}{2}\left(2\fou{y}''_0-\fou{y}''_2\right)-\frac{1}{6}\left(\fou{y}''_2-\fou{y}''_4\right) \right]\\
&\fou{y}_n = \frac{1}{2n}\left(\fou{y}'_{n-1}-\fou{y}'_{n+1}\right)=\\
&\quad\quad\frac{1}{2n}\left[\frac{1}{2n-2}\left(\fou{y}''_{n-2}-\fou{y}''_n\right)-\frac{1}{2n+2}\left(\fou{y}''_n-\fou{y}''_{n+2}\right) \right], \quad n \geq 3.
\end{aligned}
\end{equation}
This formulation gives two free parameters $\fou{y}_0$ and $\fou{y}'_0$, which are obtained using the \glspl{BC}. Additionally, we consider $\fou{y}''_n = \fou{y}'_n = 0$ for $n > N-1$ when calculating $\fou{y}_n$ using \eqref{eq:bvpyn}.

Now, we can reformulate the boundary value problem using the Chebyshev series representations in \eqref{eq:bvpchebexp} as
\begin{equation}
\label{eq:bvpn0}
\sum_{n=0}^{N-1} (\fou{y}''_n-k^2\fou{y}_n)T_n(z) = \sum_{n=0}^{N-1}\fou{f}_n T_n(z).
\end{equation}
Matching modes gives a system of $N+2$ equations for the Chebyshev coefficients
\begin{equation}
\label{eq:bvpmodes}
\begin{aligned}
&\fou{y}''_n-k^2\fou{y}_n=\fou{f}_n \qquad n=0, \dots, N-1\\
&\sum_{n=0}^{N-1} (\fou{y}'_n+k\fou{y}_n) = \alpha\\
&\sum_{n=0}^{N-1} (\fou{y}'_n-k\fou{y}_n) (-1)^n = \beta
\end{aligned}
\end{equation}
Where we have $N+2$ unknowns $\fou{y}_0, \fou{y}'_0, \fou{y}''_0, \dots \fou{y}''_{N-1}$ and equations. We solve the algebraic system of equations\ \eqref{eq:bvpmodes} for the second derivative coefficients $\fou{y}''_0, \dots \fou{y}''_{N-1}$ and integration constants $\fou{y}_0$ and $\fou{y}'_0$, then determine $\fou{y}_n$ by integrating twice using \eqref{eq:bvpyn}.

For the $k=0$ mode, the system reduces to the trivial system of equations
\begin{equation}
\label{eq:bvpmodes0}
\fou{y}''_n=\fou{f}_n \qquad n=0, \dots, N-1.
\end{equation}
The factors $\alpha$ and $\beta$, which in general can be different for each $k$, are zero for $k=0$, so in this case we enforce
\begin{equation}
\label{eq:bvpBCs0}
\sum_{n=0}^{N-1} \fou{y}_n = 0, \qquad \sum_{n=0}^{N-1} \fou{y}_n (-1)^n = 0.
\end{equation}

We use a Schur complement approach to solve the algebraic system of equations \eqref{eq:bvpmodes}. We can write the system in block form as 
\begin{equation}
\label{eq:blocksys}
\begin{bmatrix} \bm{A} & \bm{B} \\[2 pt] \bm{C} & \bm{D} \end{bmatrix}
\begin{bmatrix} \fou{\bm{y}}''\\ \fou{y}_0 \\ \fou{y}'_0 \\ \end{bmatrix}
= \begin{bmatrix}  \fou{\bm{f}}\\ \alpha \\ \beta\\ \end{bmatrix}. 
\end{equation}
Here $\bm{B}$ is $N \times 2$, $\bm{C}$ is $2 \times N$, $\bm{D}$ is $2 \times 2$, and $\bm{A}$ is an $N \times N$ \textit{pentadiagonal} matrix. In particular, the matrix $\bm{A}$, referred to as the second integral matrix, has only three non zero diagonals ($i=j$ and $i = j\pm 2$) which allows to use a specialized factorization algorithm. In \uammd we use the so-called \emph{KBPENTA} in ~\cite{Karawia2010}, modified for the special case of only three nonzero diagonals.\footnote{The matrix $\bm{A}$ is pre factorized (incurring an auxiliar storage of $3N$ elements for a given $k$). Using this our special pentadiagonal system can be solved in just $2N$ operations.} 

We start by solving the $2 \times 2$ system
\begin{equation}
(\bm{C}\bm{A}^{-1}\bm{B}-\bm{D})\begin{bmatrix}  \fou{y}_0 \\ \fou{y}'_0\\ \end{bmatrix} = \bm{C}\bm{A}^{-1}\bm{f}-\begin{bmatrix}  \alpha \\ \beta\\ \end{bmatrix}
\end{equation}
for $\fou{y}_0$ and $\fou{y}'_0$. The $2\times 2$ matrix in the left hand side as well as $\bm{C}\bm{A}^{-1}$ ($2\times N$ size) are precomputed and stored for each $k$. Since the inverse of $\bm{A}$ is only needed at the precomputation stage no special performance considerations are required for it\footnote{In \uammd, the CPU \emph{getrf} BLAS square matrix inversion routine is used.}.

Finally we obtain the coefficients $\fou{\bm{y}}''=(\fou{y}''_0, \dots \fou{y}''_{N-1})$ by solving
\begin{equation}
\bm{A}\fou{\bm{y}}''=\left(\bm{f}-\bm{B}\begin{bmatrix}   \fou{y}_0 \\ \fou{y}'_0\\ \end{bmatrix}\right)
\end{equation}
using the pre-factorized modified \emph{KBPENTA} algorithm.

Note that we need to solve this system for each wave number, $k$. Our solver requires the evaluation of several recurrent relations, making it mostly a serial algorithm not worth parallelizing. In order to take advantage of the \gpu, we assign a thread to each $k$. As described above, much of the problem can be precomputed at the expense of memory storage (around $5N_kN$ values in total, being $N_k$ the number of wave vectors\footnote{Since we use R2C \gls{FFT} we will typically have $N_k=(N_x/2+1)N_y$.}
Except for the $k=0$ wave number, every solve is identical in operations, meaning that each thread will present the same memory access pattern in the, precomputed, auxiliar arrays. We take advantage of this by storing them in a coherent (strided) way, where a given element of an auxiliar array is stored contiguously (and ordered) for all wave numbers (so that the thread access pattern is cache friendly)\footnote{I wrote a special \gpu auxiliar storage handler for this that allows to switch between a strided and contiguous pattern. It can be found in the file \emph{BVPMemory.cuh} in \uammd. In general a strided access provides a much better performance.}. Testing suggests that our \gls{BVP} solver yields similar performance as the \gls{FCT} for small $N_k$, becoming increasingly faster for increasing $N_k$ (as expected given that our solver has linear scaling as opposed to the $O(Nlog(N))$ operation count of the \gls{FCT}).

%\chapter{The Fast Chebyshev Transform}\label{sec:fct}
%
%\begin{itemize}
%\item Describe the FCT.
%\item Its the FFT of the periodic extension of the signal evaluated at the Chebyshev roots
%\item Numerical periodic extension is $F_{2N-i} = f_i$ where $i=1,2,...,N-1$.
%\item Describe the numerical indexing intricacies and the scaling factors
%\end{itemize}
%

\defbibheading{bibintoc}[\bibname]{%
  \phantomsection
  \manualmark
  \markboth{\spacedlowsmallcaps{#1}}{\spacedlowsmallcaps{#1}}%
  \addtocontents{toc}{\protect\vspace{\beforebibskip}}%
  \addcontentsline{toc}{chapter}{\tocEntry{#1}}%
  \chapter*{#1}%
}

\chapter{UAMMD's online documentation}\label{ch:online}
This manuscript is based on UAMMD version 2.0 (which was released at the same time as this manuscript). Its contents, as well as the code examples, should be valid at least until version 3.0, the next release that might contain API breaking changes.

At the time of writing, the \uammd codebase is hosted as a git repository on the github platform (currently the de facto standard git hosting platform), at \url{https://github.com/RaulPPelaez/UAMMD}. Note that the UAMMD repository offers a git branch for each major version. Be sure to check out the branch \emph{v2.x} to have a version that corresponds to this manuscript. A summary of changes between releases can be found at the \emph{CHANGELOG} file included in UAMMD's repository.

Platforms change over time and github might not exist at the time you are reading this\footnote{As a side note, \uammd was included among github's Artic Code Vault program repositories. Thus, a physical copy of the codebase can be found 250 meters under the permafrost in a vault inside an abandoned coal mine in the Svalbard archipelago. This means that the codebase should be retrievable for at least the next thousand years.}. If that is the case and a google search does not help you locate its new hosting you have two options; either find me and request it directly or look in the wayback machine. At the time of writing, the internet archive has already scraped the uammd repository at least once, you can find the archived versions at \url{http://web.archive.org/web/202*/https://github.com/RaulPPelaez/UAMMD}.

This very manuscript is focused on the underlying theory behind the many algorithms exposed by the code as well as the design choices involved in its creation. However, it is not intended as a usage guide for \uammd (although several code examples are included). A user wanting to learn how to use \uammd can also benefit from the online resources. In particular, \uammd offers three distinct sources of knowledge:
\begin{enumerate}
\item Code examples.
  The repository itself contains a plethora of organized usage examples showcasing the different \uammd functionalities. In particular, the \emph{basic\_concepts} folder contains a series of tutorial-like examples intended as an introduction to the \uammd ecosystem. This is the recommended next step into UAMMD for the reader of this manuscript.
\item Wiki.
  A wiki accompanying the codebase provides extensive information about all the available modules, as well as detailed information about the compilation process.
\item The book, ``A Painless Introduction to Programming UAMMD Modules'' by Marc Meléndez~\cite{Marc2020}.
  This book provides an introduction to \uammd from the very ground up, while making little assumptions about the readers familiarity with programming, complex fluids and their related numerical methods. Note, however, that at the time of writing \cite{Marc2020} introduces UAMMD ``v1.x''. Therefore some modifications might be required to translate its lessons into ``v2.0'' (the \emph{CHANGELOG} can help with this).
\end{enumerate}

\chapter{The Transverser and Potential interfaces}\label{ch:transverser}

\section{The Transverser Interface} \label{sec:transverser}

Many of the algorithms described in this manuscript require some kind of particle traversal.
%A Transverser is a concept that refers to the action of going through each element of something.  
Say, for instance, that for each particle we want to visit the rest of the particles that are closer than a certain distance. Or simply all of the other particles. More generally, we might want to perform some kind of operation equivalent to a matrix-vector multiplication, for which in order to compute one element of the result, the vector needs to go through a row of the matrix.
In these cases, a \emph{Transverser}\footnote{The word \emph{Transverser} was chosen to convey that it is used to traverse and transform} is used.

A \emph{Transverser} holds information about what to do with a pair of particles, what information is needed to compute this interaction, and what to do when a particle has interacted with all pairs it is involved in.  

Being such a general concept, a \emph{Transverser} is used as a template argument, and therefore cannot be a base virtual class that can be inherited. This is why it is a "concept". No assumption can be made about the return types of each function, or the input parameters, the only common things are the function names.  

For each particle to be processed the \emph{Transverser} will be called for: %\todo{Draw this in a skematik}
\begin{itemize}
\item Setting the initial value of the interaction result (function \emph{zero})
\item Fetching the necessary data to process a pair of particles  (function \emph{getInfo})
\item Compute the interaction between the particle  and each of its neighbours (function \emph{compute})
\item Accumulate/reduce  the result for each neighbour (function \emph{accumulate})
\item  Set/write/handle the accumulated result for all neighbours (function \emph{set})
 \end{itemize}
The same \emph{Transverser} instance will be used to process every particle in an arbitrary order. Therefore, the Transverser must not assume it is bound to a specific particle.

The \emph{Transverser} interface requires a given class/struct to provide the following public device (unless, ``prepare'', that must be a host function) member functions:

\begin{itemize}
\item \mintinline[breaklines]{\ucpp}{Compute compute(real4 position_i, real4 position_j,Info info_i, Info info_j);}

  
  For a pair of particles
  characterized by position and info this function must return the
  result from the interaction for that pair of particles. The last
  two arguments must be present only when \emph{getInfo} is defined.The
  returning type, \emph{Compute}, must be a POD type (just an aggregate of
  plain types), for example a real when computing energy.

\item \mintinline{\ucpp}{void set(int particle_index, Compute &total);}

  
   After calling compute for all neighbours this function will be called with the contents of "total" after the last call to "accumulate".
   Can be used to, for example, write the final result to main memory.

 \item \mintinline{\ucpp}{Compute zero();}

   
   This function returns the initial value of the computation, for example {0,0,0} when computing the force. 
   The returning type, \emph{Compute}, must be a POD type (just an aggregate of plain types), for example a real when computing energy. Furthemore it must be the same type returned by the "compute" member.
   This function is optional and defaults to zero initialization (it will return Compute() which works even for POD types).
    
 \item \mintinline{\ucpp}{Info getInfo(int particle_index);}

   
   Will be called for each particle to be processed and returns the per-particle data necessary for the interaction with another particle (except the position which is always available). For example the mass in a gravitational interaction or the particle index for some custom interaction.
   The returning type, Info, must be a POD type (just an aggregate of plain types), for example a real for gravitation.
   This function is optional and if not present it is assumed the only per-particle data required is the position. 
   In this case the function "compute" must only have the first two arguments.

 \item \mintinline{\ucpp}{void accumulate(Compute &total, const Compute &current);}

   
  This function will be called after "compute" for each neighbour with its result and the accumulated result.
  It is expected that this function modifies "total" as necessary given the new data in "current".
  The first time it is called "total" will be have the value as given by the "zero" function.
  This function is optional and defaults to summation: total = total + current. Notice that this will fail for non trivial types.
     
\item \mintinline{\ucpp}{void prepare(std::shared_ptr<ParticleData> pd);}

  
  This function will be called one time on the CPU side just before processing the particles.
  This function is optional and defaults to simply nothing.
 \end{itemize}

Example code \ref{code:ncounter} contains a very bare-bones instance of a \emph{Transverser}. In particular, \emph{NeighbourCounter} relies on as much default behavior as possible, presenting only a \emph{compute} and \emph{set} functions.
If we apply the \emph{NeighbourCounter} \emph{Transverser} to one of the neighbour lists in \uammd (see chapter \ref{sec:nlist}), the output (``nneigh'' array) will hold, for each particle, the number of neighbour particles.
\begin{code2}[A \emph{Transverser} that counts the number of neighbours of each particle.]{label=code:ncounter}
struct NeighbourCounter{
  int *nneigh;
  real rc;
  Box box;
  NeighbourCounter(Box i_box, real i_rc,int *nneigh):
    rc(i_rc),box(i_box),
    nneigh(nneigh){}

  //There is no "zero" function so the total result starts being 0.
  
  //For each pair computes counts a neighbour 
  //if the particle is closer than rcut
  __device__ auto compute(real4 pi, real4 pj){
    const real3 rij = box.apply_pbc(make_real3(pj)-make_real3(pi));
    const real r2 = dot(rij, rij);
    if(r2>0 and r2< rc*rc){
      return 1;
    }
    return 0;
  }
  //There is no "accumulate"
  // the result of "compute" is added every time.
  //The "set" function will be called with the accumulation
  // of the result of "compute" for all neighbours. 
  __device__ void set(int index, int total){
    nneigh[index] = total;
  }
};
\end{code2}
Alternatively, if we apply the \emph{Transverser} in the code example \ref{code:ncounter} to the \emph{NBody} module (see chapter \ref{sec:nbody}) each particle will go through every other one, and thus all the elements of the \emph{NeighbourCounter} output will be equal to the total number of particles.
\section{The Potential Interface} \label{sec:potential}

This interface is just a connection between the \emph{Transverser} and \emph{Interactor} concepts. Additionally, \emph{Potential} aids with one limitation of the CUDA programming language and \gls{GPU} programming in general. On the one hand, register memory in a \gpu is quite limited, so it is not a good idea to use large objects in a kernel. On the other, there are some technical details that prevent certain objects from existing in a \gpu kernel. For example, objects are provided by value to a kernel, which can incur undesired copies and/or destructors being called. Thus, it is some times worth it to make a conceptual and programmatic separation between CPU and \gpu objects.
In this regard, \emph{Transversers} are \gpu objects, while \emph{Interactors} or \emph{Potentials} are meant to be used in the CPU.
Furthermore, while \emph{Transverser} describes a very general computation, \emph{Potential} only holds the logic on how to compute forces, energies and/or virials.
\emph{Potential}s are used to provide force-, energy- and/or virial-calculating \emph{Transversers} to an \emph{Interactor}\footnote{In particular, the \emph{PairForces} module (see chapter \ref{sec:shortrange}) needs a \emph{Potential} encoding the specific particle interaction.} (alternatively, the \emph{Transverser} provided by a \emph{Potential} could be used with a neighbour list directly). In turn, this \emph{Interactor} can be either used on its own to compute directly, or provided to a \emph{Integrator}. %\todo{This is too abstract}

The \emph{Potential} interface is straightforward, requiring only two functions:
\begin{itemize}
\item \mintinline{\ucpp}{real getCutOff();}
  
  This function must return the highest cut off distance required by the interaction.
\item \mintinline{\ucpp}{Transverser getTransverser(Interactor::Computables comp, Box box, std::shared_ptr<ParticleData> pd);}
  
  This function must provide an instance of a \emph{Transverser} that, using the provided \emph{ParticleData} and \emph{Box} instances, computes anything requested by the \emph{Computables} list (mainly forces, energies and/or virials, see chapter \ref{sec:interactor} for more information).
  The return type of this function, called Transverser here, can be any valid \emph{Transverser} (see Appendix \ref{sec:transverser}) with only one restriction: The return type of the \mintinline{\ucpp}{compute} function (called \emph{Compute} in Appendix \ref{sec:transverser}) must be \mintinline{\ucpp}{ForceEnergyVirial} (a simple POD type that holds members for the force, energy and virial). See example code \ref{code:potential}.
\end{itemize}

%\todo{Check highlight in example}
\begin{code2}[An example \emph{Potential} that computes Lennard-Jones forces, energies and/or virials. For simplicity, all relevant parameters are hardcoded here. In particular, $\sigma_{lj} = 1$, $\epsilon_{lj}=1$ and the cut off is set at $r_c = 2.5\sigma = 2.5$. The potential here defined (called \emph{SimpleLJ}) calculates forces, energies and virials. Note, however, that it does so only when provided to a \emph{PairForces} \emph{Interactor} (see chapter \ref{sec:shortrange}) and, subsequently, to an \emph{Integrator}. In other words, we use \emph{Potentials} to define an \emph{Interactor}, which will be used  by an \emph{Integrator} to calculate forces, energies, etc.]{label=code:potential}
  //Some functions to compute forces/energies
  __device__ real lj_force(real r2){
    const real invr2 = real(1.0)/r2;
    const real invr6 = invr2*invr2*invr2;
    const real fmoddivr = (real(-48.0)*invr6 + real(24.0))*invr6*invr2;
    return fmoddivr;
  }

  __device__ real lj_energy(real r2){
    const real invr2 = real(1.0)/r2;
    const real invr6 = invr2*invr2*invr2;
    return real(4.0)*(invr6 - real(1.0))*invr6;
  }

  //A Transverser for computing, energy, virial and force (or just some of them).
  //It is the simplest form of Transverser, as it only provides the "compute" and "set" functions
  //When constructed, if the i_force, i_energy or i_virial pointers are null that computation will be avoided.
  struct LJTransverser{
    real4 *force;
    real *virial;
    real* energy;
    Box box;
    real rc;
    LJTransverser(Box i_box, real i_rc, real4* i_force, real* i_energy, real* i_virial):
    box(i_box), rc(i_rc), force(i_force), virial(i_virial), energy(i_energy){
      //All members will be available in the device functions
    }
    //For each pair computes and returns the LJ force and/or energy and/or virial based only on the positions
    __device__ ForceEnergyVirial compute(real4 pi, real4 pj){
      const real3 rij = box.apply_pbc(make_real3(pj)-make_real3(pi));
      const real r2 = dot(rij, rij);
      if(r2>0 and r2< rc*rc){
        real3 f;
        real v, e;        
        f = (force or virial)?lj_force(r2)*rij:real3();	
        v = virial?dot(f, rij):0;
        e = energy?lj_energy(r2):0;
        return {f,e,v};
      }
      return {};
    }
    //Note that we are making use of the default behaviors by not defining an accumulate or zero functions.
    __device__ void set(int id, ForceEnergyVirial total){
      //Write the total result to memory if the pointer was provided
      if(force)  force[id] += make_real4(total.force, 0);
      if(virial) virial[id] += total.virial;
      if(energy) energy[id] += total.energy;
    }
  };

//A simple LJ Potential, can compute force, energy, virial or all at the same time using the above Transverser.
struct SimpleLJ{
  real rc = 2.5;
  //A function returning the maximum required cut off for the interaction
  real getCutOff(){
    return rc;
  }
  //This function is required to provide a Transverser that has the ability to compute the requested Computables.
  auto getTransverser(Interactor::Computables comp,
  Box box,
  std::shared_ptr<ParticleData> pd){
    auto force = comp.force?pd->getForce(access::gpu, access::readwrite).raw():nullptr;
    auto energy = comp.energy?pd->getEnergy(access::gpu, access::readwrite).raw():nullptr;
    auto virial = comp.virial?pd->getVirial(access::gpu, access::readwrite).raw():nullptr;
    return LJTransverser(box, rc, force, energy, virial);
  }
  
};
\end{code2}




\chapter{A full UAMMD simulation example}\label{ch:fullexample}
   
The UAMMD repository contains many examples encoding full-fledged simulations\footnote{Most notably, the example \emph{generic\_md.cu} shows off virtually every module and concept described in this thesis.} making use of the tools described in this manuscript. Nonetheless, it is worth to lay out here a putting-it-all-together example that shows how to set up and run a simulation from scratch using UAMMD. In particular, source code \ref{code:full} makes use of some of the example codes written throughout this manuscript to craft a simulation of a gas of \gls{LJ} atoms (for instance, modeling argon).

\begin{code2}[A simulation of \gls{LJ} particles liquid using UAMMD. Source codes \ref{code:verletnvt}, \ref{code:pf} and \ref{code:potential} serve as a preamble (must be included or copy/pasted) here. To ease initialization, only two particles are simulated.]{label=code:full}
  #include<uammd.cuh>
  using namespace uammd;
  //In order to use the rest of the examples, we need to define all the necessary parameters inside the Parameters structure.
  //For simplicity, let us simply hardcode here the parameters
  struct Parameters{
    real dt = 0.01;
    real temperature = 0.1;
    real friction = 1.0;
    //A periodic cubic box of size L=32
    Box box = Box({32, 32, 32});
  };
  
  struct UAMMD{
    std::shared_ptr<ParticleData> pd;
    Parameters par;
  };
  //Include here the source codes listed in the caption

  int main(){
    UAMMD sim;
    //We start by initializing ParticleData
    int numberParticles = 2;
    sim.pd = std::make_shared<ParticleData>(numberParticles);
    //Now we can set the positions of the two particles
    {//Our LJ Potential example hardcodes sigma = 1, lets place out particles at a distance of 2sigma=2.
      auto positions = sim.pd->getPos(access::cpu, access::write);
      positions[0] = {0,0,0,0};
      positions[1] = {2.0,0,0,0};
    }//Remember that the handle, positions, must be destroyed ASAP
    //We can now create our integrator using the function we wrote when describing Langevin dynamics
    auto nvt = createIntegratorVerletNVT(sim);
    //Using the SimpleLJ Potential defined in a previous example we can create a PairForces Interactor
    auto ljpot = SimpleLJ(); //The SimpleLJ constructor does not have any arguments
    auto lj = createPairForcesWithPotential(sim, ljpot);
    //We now add the newly created interactor to nvt.
    nvt->addInteractor(lj);
    //Any number of Interactors can be added this way.
    //Finally, we can advance the simulation as many times as needed, one dt at a time:
    int numberSteps = 10;
    for(int step = 0; step<numberSteps; step++){
      nvt->forwardTime();
      //Whenever needed, we can request the particles state via ParticleData, for instance, to print the positions.
      {
        auto positions = sim.pd->getPos(access::cpu, access::read);
        std::cout<<"First particle is at: "<<positions[0];
        std::cout<<" at time "<<step*sim.par.dt<<std::endl;
      }
    }
    return 0;
  }
\end{code2}



   
\chapter{List of currently implemented modules}\label{sec:modulelist}

For convenience, we present here a list of all the modules available in \uammd at the time of writing. Each of them can be created and used on its own. Alternatively, several of them can be joined to construct a simulation.

Most of the modules listed below have a corresponding chapter in this manuscript. The ones that have not been described are available in \uammd's online resources.

\section{Integrators}

\begin{itemize}
\item Molecular Dynamics
\item Langevin Dynamics
\item Dissipative Particle Dynamics
\item Smoothed-particle Hydrodynamics
\item Brownian Dynamics
\item Brownian Hydrodynamics
  \begin{itemize}
  \item Open Boundaries
    \begin{itemize}
    \item Cholesky
    \item Lanczos
    \end{itemize}
  \item Triply Periodic
    \begin{itemize}
    \item Force Coupling Method
    \item Positively Split Ewald
    \end{itemize}
  \end{itemize}
\item Fluctuating Hydrodynamics
  \begin{itemize}
  \item Fluctuating Immersed Boundary
  \item Inertial Coupling Method
  \item Quasi 2D
  \end{itemize}
\item Lattice Boltzmann
\end{itemize}

\section{Interactors}

\begin{itemize}
\item Short-ranged
\item Long-ranged (nbody)
\item Triply and Doubly periodic electrostatics
\item Bonds (particle-particle, particle-point, angular, torsional...)
\item External interactions
\end{itemize}

\section{Other modules}

\begin{itemize}
\item Neighbour lists
  \begin{itemize}
  \item Cell list
  \item Verlet list
  \item LBVH list
  \end{itemize}
\item Immersed Boundary Method (spreading and interpolation between markers and a grid)
\item Lanczos iterative Krylov decomposition
\item Boundary Value Problem solver
\item Tabulated functions
\end{itemize}

\newpage


\printbibliography[heading=bibintoc]

%\newpage

%\cleardoublepage\input{FrontBackmatter/Declaration}
%\cleardoublepage\input{FrontBackmatter/Colophon}
%\newpage

\end{document}

